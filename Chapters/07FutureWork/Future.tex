
In this thesis, two main practical algorithms are proposed: the adaptive Tractor spline and the adaptive sequential MCMC algorithm. Being honest, there are a few flaws in these two algorithms and future work needs to be done to improve their performances. 


\subsection*{Gradient Boosting Tractor Spline}

Tractor spline is an advanced smoothing spline algorithm returning least true mean squared errors. However, to implement this algorithm on-line needs feasible solutions. One of them probably is combining spline method and gradient boosting algorithm. 


In machine learning application, it is better to build a non-parametric regression or classification model from the data itself. A connection between statistical framework and machine learning is the gradient-descent based formulation of boosting methods, which was derived by \cite{freund1995desicion}, \cite{friedman2001greedy}. The gradient boosting algorithm is a powerful machine-learning technique that has shown considerable success in a wide range of practical applications, particularly in machine learning competitions on \textit{Kaggle}. 


The motivation of gradient boosting algorithms is combining weak learners together as a strong leaner, which keeps minimizing the target loss function. 
It has high customizable application to meet particular needs, like being learned with respect to different loss functions. For example, for a continuous response $y\in \mathit{R}$, the loss function can be chosen a Gaussian $L_2$ loss function. Hence the squared error $L_2$ loss function is 
\begin{equation*}
L_2(y,f(x)) = \frac{1}{2}\left(y-f(x)\right)^2,
\end{equation*}
and the best trained $f^*$ is 
\begin{equation*}
f^* = \arg_{f(x)}\min E_{x,y}L_2(y,f(x)).
\end{equation*}
To find $f^*$, it is reducing the loss $\tilde{y}_i=y_i-F_{m-1}(x_i)$ recursively. Consequently, the 
\begin{equation*}
f_m(x) = f_{m-1}(x)+ \rho_mh(x,\alpha_m),
\end{equation*}
is the sum of some basic learners $h_m(x,\alpha)$. $m=1,\ldots,M$ determines the complexity of the solution. 


On-line boosting algorithms are given by \cite{babenko2009family} and \cite{beygelzimer2015online}. It is assumed that the loss over the entire training data can be expressed as a sum of the loss for each point $t$, that is $L(f(x,y))=\sum_i L(f(x_i,y_i))$. Instead of computing the gradient of entire
loss, the gradient is computed with respect to just one data point. 

Further, by adding additional regularization term will help to smooth the final learned weights to avoid over-fitting in a penalized regression problem \cite{chen2016xgboost}. 

Inspired by the above ideas, the basis functions of Tractor spline can be treated as basic learners and $f_m(t)=f_{m-1}(t)+\rho_m N_{2m-1}(t,\theta)+\eta_mN_{2m}(t,\theta)$ is obtained along new observations $\{y_t,v_t\}$ coming into data stream with respect to minimizing the loss function $L(f(t,x,u,y,v))$. 



%\subsection*{Error Analysis}
%
%In Chapter \ref{ChapterTS}, the reconstruction is found by Tractor spline, which returns the least true mean squared errors. However, a further error analysis is not given. 



\subsection*{Non-trivial Tractor Spline with Correlated Errors}

In Chapter \ref{ChapterGPR}, the Bayesian estimate of Tractor spline with correlated errors is given. The extended GCV helps to find the optimal parameters for the estimate. Whereas, the extended GCV is only applicable for trivial Tractor spline, where $\lambda(t)$ is a constant. 


In \cite{opsomer2001nonparametric} and \cite{wang1998smoothing}, the authors present a few extensions of the generalized maximum likelihood (GML), generalized cross-validation (GCV) and unbiased risk (UBR) methods to find optimal parameters for smoothing spline ANOVA models when observations are correlated. These algorithms in applied on conventional polynomial smoothing spine, but not on Tractor spline. Suppose the errors of the observations $y$ and $v$ are $\varepsilon_y\sim N(0,\sigma^2W^{-1})$ and $\varepsilon_v\sim N(0,\tau^2U^{-1})$ with unknown parameters $\sigma^2$ and $\tau^2$, I hope to find general solutions, similarly to GML, GCV and UBR, to tune the parameters for Tractor spline returning least true mean squared errors. 




\subsection*{Informative Proposals}

In Chapter \ref{ChapterMCMC}, the proposed adaptive MCMC algorithm draws samples of $\theta=\{\gamma,\xi^2,\lambda^2,\sigma^2,\tau^2\}$ from $N(m,C)$, where the information of $m$ and $C$ are learned from a self-tuning learning phase. For each $\theta^{(i)}$, it generates a paired $\{\mu_t^{(i)},\Sigma_t^{(i)}\}$ for mixture Gaussian $x_t$. 

In the sampling step of real data application, the mechanic boom status is not incorporated, which may provide useful information. Like the penalty parameter $\lambda$ being classified by $\lambda_d$ and $\lambda_u$ according to boom status, the MCMC sampler may use this information to propose $\theta$ with different strategy, such as different step sizes, different $m$s and $C$s. In this way, the parameter $\theta$ is classified by $\theta_u$ and $\theta_d$. 

(not finished yet)

%Propose samples on two direction $\begin{bmatrix}
%W & 0 \\ 0 & U
%\end{bmatrix}$.



\subsection*{Grid-based MCMC}

In several references, Grid-based methods have been proved that it provides an optimal recursion of the filtered density $p(x_t\mid y_{1:t})$ if the state space is discrete and consists of a finite number of states \cite{ristic2004beyond}, \cite{stroud2016bayesian}, \cite{arulampalam2002tutorial} and \cite{hartmann2016grid}. 

In the application of real time series dataset, the model is assumed as an OU-process containing five unknown parameters. With the idea of grid-based algorithm, the 5-dimension parameter space $\mathit{R}^5$ can be initialized by spanning $\theta_0^{(i)}$ with equal weights $w_0=\{\frac{1}{N},\frac{1}{N},\frac{1}{N},\frac{1}{N},\frac{1}{N}\}$ at time $t=0$, where $i=1,\ldots,N$. When a new observation $Y_t=\{y_t,v_t\}$ comes into the system, the weights for each parameter in each subspace are updated via $w_t\propto p(Y_t\mid \theta,Y_{1:t-1})w_{t-1}$. 

However, the grid-based MCMC may not practical for a higher $n$-dimensional space, which requires $\BigO{N^n}$ computation cost per MCMC step. 






















