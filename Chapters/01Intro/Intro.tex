
%% % % %第一段： 提出并介绍这个问题
%The problem in the world I explore in this paper is....... 
%The reason why it is an important problem is......
%
%% % % %第二段：找出解决问题的视角
%The lends I adopt to explore this problem is.... it is a useful lens because..........
%
%% % % %第三段：描述解决问题的场景和发现Analyzing a dataset of xxxxxxxxxxx, 
%I reach the following findings.......
%
%% % % %第四段：介绍你的贡献
%This study contributes to.........

%Inference and Characterization of Planar Trajectories
%Inference and Characterization of Planar Trajectories
%Inference and Characterization of Planar Trajectories


\section{Background}


The Global Positioning System (abbreviated as GPS) is a space-based navigation system consisting of a network of 24 satellites in six different 12-hour orbital paths spaced. So that at least five of them are in view from every point on the globe \cite{kaplan2005understanding} \cite{bajaj2002gps}. With these satellites, a GPS receiver provides geographic information to them and triangulate its location on the ground, such as longitude, latitude and elevation. These receivers are using passive locating technology, by which they can receive signals without transmitting any data. It is perhaps the most widely publicized location-sensing system providing an excellent lateration framework for determining geographic positions \cite{hightower2001location}. Offered free of charge and accessible worldwide, GPS is used in plenty applications in military and general public, including aircraft tracking, vehicle navigation, robot localization, surveying, astronomy and so on. 

For a moving vehicle, it is often mounted with a GPS unit that communicates with the satellites and records position, average speed and direction of traveling information. With this kind of information, a maneuvering target tracking system becomes available and useful. This tracking system can be used to reduce the cost by knowing in real-time the current location of a vehicle, such as a truck or a bus \cite{chadil2008real}, and also be very useful for Intelligent Transportation System (ITS) \cite{mcdonald2006intelligent}. For instance, it can be used in probe cars to measure real-time traffic data to identify the congesting area. In particular, an orchardist is able to follow the trajectory and motion patterns of tractors, which are working on an orchard or vineyard, and monitor their working status with tracking system. 

No doubt that in a conventional target tracking system, the most common method is the standard Kalman filter \cite{kalman1960new}, which is a recursive solution to the discrete data linear filtering problem. The Kalman filter is a set of mathematical equations that provides an efficient computational means to estimate the state of a process in a recursive way, that minimizes the mean of the squared errors \cite{bishop2001introduction}. \cite{tusell2011kalman} gives a review of some \textit{R} packages, which are used to fit data with Kalman filter methods. Limited to its property, Kalman filter is tied up for a dynamic system, where the parameters and noise variances are unknown. In some dynamic systems, the variances are obtained based on the system identification algorithm, correlation method and least squares fusion criterion. To solve this issue, a self-tuning weighted measurement fusion Kalman filter is presented in \cite{ran2010self}. Likewise, a new adaptive Kalman filter will be another choice \cite{oussalah2001adaptive}. 

When the target maneuver occurs, Kalman filtering accuracy will be reduced or even diverged due to the model mismatch and noise characteristics that cannot be known exactly \cite{liu2014filtering}. Additionally, Kalman filter based methods require the state vector contains pre-specified coefficients during the whole approximation procedure and are within the bounded definition range determined at the beginning \cite{jauch2017recursive}. It is suggested that the smoothing spline fitting through the measured values reflecting the movements is an alternative approach. Even though fitting a univariate local linear trend model using a Kalman filter is equivalent to fitting a cubic spline, see \cite{eubank2004simple}, \cite{durbin2012time}, the latter algorithm overcomes the current limitation and can approximate data points throughout the whole process. 

 

\section{Smoothing Spline Based Reconstruction}

Starting from interpolation, the simplest way of connecting a set of sequences $(t_1,y_1),\ldots, (t_n,y_n)$ is by straight line segments. Known as piecewise
linear interpolation. Apparently, sharp turnings occur at joint knots. A smooth path is expected and more common in real life application. A single polynomial function goes through the entire interval, such as B\'ezier curve, is not as flexible as a combination of several polynomials, each of which is defined on subintervals and joint at certain knots. This kind of piecewise polynomial interpolation is called a spline. 

The core idea of splines is to augment the vector of inputs $T$ with additional variables, then use linear models in this space of derived input features. Adding constraints to construct basis functions $h_i(t), i = 1, 2,\ldots, n$, a linear basis expansion in $T$ is represented as
\begin{equation*}
f(t)=\sum_{i=1}^n \theta_i h_i(t).
\end{equation*}
The key step of a spline interpolation is the choice of basis functions. Once the $h_i(t)$ have been determined, the models are linear in the variables space. 

Several kinds of splines were introduced by \cite{esl2009}. One of them is B-splines, short for basis spline, which are constructed from polynomial pieces and joint at knots. B-spline have a closed-form expression of positions and allows continuity between the curve segments and goes through the points smoothly with ignoring the outliers, see \eg \cite{komoriya1989trajectory}, \cite{ben2004geometric}. It is flexible and has minimal support with respect to a given degree, smoothness, and domain partition. Once the knots are give, it is easy to compute B-spline recursively for any desired degree of the polynomial \cite{de1978practical} \cite{cox1982practical}. Basic simplicity of the idea is explained in \cite{dierckx1995curve} and \cite{eilers1996flexible}. The attractive feature of B-spline is its flexibility for univariate regression. For example,  \cite{gasparetto2007new} is using fifth-order B-spline to compose the overall trajectory with a set of paired data. Roughly, every spline can be represented by B-spline basis. 

Another widely used spline is piecewise cubic spline, which is continuous on interval $[a,b]$ and its first and second derivatives are continues as well \cite{wolberg1988cubic}. For example, consider the model in the following form 
\begin{equation*}
f(t)=d_i(t-t_i)^3+c_i(t-t_i)^2+b_i(t-t_i)+a_i,
\end{equation*}
for given coefficients $d_i, c_i, b_i$ and $a_i$, where $t_i\leq t\leq t_{i+1}$, $i=1,2,\ldots,n$. $f$ is a cubic spline on $[a,b]$ if (1) on each intervals $f$ is a polynomial; (2) the polynomial pieces fit together at knots $t_i$ in such a way that $f$ itself and its first and second derivatives are continuous at each $t_i$. If the second and third derivatives of $f$ are zero at 0 and 1, $f$ is said to be a natural cubic spline \cite{green1993nonparametric}. 


To find the best estimation $\hat{f}(t)$ goes through observation $y_i$, $i=1,\ldots,n$, one can use regression methods returning the least sum square errors among all the sequences. Consider a model $y_i=f(t_i)+\varepsilon_i$ with random errors $(\varepsilon_i)_{i=1}^n \sim N(0,\sigma^2)$ in space $\mathit{C}^m[a,b]$, classical parametric regression assumes that $f$ has the form $f(t,\beta)$, which is known up to the data estimated parameters $\beta$ \cite{kim2004smoothing}. When $f(t,\beta)$ is linear in $\beta$, we will have a standard linear model. 


However, a parametric approach only captures features contained in the preconceived class of functions and increases model bias \cite{yao2005functional}. To avoid this, nonparametric methods have been developed. Rather than giving specified parameters, it is desired to reconstruct $f$ from the data $y(t_i)\equiv y_i$ itself  \cite{craven1978smoothing}. The estimates of polynomial smoothing splines appear as a solution to the following minimization problem: find $\hat{f} \in \mathit{C}^m[a,b]$ that minimizes the penalized residual sum of squares: 
\begin{equation}\label{introSmoothingOb}
\mbox{RSS}=\sum_{j=1}^{n}\left(  y_j-f(t_j)\right) ^2+\lambda\int_a^b (f^{(m)})^2dt
\end{equation}
for pre-specified value $\lambda>0$ \cite{aydin2012smoothing}. In the above equation, the first term is the residual sum squares controlling the lack of fit. The second term is the roughness penalty weighted by a smoothing parameter $\lambda$, which varies from $0$ to $+\infty$ and establishes a trade-off between interpolation and straight line in the following way: 
\begin{align*}
\begin{cases}
\lambda = 0  & \mbox{$f$ can be any function that interpolates the data}\\
\lambda = +\infty & \mbox{the simple least squares line fit since no second derivative can be tolerated}\\
\end{cases}
\end{align*}\cite{esl2009}. 
So the cost of the equation (\ref{introSmoothingOb}) is determined not only by its goodness-of-fit to the data quantified by the residual sum of squares, but also by its roughness \cite{schwarz2012geodesy}. The motivation of the roughness penalty term is from a formalization of a mechanical device: if a thin piece of flexible wood, called a spline, is bent to the shape of the curve $g$, then the leading term in the strain energy is proportional to $\int f''^2$ \cite{green1993nonparametric}. 




%
%\subsection{Bayesian Estimation of Polynomial Smoothing Spline}
%
%Being bounded on a Hilbert space $C^{(m)}[0,1]$ with an inner product  $\langle f,g\rangle=\int_0^1fgdt$, \cite{wahba1978improper} showed that a Bayesian version of smoothing spline problem is to take a Gaussian process prior $f(t_i) = a_0+a_1t_i+\cdots + a_{m-1}t_i^{m-1} + x_i$, on $f$ with $x_i=X(t_i)$ being a zero mean Gaussian process whose $m$th derivative is scaled white noise, $i=1,\ldots,n$ \cite{speckman2003fully}. The extended Bayes estimates $f_\lambda$ with a "partially diffuse" prior is as exactly the same as spline solution. Some works have been done on discovering the relationship between nonparametric regression and Bayesian estimation. \cite{heckman1991minimax} shows that if $f$ the regression function $\E(y\mid f)$ has unknown prior distribution  $\mathbf{f}=(f(t_1),\ldots,f(t_n))^\top$ lying in a known class of $\Omega$, then the maximum is taken over all priors in $\Omega$ and the minimum is taken over linear estimator of $\mathbf{f}$. \cite{branson2017nonparametric} propose a Gaussian process regression method that acts as a Bayesian analog to local linear regression for sharp regression discontinuity designs. It is no doubt that one of the attractive features of the Bayesian approach is that, in principle, one can solve virtually any statistical decision or inference problem. Particularly, one can provide an accuracy assessment for $\hat{f}=\E (f\mid \mathbf{y})$ using posterior probability regions \cite{cox1993analysis}. 
%
%
%Based on the correspondence, \cite{craven1978smoothing} proposed an generalized cross-validation estimate for the minimizer $f_\lambda$. The estimate $\hat{\lambda}$ is the minimizer of the function where the trace of matrix $A(\lambda)$ in (\ref{crossvalidationmatrixA}) is incorporated. It is also able to establish an optimal convergence property for their estimator when the number of observations in a fixed interval tends to infinity \cite{wecker1983signal}. A high efficient  algorithm to optimize generalized cross-validation and generalized maximum likelihood scores with multiple smoothing parameters via the newton method was proposed by \cite{gu1991minimizing}. This algorithm can also be applied to the maximum likelihood and the restricted maximum  likelihood estimation. The behavior of the optimal regularization parameter in the method of regularization was investigated by \cite{wahba1990optimal}. 
%



\section{Parameter Selection}

As discussed in the previous section, the determination of an optimum smoothing parameter $\lambda$ in the interval $(0,+\infty)$ was found to be an underlying complication and the fundamental idea of nonparametric smoothing is to let the data choose the amount of smoothness, which consequently decides the model complexity \cite{gu1998model}. Various studies for selecting an appropriate smoothing parameter are developed and compared in literatures. Most methods focus on data driven criteria, such as cross validation (CV), generalized cross-validation (GCV) \cite{craven1978smoothing} and generalized maximum likelihood (GML) \cite{wahba1985comparison} and recently developed methods, such as improved Akaike information criterion (AIC) \cite{hurvich1998smoothing}, exact risk approaches \cite{wand1997exact} and so on. See \eg \cite{craven1978smoothing}, \cite{hardle1988far}, \cite{hardle1990applied}, \cite{wahba1990spline},  \cite{green1993nonparametric}, \cite{cantoni2001resistant} \cite{aydin2013smoothing}.  


A classical parameter selection method is called cross-validation (CV). The idea behind this method can be traced back to 1930s \cite{larson1931shrinkage}. Because in most applications, only a limited amount of data is available. Thus an idea is to split this dataset into two sub groups, one of which is used for training the model and the other one is used to evaluating its statistical performance. The sample used in evaluation is considered as "new data" as long as data are \iid. 

A single data split yields a validation estimate of the risk, and averaging over several splits yields a cross-validation estimate \cite{arlot2010survey}. Because of the assumption that data are identically distributed, and training and validation samples are independent, CV methods are widely use in parameter selection and model evaluation. 


For example, a $k$-fold CV splits the data into $k$ roughly equal-sized parts. For the $k$th part, we fit the model to the other $k-1$ parts
of the data, and calculate the prediction error of the fitted model when
predicting the $k$th part of the data. A detailed procedure is given by \cite{wahba1975completely}: suppose we have $n$ paired data $(t_1,y_1), \ldots, (t_n,y_n)$. Run a $k$-fold CV according to the following process 
\begin{itemize}
\item 1. Remove the first data $t_1$ and last date $t_n$ from the dataset.
\item 2. Split the rest data $t_2,\ldots,t_{n-1}$ into $k$ groups by:
\begin{align*}
\mbox{Group 1} &: t_2, t_{2+k}, \cdots \\
%& \mbox{Group 2}: t_3, t_{3+k}, \cdots \\
& \vdots \\
 \mbox{Group k} &: t_{k+1}, t_{2k+1}, \cdots
\end{align*}
\item 3. Guess a value $\lambda^*$. 
\item 4. Delete the first group of data. Fit a smoothing spline to the first data $(t_1,y_t)$, the rest groups of dataset and the last data $(t_n,y_n)$ with $\lambda^*$ in step 3. Compute the sum of squared deviations of this smoothing spline from the deleted data points. 
\item 5. Delete instead the second group of data. Fit a smoothing spline to the remaining data with  $\lambda^*$. Compute the sum of squared deviations of the spline from deleted data points.
\item 6. Repeat Step 5 for the 3rd, 4th, $\cdots$, $k$th group of data.
\item 7. Add the sums of squared deviations from steps 4 to 6 and divide by $k$ to achieve a cross-validation score of $\lambda^*$.
\item 8. Vary  $\lambda$ systematically and repeat steps 4-7 until CV shows a minimum.
\end{itemize}
Mathematically, we denote the CV score as 
\begin{equation*}
\mbox{CV}(\hat{f},\lambda) = \frac{1}{n}\sum_i^n (y_i -\hat{f}^{-k(i)}(t_i,\lambda)^2,
\end{equation*}
where $\hat{f}^{-k}(t)$ denotes the fitted function computed with the $k$th part of the data removed. Typical choices for $k$ are 5 and 10 \cite{esl2009}.  The function $\mbox{CV}(\hat{f},\lambda)$ provides an estimate of the test error curve, and we find the tuning parameter $\lambda$ that minimizes it will be the optimal solution. 

A special case of $k$-fold CV is setting $k=n$, which is known as leave-one-out CV. In this scenario, the CV function takes each of the data out and calculate the errors between $\hat{f}^{(-i)}$ and the remaining $n-1$ data. In fact, the property that by taking one point out doesn't affect the estimation of a smoothing spline allows us to implement CV methods without hesitation. 

As an improvement of CV, a GCV algorithm was proposed to calculate the trace of the estimation matrix $A(\lambda)$ instead of calculating individual element for linear fitting under squared error loss, in which way it provides further computational savings. Suppose we have $\hat{f}=A(\lambda)y$ with a given $\lambda$, for many linear fittings, the CV score is 
\begin{equation*}
\mbox{CV} = \frac{1}{n}\sum_{i=1}^{n} \left( y_i - \hat{f}^{(-i)}(t_i)\right)^2 = \frac{1}{n} \sum_{i=1}^{n}\left( \frac{y_i-\hat{f}(t_i)}{1-A_{ii}}  \right)^2, 
\end{equation*}
where $A_{ii}$ is the $i$th diagonal element of $A(\lambda)$. Then the GCV approximation score is 
\begin{equation*}
\mbox{GCV} =\frac{1}{n} \sum_{i=1}^{n}\left( \frac{y_i-\hat{f}(t_i)}{1-\tr(A)/n}  \right)^2.
\end{equation*}
In smoothing problems, GCV can also alleviate the tendency of cross-validation to under-smooth \cite{esl2009}. 


%Almost every technique found in the scientific literature on the reconstruction and trajectory planning problem is based on the optimization of some parameters or some objective function, such as equation (\ref{introSmoothingOb}), \cite{gasparetto2007new}. The most significant optimality criteria are: 
%\begin{itemize}
%\item[1] minimum execution time,
%\item[2] minimum energy (or actuator effort),
%\item[3] minimum jerk.
%\end{itemize}
%Besides the aforementioned approaches, some hybrid optimality criteria have also been proposed (e.g. time energy optimal trajectory planning).
%


Rather than $\lambda$ being a constant number, a new challenge is posed that the smoothing parameter becomes a function $\lambda(t)$ and is varying in domains. The structure of this penalty function controls the complexity on each domain and the whole final model.  \cite{donoho1995wavelet} introduced adaptive splines and a method to calculate piecewise parameters and \cite{liu2010data} gives an improved formula for this method. \cite{liu2010data} proposed an approximation to the penalty function with an indicator and extended the generalized likelihood to the adaptive smoothing spline. This will be another interesting research topic. 





%Craven and Wahba [5], Hardle [8], Hardle, Hall and Marron
%[9], Wahba [26], Hurvich, et al. [11], Eubank [6], Lee and Solo [17], Hastie and Tibshirani
%[10], Schimek [22], Cantoni and Ronchetti [4], Ruppert, Wand and Carroll [21], Lee [15, 16],
%and Kou [12] supplement on the selection of the smoothing parameter




%\textbf{Temporally put here }Due to the noise generated from observation units, one can use regression methods to find the best reconstruction returning the least sum square errors among all the sequences. Consider a regression model $y_i=f(t_i)+\epsilon_i$, where $a \leq t_1 < \cdots < t_n \leq b$ and $f \in \mathit{C}^2[a,b]$ is an unknown smooth function, $(\epsilon_i)_{i=1}^n \sim N(0,\sigma^2)$ are random errors. In a classical parametric regression, $f$ is assumed having the form $f(x,\beta)$, which is known up to the data estimated parameters $\beta$ \cite{kim2004smoothing}. When $f(x,\beta)$ is linear in $\beta$, we will have a standard linear model. 




\section{Bayesian Filtering}

Smoothing spline algorithms have several advantages in inferring and characterizing planar trajectories, particularly in reconstructing. However, subject to the property that smoothing splines require the solution of a global problem that involves the entire set of points to be interpolated, it might not be suitable for on-line estimation \cite{biagiotti2013online}. Therefore, it is time to use Bayesian algorithms to implement instant/on-line estimation and prediction. 


The word "filtering" refers to the methods for estimating the state of a time-varying system, which is indirectly observed through noisy measurements. A Bayes filter is a general probabilistic approach to infer an unknown probability density function recursively over time using incoming measurements and a mathematical process model. And an optimal Bayesian filtering is using Bayesian way of formulating optimal filtering, which meets some statistical criteria, such as least mean squired errors, maximum likelihood approximation and so on \cite{chen2003bayesian} \cite{sarkka2013bayesian}.  


In a linear system, the optimal Bayesian solution coincides with the least squares solution. The successful optimal solution was given by \cite{kalman1960new},  known as Kalman filter. Consider the model 
\begin{align*}
\begin{array}{cccccccccc}\cdots &\to &x_{t-1}&\to &x_{t}&\to &x_{t+1}&\to &\cdots &{\text{truth}}\\ \cdots &&\downarrow &&\downarrow &&\downarrow &&\cdots &\\ \cdots&&y_{t-1}&&y_{t}&&y_{t+1}&&\cdots &{\text{observation}}\end{array}
\end{align*}
in which $x_t=F(x_{t-1})+\varepsilon_x$ is the true hidden state of the system and $y_t=G(x_t)+\varepsilon_y$ is the observed measurement. To estimate the filtering state $x_t$ from $y_{1:t}=\{y_1,\ldots,y_t\}$, Bayesian wants to maximize the posterior $p(x_t\mid y_{1:t})$ by marginalizing out all the previous measurements. Starting from the joint distribution of 
\begin{equation*}
p(x_t,x_{t-1},y_{1:t}) = p(y_t\mid x_t)p(x_t\mid x_{t-1})p(x_{t-1}\mid y_{1:t-1})p(y_{1:t-1}),
\end{equation*}
and $p(y_t\mid x_t)\sim N(H_tx_t,R_t)$, in Kalman filter, by supposing the expectation $\hat{x}_{t-1}$ and variance $S_{t-1}$ of $x_{t-1}$ are known, one can find that 
\begin{equation*}
\int p(x_t\mid x_{t-1})p(x_{t-1}\mid \hat{x}_{t-1},S_{t-1})dx_{t-1} = N(x_t\mid \hat{x}_t,S_t), 
\end{equation*}
where $\hat{x}_t=F\hat{x}_{t-1}$ and $S_t=FS_{t-1}F^\top + Q_t$. More specifically, the log likelihood function is written as
\begin{align*}
\log p(x_t\mid y_{1:t}) &\propto \log p(y_t\mid x_t) + \log p(x_t\mid \hat{x}_{t-1}) \\% + const\\
                          &=\frac{1}{2}(y_t-Gx_t)^2-\frac{1}{2}(x_t-\hat{x}_{t-1})^2.  %+const.
\end{align*} 
The solution to the above equation is
\begin{equation*}
\hat{x}_t = \left(G^\top R^{-1}G+S_{t-1}^{-1}\right)^{-1}\left( G^\top R_t^{-1}y_t+S_{t-1}^{-1}\hat{x}_{t-1} \right).
\end{equation*}
Additionally, by setting $S_t^{-1} = G^\top R_t^{-1}G+S_{t-1}^{-1}$, the recursive estimation of covariance matrix is $S_t = S_{t-1} - K_t GS_{t-1}$, 
and $K_t = S_{t-1} G^\top (R_t +GS_{t-1}G^\top)^{-1}$ is named Kalman gain matrix. Consequently, the recursive estimation is 
\begin{equation}\label{KalmanEstimation}
\hat{x}_t = \hat{x}_{t-1}+K_t(y_t-G\hat{x}_{t-1}).
\end{equation}

Moreover, compared with filtering distribution $p(x_t\mid y_{1:t})$, the prediction distribution is trying to find the future state $x_{t+n}$ after $n$ steps from current state, which will be $p(x_{t+n}\mid y_{1:t})$, and the smoothing distribution is to find a specific state in the past steps, which is $p(x_k\mid y_{1:t})$ and $k<t$. 


Kalman Filter has limitation that it does not apply to general non-linear model and non-Gaussian distributions. For a non-linear system, one can use unscented Kalman filter \cite{wan2000unscented} and Monte Carlo filter \cite{chen2003bayesian}. The idea of an unscented Kalman filter is to create a normal distribution that approximates the result of a non-linear transformation by seeing what happens to a few carefully chosen points. It is a straightforward extension of the unscented transformation to the recursive estimation in equation (\ref{KalmanEstimation}), where the state random variable is redefined as the concatenation of the original state and noise variables. 

Monte Carlo filter is a class of Monte Carlo approaches. The power of these approaches is that they can numerically and efficiently handle integration problems. 
Sequential Monte Carlo method is using Monte Carlo approaches to estimate and compute recursively. One of the attractive merits of sequential Monte Carlo approaches lies in the fact that they allow on-line estimation by combining the powerful Monte Carlo sampling methods with Bayesian inference, at an expense of reasonable computational cost \cite{chen2003bayesian}. One of the most popular methods is particle filter, which randomly generates a cloud of points and push these points through the non-linear transformation. 

Suppose we can draw some particles $x_t^{(1)}, \ldots, x_t^{(N)}$ from the target probability distribution function $q(x)$, then these particles are used to estimate the expectation and variance of $f(x)$. The posterior distribution or density is empirically represented by a weighted sum of these samples $x_t^{(1)}, \ldots, x_t^{(N)}$ by 
\begin{align*}
\hat{p}(x_t\mid y_{1:t})=\frac{1}{N}\sum_{i=1}^N\delta (x_t-x_t^{(i)})\approx p(x_t\mid y_{1:t}).
\end{align*}
When $N$ is sufficiently large, $\hat{p}(x_t\mid y_{1:t})$ approximates the true posterior $p(x_t\mid y_{1:t})$. By this approximation, the filtering problem becomes to get the expectation of current status
\begin{align*}
\E(f(x_t)) &\approx \frac{1}{N}\sum_{i=1}^Nf(x_t^{(i)}).
\end{align*}
The expectation is the mean of the status of all particles $x_t^{(1)}, \ldots, x_t^{(N)}$. Monte Carlo method helps us to draw samples $\{x_t^{(i)}\}_{i=1}^N$ from $q(x_t\mid y_{1:t})$ and get their expectation, which approximate by
\begin{align*}
\E(f(x_t)) &\approx \sum_{i=1}^{N} \tilde{W}_t(x_t^{(i)})f(x_t^{(i)}),
\end{align*}
where $\tilde{W}_t(x_t^{(i)}) = \frac{ W_t(x_t^{(i)})}{\sum_{i=1}^NW_t(x_t^{(i)})}$ is factorized weight. Each particles has its own weighted value, so the expectation is a weighted mean. A smarter way is to update $W_t^{(i)}$ recursively in the form of
\begin{align*}
W_t^{(i)} &\propto W_{t-1}^{(i)} \frac{ p(y_{1:t}\mid x_{0:t}^{(i)}) p(x_{t}^{(i)}\mid x_{t-1}^{(i)}) }   {q(x_{t}^{(i)}\mid x_{0:t-1}^{(i)},y_{t})}.
\end{align*}
To keep particles the same size, replace the low weights particles with new ones by 
\begin{align*}
p(x_t\mid y_{1:t})=\sum_{i=1}^Nw_t^{(i)} \delta (x_t-x_t^{(i)}).
\end{align*}
After re-sampling process, it becomes
\begin{align*}
\tilde{p}(x_t\mid y_{1:t})=\sum_{j=1}^N\frac{1}{N} \delta (x_t-x_t^{(i)}).= \sum_{i=1}^N\frac{n_i}{N}\delta (x_t-x_t^{(i)}).,
\end{align*}
where $n_i$ represents how many times the new particles $x_t^{(j)}$ were duplicated from $x_t^{(i)}$. 


However, this sampling and re-sampling methods may cause samples degeneracy, in which way the samples are not representative. The improvements of particle filter's performance have been devoted by \cite{carpenter1999improved}, \cite{godsill2001maximum}, \cite{stavropoulos2001improved}, \cite{smcmip2011}. 


Nowadays, Bayesian filtering has become a broad topic involving many scientific areas that a comprehensive survey and detailed treatment seems crucial to cater the ever growing demands of understanding this important field for many novices, though it is noticed by the author that in the literature there exist a number of excellent tutorial papers on particle filters and Monte Carlo filters \cite{chen2003bayesian} \cite{doucet2000sequential} \cite{doucet2000rao} \cite{chen2012monte}.  
 
 

\section{Outline of Thesis}

The structure of this thesis is organized in the following order. In chapter \ref{ChapterTS}, I propose an adaptive smoothing spline method based on \textit{Hermite} spline basis functions to obtain a reconstruction of $f$ and $f'$ from noisy data $\mathbf{y}$ and $\mathbf{v}$. Rather than only using residuals of $f(t_i)-y_i$, residuals of $f'(t_i)-v_i$ with a new parameter $\gamma$ are included. Numerical simulation and real data implementation are given after theoretical methodology. In chapter \ref{ChapterGPR}, I give the Bayesian estimation form of the Tractor spline in a second-derivative-piecewise-continuous reproducing kernel Hilbert space $\mathcal{C}_{\mbox{p.w.}}^2[0,1]$. An extended GCV is used to find the optimal parameters for Tractor spline. In chapter \ref{ChapterFR}, I will give a brief review on existing methods for sequential state and parameter inference, including concepts and popular algorithms on estimating states sequentially and some algorithms that can estimate both unknown parameters and states simultaneously. In chapter \ref{ChapterMCMC}, an Adaptive Delayed-Acceptance Metropolis-Hastings algorithm is proposed to estimate the posterior distribution for combined states and parameters. To keep a higher running efficiency for the algorithm, a sliding window approach, in which historical data will be cut off when a new observation comes into the data stream,  is used to improve the sampling efficiency. Theorems proofs, equation calculations and part of simulation results are illustrated in appendices. 







