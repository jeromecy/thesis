
%% % % %第一段： 提出并介绍这个问题
%The problem in the world I explore in this paper is....... 
%The reason why it is an important problem is......
%
%% % % %第二段：找出解决问题的视角
%The lends I adopt to explore this problem is.... it is a useful lens because..........
%
%% % % %第三段：描述解决问题的场景和发现Analyzing a dataset of xxxxxxxxxxx, 
%I reach the following findings.......
%
%% % % %第四段：介绍你的贡献
%This study contributes to.........

%Inference and Characterization of Planar Trajectories
%Inference and Characterization of Planar Trajectories
%Inference and Characterization of Planar Trajectories


\section{Background}


The Global Positioning System (abbreviated as GPS) is a space-based navigation system consisting of a network of 24 satellites in six different 12-hour orbital paths spaced. So that at least five of them are in view from every point on the globe \cite{kaplan2005understanding} \cite{bajaj2002gps}. With these satellites, a GPS receiver provides geographic information to them and triangulate its location on the ground, such as longitude, latitude and elevation. These receivers are using passive locating technology, by which they can receive signals without transmitting any data. It is perhaps the most widely publicized location-sensing system providing an excellent lateration framework for determining geographic positions \cite{hightower2001location}. Offered free of charge and accessible worldwide, GPS is used in plenty applications in military and general public, including aircraft tracking, vehicle navigation, robot localization, surveying, astronomy and so on. 

For a moving vehicle, it is often mounted with a GPS unit that communicates with the satellites and records position, average speed and direction of traveling information. With this kind of information, a maneuvering target tracking system becomes available and useful. This tracking system can be used to reduce the cost by knowing in real-time the current location of a vehicle, such as a truck or a bus \cite{chadil2008real}, and also be very useful for Intelligent Transportation System (ITS) \cite{mcdonald2006intelligent}. For instance, it can be used in probe cars to measure real-time traffic data to identify the congesting area. In particular, an orchardist is able to follow the trajectory and motion patterns of tractors, which are working on an orchard or vineyard, and monitor their working status with tracking system. 

No doubt that in conventional target tracking, the most common method is the standard Kalman filter \cite{kalman1960new}, which is a recursive solution to the discrete data linear filtering problem. The Kalman filter is a set of mathematical equations that provides an efficient computational means to estimate the state of a process in a recursive way, that minimizes the mean of the squared error \cite{bishop2001introduction}. Fernando Tusell gave a review of some \textit{R} packages, which could be used to fit data with Kalman Filter methods \cite{tusell2011kalman}. Limited to its property, Kalman Filter is tied up for a dynamic system, where the parameters and noise variances are unknown. In some dynamic systems, the variances are obtained based on the system identification algorithm, correlation method and least squares fusion criterion. To solve this issue, a self-tuning weighted measurement fusion Kalman filter is presented in \cite{ran2010self}. Likewise, a new adaptive Kalman filter will be a good choice \cite{oussalah2001adaptive}. 

When the target maneuver occurs, Kalman filtering accuracy will be reduced or even diverged due to the model mismatch and noise characteristics that cannot be known exactly \cite{liu2014filtering}. Moreover, Kalman filter based methods require the state vector contains pre-specified coefficients during the whole approximation procedure and are within the bounded definition range determined at the beginning \cite{jauch2017recursive}. It is suggested that the smoothing spline fitting through the measured values reflecting the movements is an alternative approach. Even though fitting a univariate local linear trend model using a Kalman filter is equivalent to fitting a cubic spline, see \cite{eubank2004simple}, \cite{durbin2012time}, the latter algorithm overcomes the current limitation and can approximate data points throughout the whole process. 




\section{Smoothing Spline Based Reconstruction}

Starting from interpolation, the simplest way of connecting a set of sequences $(t_1,y_t),\ldots, (t_n,y_n)$ is by straight line segments. Known as piecewise
linear interpolation. Apparently, sharp turnings occur at joint knots. A smooth path is expected and quite common in real life application. The polynomial spline $f(t) = f_n^m(t)$ is a real-valued function on $[a,b]$ defined with the aid of $n$ knots: $-\infty \leq a <t_1 <\cdots <t_n<b\leq +\infty$. 


Several kinds of splines were introduced by \cite{esl2009}. The core idea of splines is to augment the vector of inputs $X$ with additional variables, then use linear models in this space of derived input features. Adding constraints to construct basis functions $h_i(x), i = 1, 2,\ldots, n$, a linear basis expansion in $X$ could be represented as
\begin{equation*}
f(x)=\sum_{i=1}^n \theta_i h_i(x) 
\end{equation*}
Once the basis functions $h_i(x)$ have been determined, the models are linear in the variables space. 

Smoothing spline is a basic spline and could be used in two-dimensions. Mehdi Zamani proved that in two-dimensional spaces, splines have the advantages of simplicity and less computational operations \cite{zamani2010simple}. The most important part of this method is to find the parameter $\lambda$. We have a process (function)  to calculate the $RSS$ (Residual Sum of Square), where the smallest $RSS$ is, the best $\lambda$ would be
\begin{align*}
RSS(f,\lambda)=\sum_{n=1}^{n}\{(y_i)-g(x_i)\}^2+\lambda\int g''(x_i)^2
\end{align*}
The function $g(x)$ that minimizes $RSS$ is a natural cubic spline with knots at $x_1,\ldots ,x_n$ \cite{esl2009}. 

Some ways are give to estimate the parameter \cite{wood2000modelling}, \cite{kim2004smoothing}. However, we hope that $\lambda$ could be a piecewise parameter, rather than a constant number, then we will have piecewise penalty functions. Grace Wahba introduced adaptive splines and a method to calculate piecewise parameters \cite{donoho1995wavelet} and Ziyue Liu and Wensheng Guo improved the method \cite{liu2010data}. But to get these parameters, we have to compromise to use adaptive smoothing splines. 

Using splines to fit data has been studies by many researchers. Some simple examples are given in the book \cite{esl2009}. Moreover, Fujiichi Yoshimoto offers a method can treat not only data with a smooth underlying function, but also data with an underlying function having discontinuous points and/or cusps \cite{yoshimoto2003data}. Dan Simon discussed a new type of algebraic spline, which is used to derive a filter for smoothing or interpolating discrete data points in eighth-order \cite{simon2004data}.

Based on these methods, Matthew decided to create a new spline. Temporarily, we name the new spline "Tractor Spline". This spline has some properties. This spline has two linear segments outside the knots and $n-1$ internal cubics. The  spline needs $4n = 4(n -1)+ 2\times 2$ constraints per component. These are provided by specifying the values and first derivatives at the knots. This means $2n$ constraints per component but they count twice since they constrain the spline on both sides of the knot. After that, once we know the functions of tractor spline, the position of tractor at time $t$ will be known. Moreover, we hope to integrate the details of orchard to improve predictions and begin to characterise the type of tractor trajectories at a higher level.






The observed position data set $y_i$ always comes with some errors $\varepsilon_i$, which are assumed independent Gaussian distribution with variance $\sigma_n^2$. A popular method for finding $f(x)$ that fits these data is to augment/replace the vector of inputs $\mathbf{X}$ with additional variables, which are transformations of $\mathbf{X}$, and then use linear models in this new space of derived input features.\cite{esl2009}

In regression problem, linear regression, linear discriminant analysis, logistic regression and separating hyperplanes all rely on a linear model. With the good property of linear model, easy to be interpreted and first order Taylor approximation to $f(t)$, it is more convenient to represent $f(t)$ by linear model. However, the true function $f(t)$ is unlikely to be an actual linear function in space $\mathbb{R}$. Researchers found some methods for moving beyond linearity. One of them is replacing the vector of inputs $\mathbf{T}$ with its transformations as new variables, and then use linear models in this new space of derived input features.


\textbf{Temporally put here }Due to the noise generated from observation units, one can use regression methods to find the best reconstruction returning the smallest sum square errors among all the sequences. Consider a regression model $y_i=f(t_i)+\epsilon_i$, where $a \leq t_1 < \cdots < t_n \leq b$ and $f \in \mathit{C}^2[a,b]$ is an unknown smooth function, $(\epsilon_i)_{i=1}^n \sim N(0,\sigma^2)$ are random errors. In a classical parametric regression, $f$ is assumed having the form $f(x,\beta)$, which is known up to the data estimated parameters $\beta$ \cite{kim2004smoothing}. When $f(x,\beta)$ is linear in $\beta$, we will have a standard linear model. 

Almost every technique found in the scientific literature on the trajectory planning problem is based on the optimization of some parameter or some objective function \cite{gasparetto2007new}. The most significant optimality criteria are: 
\begin{itemize}
\item (1) minimum execution time,
\item (2) minimum energy (or actuator effort),
\item (3) minimum jerk.
\end{itemize}
Besides the aforementioned approaches, some hybrid optimality criteria have also been proposed (e.g. time energy optimal trajectory planning).


\cite{mitchell1972construction} \textit{Construction of basis functions in the finite element method}

Denote by $h_m(t):\mathbb{R} \mapsto \mathbb{R}$ the $m$th transformation of $t$, $m =1, \cdots ,M$. We then model
\begin{equation}\label{introfbasis}
f(t) =\sum_{m=1}^{M}\beta_mh_m(t).
\end{equation}
a linear basis expansion of $\mathbf{t}$ in $\mathbb{R}$, where $h_m(t)$ are named basis functions, $\beta_m$ are coefficients. Once the basis functions $h_m$ have been determined, the models are linear in these new variables, and the fitting proceeds as before.

Suppose we are given observed data $t_1,t_2, \ldots, t_n$ on interval $[0,1]$, satisfying $0\leq t_1< t_2 < \cdots <t_n \leq 1$. A piecewise polynomial function $f(t)$ can be obtained by dividing the interval into contiguous
intervals $(t_1,t_2),\ldots,(t_{n-1},t_n)$, and representing $f$ by a separate polynomial in each interval. The points $t_i$ are called knots. For example,
\begin{equation}
f(t)=d_i(t-t_i)^3+c_i(t-t_i)^2+b_i(t-t_i)+a_i,
\end{equation}
for given coefficients $d_i, c_i, b_i$ and $a_i$, where $t_i\leq t\leq t_{i+1}$, $i=1,2,\ldots,n$. $f$ is a cubic spline on $[0,1]$ if (1) on each intervals $f$ is a polynomial; (2) the polynomial pieces fit together at knots $t_i$ in such a way that $f$ itself and its first and second derivatives are continuous at each $t_i$. If the second and third derivatives of $f$ are zero at 0 and 1, $f$ is said to be a natural cubic spline. These conditions are called natural boundary conditions.

Over all spline functions $f(t)$ with two continuous derivatives fitting these observed data, the curve estimate  $\hat{f}(t)$ will be defined to be the minimizer the following penalized residual sum of squares, \EDITED{edited expressions}
\begin{equation}\label{intromse}
\text{MSE}(f,\lambda)=\frac{1}{n}\sum_{i=1}^n(f(t_i)-y_i)^2+\lambda \int_{0}^{1}(f''(t))^2dt
\end{equation}
where $\lambda$ is a fixed smoothing parameter, $(t_i,y_i)$, $i=1, \cdots, n$ are observed data and $0 \leq t_1< t_2 < \cdots <t_n \leq 1$. In equation (\ref{intromse}),  the smoothing parameter $\lambda$ controls the trade-off between over-fitting and bias,
\begin{align}
\begin{cases}
\lambda = 0 : & \mbox{$f$ can be any function that interpolates the data,}\\
\lambda = \infty: & \mbox{the simple least squares line fit since no second derivative can be tolerated.}\\
\end{cases}
\end{align}

In our case, the velocity data set $v_i$ with some independent Gaussian distributed errors $\varepsilon_i \sim N(0, \frac{\sigma_n^2}{\gamma})$ are used to estimate $f(t)$ simultaneously. $f$ is a linear combination of basis functions, as shown in equation (\ref{introfbasis}), in the meantime, $f'$ is a linear combination of the first derivative of these basis functions
\begin{equation}
f'(t) =\sum_{m=1}^{M}\alpha_mh'_m(t).
\end{equation}

The velocity information  is incorporated into MSE  equation (\ref{intromse}) by the addition of velocity term $(f'(t_i)-v_i)^2$. Then it becomes 
\begin{equation}\label{mse2}
\text{MSE}(f,\lambda,\gamma)=\frac{1}{n}\sum_{i=1}^n(f(t_i)-y_i)^2+\frac{\gamma}{n} \sum_{i=1}^n(f'(t_i)-v_i)^2+\lambda \int_{0}^{1}(f''(t))^2dt,
\end{equation}
and $\hat{f}$ is the minimizer of the MSE equation (\ref{mse2}).

In the model $y=f(t)+\varepsilon$, it is reasonable to assume that the observed data $y_i$ is Gaussian distribution with mean $f(t_i)$ and variance $\sigma_n^2$. In a similar way, the velocity is estimated as  $v=f'(t)+\frac{\varepsilon}{\gamma}$, where $v_i$ is Gaussian distribution with mean $f'(t_i)$ and variance $\frac{\sigma_n^2}{\gamma}$. Then the joint distribution of $\mathbf{y},\mathbf{v},f(t)$ and $f'(t)$ is normal with zero mean and a covariance matrix, which can be estimated through Gaussian Process Regression.


Given a series of such fixes, I want to construct the most likely path taken by the tractor with some smart modeling of the movement. I then want to develop higher level characterizations of the trajectories and apply this to more general problems in curve and object recognition.



\subsection{Cross Validation}

\subsection{K-Fold Cross Validation}

Based on the procedure given by \cite{wahba1975completely}, we  follow the improved steps to calculate a K-fold cross validation. 

Step 1. Remove the first data $t_1$ and last date $t_n$ from the dataset.

Step 2. Divide dataset into k groups:
\begin{align*}
& \mbox{Group 1}: t_2, t_{2+k}, \cdots \\
& \mbox{Group 2}: t_3, t_{3+k}, \cdots \\
& \vdots \\
& \mbox{Group k}: t_{k+1}, t_{2k+1}, \cdots
\end{align*}


Step 3. Guess values of $\lambda_{down}, \lambda_{up}$ and $\gamma$.

Step 4. Delete the first group of data. Fit a smoothing spline to the first data, the rest groups of dataset and the last data, with  $\lambda_{down}, \lambda_{up}$ and $\gamma$ in step 3. Compute the sum of squared deviations of this smoothing spline from the deleted data points.

Step 5. Delete instead the second group of data. Fit a smoothing spline to the remaining data with  $\lambda_{down}, \lambda_{up}$ and $\gamma$. Compute the sum of squared deviations of the spline from deleted data points.

Step 6. Repeat Step 5 for the 3rd, 4th, $\cdots$, $k$th group of data.

Step 7. Add the sums of squared deviations from steps 4 to 6 and divide by $k$. This is the cross validation score of three parameters  $\lambda_{down}, \lambda_{up}$ and $\gamma$.

Step 8. Vary  $\lambda_{down}, \lambda_{up}$ and $\gamma$ systematically and repeat steps 4-7 until CV shows a minimum.


\section{Gaussian Process Regression}

A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution, \cite{b_gpml}.

A GP is fully defined by its mean $m(t)$ and covariance $K(s,t)$ functions as
\begin{align}
m(t)&=\mathbb{E}[f(t)] \\
K(s,t)&=\mathbb{E}[(f(s)-m(s)) (f(t)-m(t))],
\end{align}
where $s$ and $t$ are two variables, and a function $f$ distributed as such is denoted in form of
\begin{equation}
f \sim GP(m(t),K(s,t)).
\end{equation}
Usually the mean function is assumed to be zero everywhere. 

Given a set of input variables $\mathbf{T}$ for function $f(t)$ and the output $\mathbf{y}=f(\mathbf{T})+\varepsilon$ with independent identically distributed Gaussian noise $\varepsilon$ with variance $\sigma_n^2$,  we can use the above definition to predict the value of the function $f_*=f(t_*)$ at a particular input $t_*$. As the noisy observations becoming
\begin{align}\label{covdef}
\text{cov}(y_p,y_q) = K(t_p,t_q)+\sigma_n^2 \delta_{pq}
\end{align}
where $\delta_{pq}$ is a Kronecker delta which is one iff $p=q$ and zero otherwise, the joint distribution of the observed outputs $\mathbf{y}$ and the estimated output $f_*$ according to prior is
\begin{equation}
\left[ \begin{matrix}
\mathbf{y}\\
f_*
\end{matrix} \right] \sim N \left(  
0,\left[   \begin{matrix}
K(\mathbf{T},\mathbf{T}) +\sigma_n^2I& K(\mathbf{T},t_*) \\
K(t_*,\mathbf{T}) & K(t_*,t_*)
\end{matrix}  \right] 
\right).
\end{equation}
The posterior distribution over the predicted value is obtained by conditioning on the observed data
\begin{equation}
f_* | \mathbf{y},\mathbf{T},t_* \sim N(\bar{f_*},\text{cov}(f_*))
\end{equation}
where 
\begin{align}
\bar{f_*}&=\mathbb{E}[f_* | \mathbf{y},\mathbf{T},t_* ]=K(t_*,\mathbf{T})[K(\mathbf{T},\mathbf{T})+\sigma_n^2I]^{-1}\mathbf{y},\\
\text{cov}(f_*)&=K(t_*,t_*)-K(t_*,\mathbf{T})[K(\mathbf{T},\mathbf{T})+\sigma_n^2I]^{-1}K(\mathbf{T},t_*).
\end{align}

We now add velocity information $\mathbf{v}=f'(\mathbf{T})+\varepsilon'$, where $\varepsilon'$ is independent distributed Gaussian noise with variance $\frac{\sigma_n^2}{\gamma}$.

It is expected that a position point $y_i$ and velocity point $v_i$ are all effected by other points $\mathbf{y}$ and $\mathbf{v}$. So the covariance matrix for $\mathbf{y}$ and $\mathbf{v}$ is
\begin{equation}\label{covYV}
\Sigma(\mathbf{y},\mathbf{v}) = 
\left[
\begin{matrix}
\text{cov}(\mathbf{y},\mathbf{y}) & \text{cov}(\mathbf{y},\mathbf{v}) \\
\text{cov}(\mathbf{v},\mathbf{y}) & \text{cov}(\mathbf{v},\mathbf{v}) 
\end{matrix}\right],
\end{equation}
where obviously $\text{cov}(\mathbf{y},\mathbf{v}) =\text{cov}(\mathbf{v},\mathbf{y})$. Then the joint distribution  is 
\begin{equation}
\left[
\begin{matrix}
\mathbf{y}\\
\mathbf{v}
\end{matrix}
\right] \sim N(\mu_{y,v},\Sigma_{y,v}).
\end{equation}
Define $f_*$ and $f'_*$ the estimated position and velocity values at point $t_*$. From equation (\ref{covYV}) and using similar idea, it is easily to get the covariance matrices 
\begin{equation}
\begin{split}
\Sigma(f_*,\mathbf{v}) &= 
\left[
\begin{matrix}
\text{cov}(f_*,f_*) & \text{cov}(f_*,\mathbf{v}) \\
\text{cov}(\mathbf{v},f_*) & \text{cov}(\mathbf{v},\mathbf{v}) 
\end{matrix}\right],\\
\Sigma(\mathbf{y},f'_*) &= 
\left[
\begin{matrix}
\text{cov}(\mathbf{y},\mathbf{y}) & \text{cov}(\mathbf{y},f'_*) \\
\text{cov}(f'_*,\mathbf{y}) & \text{cov}(f'_*,f'_*) 
\end{matrix}\right],\\
\Sigma(f_*,f'_*) &= 
\left[
\begin{matrix}
\text{cov}(f_*,f_*) & \text{cov}(f_*,f'_*) \\
\text{cov}(f'_*,f_*) & \text{cov}(f'_*,f'_*) 
\end{matrix}\right],
\end{split}
\end{equation}
\TODO{will need to give the form of these covariances at some point. in an appendix? i think you need discussion of how $f'$ is related to $f$ for a GP}

\subsection{A Reproducing Kernel in Space $\mathbb{H}$}\label{sectionRK}

A Reproducing Kernel in Space $\mathbb{H}$.


\section{Filtering Methods}

\section{Sequential Monte Carlo Markov Chain Algorithm}


\section{Summary}




