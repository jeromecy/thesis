\section{Introduction}

Data assimilation is a sequential process, by which the observations are incorporated into a numerical model describing the evolution of this system throughout the whole process. The quality of the numerical model determines the accuracy of this system, which requires sequential combined state and parameter inferences. An enormous literature exists on pure state estimation, however, less research has been carried out on combined state and parameter estimation.

\textit{Sequential Monte Carlo} (SMC) has been well studied in scientific literature and have been applied in real world applications. It allows us to specify complex, non-linear time series patterns and enables us to perform real-time Bayesian estimations when it is coupled with \textit{Dynamic Generalized Linear Models} \citep{vieira2016online}. However, model parameters are unknown in real-world applications and this restricts the usefulness of standard SMC. Extensions to standard SMC have been considered by a number of researchers. \cite{kitagawa1998self} propose a self-organizing filter and augmenting the state vector with unknown parameters. The state and parameter are estimated simultaneously by either a non-Gaussian filter or a particle filter. \cite{liu2001combined} propose an improved particle filter to kill degeneracy, which is a common issue in static parameter estimation. They use a kernel smoothing approximation, with a correction factor to account for over-dispersion. Alternatively, \cite{storvik2002particle} propose a new filter algorithm by assuming the posterior depends on a set of sufficient statistics, which can be updated recursively. However, this approach only applies to parameters with conjugate priors \citep{stroud2018bayesian}. Unlike Storvik filter, the Particle learning approach, introduced by \cite{carvalho2010particle}, uses sufficient statistics solely to estimate parameters and promises to reduce particle impoverishment. These particle-like methods use more or less sampling and resampling algorithms to update particles recursively. 

\cite{stroud2018bayesian} propose an SMC algorithm by using ensemble Kalman filter framework for high dimensional space models with observations. Their approach combines information about the parameters from data at different time points in a formal way of Bayesian updating. \cite{polson2008practical} rely on a fixed-lag length of data approximation to filtering and sequential parameter learning in a general dynamic state-space model. This approach allows for sequential parameter learning where importance sampling has difficulties and avoids degeneracies in particle filtering. A new adaptive MCMC method yields a quick and flexible way for estimating posterior distribution in parameter estimation \citep{haario1999adaptive}. This new adaptive proposal method depends on historical data, is introduced to avoid the difficulties of tunning the proposal distribution in Metropolis-Hastings methods. 



A further question is how to find an efficient way to draw samples for $\theta$. There are a few sampling algorithms that have been discussed in literatures, such as importance sampling \citep{hammersley1964percolation, geweke1989bayesian}, rejection sampling \citep{casella2004generalized, martino2010generalized}, Gibbs sampling \citep{geman1984stochastic}, Metropolis-Hastings method \citep{metropolis1953equation, hastings1970monte} and so on. Finally, delayed acceptance MCMC has been used to speed up
computations \citep{payne2018two, quiroz2018speeding}. The main idea in delayed acceptance is to avoid computations if there is an indication that the proposed draw will ultimately be rejected.


In this chapter, an adaptive Delayed-Acceptance Metropolis-Hastings algorithm is proposed to estimate the posterior distribution for combined state and parameter with two phases. In the learning phase, a self-tuning random walk Metropolis-Hastings sampler is used to learn the parameter mean and covariance structure. In the estimation phase, the parameter mean and covariance structure informs the proposed mechanism and is also used in a delayed-acceptance algorithm, which greatly improves sampling efficiency. Information on the resulting state of the system is given by a Gaussian mixture. To keep the algorithm a higher computing efficiency for on-line estimation, it is suggested to cut off historical data and to use a fixed length of data up to the current state, like a window sliding along with time. At the end of this chapter, an application of this algorithm on irregularly sampled GPS time series data is presented. 

%
%For a generalized linear model, one may want to use Kalman Filter \citep{kalman1960new} to filter out the best state estimation from noisy signals, which is known as an optimal estimator returns a minimum mean-square error for linear model \citep{li2004recursibility}. 



\section{Bayesian Inference on Combined State and Parameter}


%In a general state-space model of the following form, either the forward map $F$ in hidden states or the observation transition matrix $G$ is linear or non-linear. We are considering the model 
%\begin{align}\label{MCMCobserY}
%\mbox{Observation:}\hspace*{0.3cm}   & y_t=G(x_t,\theta), \\
%\mbox{Hidden State:}\hspace*{0.3cm} & x_t=F(x_{t-1},\theta),\label{MCMChiddX}
%\end{align}
%where $G$ and $F$ are linear processes with Gaussian white noise $\varepsilon\sim N\left( 0,R(\theta) \right)$ and $\varepsilon'\sim N\left( 0,Q(\theta) \right)$. 

In a general state-space model, a forward map $F$ controls the stochastic evolution of the state $x_t$ and an observation model $G$ connects the observation $y_t$ to the state $x_t$. The goal of inference is to estimate the state of the system and the parameters of the forward map and the observation model. Given a probability for the initial state, $p(x_1\mid \theta)$, where $\theta$ are the parameters to be estimated, and a prior distribution over the parameters, $p(\theta)$, the general Bayesian filtering problem requires computing the posterior distribution of the current state, $p(x_t \mid y_{1:t})$. If we assume that, given $x_{t-1}$, $x_t$ is conditionally independent of states at all other times and all observations, then
\begin{equation}
p(x_t\mid y_{1:t},\theta) = \int p(x_t\mid x_{t-1},\theta)p(x_{t-1}\mid y_{1:t},\theta) dx_{t-1}, 
\end{equation}
where $y_{1:t} = \left\lbrace y_1,\dots,y_t\right\rbrace$ is the observation information up to time $t$. Then given the posterior distribution for the parameters at time $t$, $p(\theta\mid y_{1:t})$, we have 
\begin{align}\label{objecfun}
p(x_t \mid y_{1:t}) = \int p(x_t \mid y_{1:t},\theta)p(\theta\mid y_{1:t})d\theta.
\end{align}
The approach in equation \eqref{objecfun} relies on the two terms: (\romannum{1}) a conditional posterior distribution for the states with given parameters and observations; (\romannum{2}) a marginal posterior distribution for parameter $\theta$. Several methods can be used in finding the second term, such as cross validation, Expectation Maximization algorithm, Gibbs sampling, Metropolis-Hastings algorithm and so on. A Monte Carlo method is popular in research area solving this problem. Monte Carlo method is an algorithm that relies on repeated random sampling to obtain numerical results. To compute an integration of $\int f(x)dx$, one has to sample as many independent $x_i$, $(i = 1,\dots, N)$, as possible and numerically to find $\frac{1}{N}\sum_i f(x_i)$ to approximate the target function. In the target function \eqref{objecfun}, we draw samples of $\theta$ and use a numerical way to calculate its posterior distribution $p(\theta\mid y_{1:t})$. 


Additionally, the marginal posterior distribution for the parameter can be written in two different ways: 
\begin{align}\label{M1}
p(\theta \mid y_{1:t}) &\propto p(y_{1:t}\mid\theta)p(\theta),\\
p(\theta \mid y_{1:t}) &\propto p(y_t\mid y_{1:t-1}, \theta)p(\theta\mid y_{1:t-1}). \label{M2}
\end{align}
The above formula \eqref{M1} is a standard Bayesian inference requiring a prior distribution $p(\theta)$. It can be used in off-line methods, in which $\hat{\theta}$ is inferred by iterating over a fixed observation record $y_{1:t}$. By contrast, formula \eqref{M2} is defined in a recursive way over time depending on the previous posterior at time $t-1$, which is known as on-line method. $\hat{\theta}$ is estimated sequentially as a new observation $y_{t+1}$ becomes available. 


In this chapter, we propose the use of a \textit{linear} state-space model to infer the trajectory of a moving vehicle. Specifically, we suppose that the forward map and the observation model are linear and homogeneous, and the noise is Gaussian. The so-called linear Gaussian state-space model which has been extensively studied in the literature \citep{durbin2012time}. Even in the linear state-space model, parameter and state estimation is difficult. Our goal is to develop a fast and efficient MCMC algorithm for online estimation. 

% cite ref to dlm, state-space....



\subsection{The Posterior Distribution}\label{sectionlogParameter}

For sampling $\theta$, we should find its distribution function first from the covariance matrix of the joint $x_{1:t}$ and $y_{1:t}$. Under the assumption that the forward map and the observation model are linear and homogeneous, the joint distribution of the states and observations is  
\begin{equation}\label{generaljointmatrix}
\begin{bmatrix} \begin{matrix} x_{1:t}\\ y_{1:t}  \end{matrix} \biggr\rvert \theta \end{bmatrix}
\sim N\left(0, \Sigma_t \right),
\end{equation}
where $x_{1:t}$ represents the hidden states $\left\lbrace x_1,\dots,x_t\right\rbrace$, $y_{1:t}$ represents observed $\left\lbrace y_1,\dots,y_t\right\rbrace$ and $\theta$ is a set of all known and unknown parameters. The inverse of the covariance matrix $\Sigma_t^{-1}$ is the precision matrix. In our application, as we will see, it is a block matrix in the form  
\begin{equation} \Sigma_t^{-1}=
\begin{bmatrix}
A_t& -B_t \\ -B_t^\top & B_t
\end{bmatrix}, 
\end{equation}
where $A_t$ is a $t \times t$ matrix coming from the forward map, $B_t$ is a $t\times t$ diagonal matrix coming from the observation model. The structure of the matrices, such as bandwidth, sparse density, depends on the details of the model. %Temporally, we use $A$ and $B$ to stand for the $A_t$ and $B_t$ here. 
Then, we may find the covariance matrix by calculating the inverse of the precision matrix 
\begin{equation}
\begin{split}
\Sigma_t &= \begin{bmatrix}
\left(A_t-B_t^\top B_t^{-1}B_t\right) ^{-1} & -\left(A_t-B_t^\top B_t^{-1}B_t\right)^{-1}B_t^\top B_t^{-1}\\
- B_t^{-1}B_t\left(A_t-B_t^\top B_t^{-1}B_t\right)^{-1} & \left(B_t-B_t^\top A_t^{-1}B_t\right) ^{-1}
\end{bmatrix} \\
&= \begin{bmatrix}
\left(A_t-B_t\right) ^{-1} & \left(A_t-B_t\right)^{-1}\\
\left(A_t-B_t\right)^{-1} & \left(I_t- A_t^{-1}B_t\right) ^{-1}B_t^{-1}
\end{bmatrix} \\
&\triangleq \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY} \\
\Sigma_{YX}  &\Sigma_{YY} 
\end{bmatrix}.
\end{split}
\end{equation}
Because of the covariance  $\Sigma_{YY} =  \left(I_t-A_t^{-1}B_t\right)^{-1}B_t^{-1}$, therefore the inverse is 
\begin{equation}\label{inverseYY}
\Sigma_{YY}^{-1} = B_t\left(I_t-A_t^{-1}B_t\right)= B_tA_t^{-1}\Sigma_{XX}^{-1}.
\end{equation}
Given the Choleski decomposition $L_tL_t^\top = A_t$, we have
\begin{equation}
\begin{split}
\Sigma_{YY}^{-1} &=B_tL_t^{-\top}L_t^{-1}\Sigma_{XX}^{-1}\\
&=\left(L_t^{-1}B_t\right)^\top\left(L_t^{-1}\Sigma_{XX}^{-1}\right) %\\
%&=\mbox{solve}\left(L,B\right)^\top\mbox{solve}\left(L,\Sigma_{XX}^{-1}\right).
\end{split}
\end{equation}
More usefully, by given another Choleski decomposition $R_tR_t^\top=A_t-B_t=\Sigma_{XX}^{-1}$,
\begin{align}\label{sigmayy01}
%\begin{split}
%Y^\top \Sigma_{YY}^{-1} Y &= \mbox{solve}\left(L,BY\right)^\top\mbox{solve}\left(L,\Sigma_{XX}^{-1}Y\right)\\
%&\triangleq W^\top \mbox{solve}\left(L,\Sigma_{XX}^{-1}Y\right)\\
%\end{split}\\
\begin{split}
y_{1:t}^\top \Sigma_{YY}^{-1} y_{1:t} &= \left(L_t^{-1}B_ty_{1:t}\right)^\top\left(L_t^{-1}\Sigma_{XX}^{-1}y_{1:t}\right)\\
&\triangleq W_t^\top \left(L_t^{-1}\Sigma_{XX}^{-1}y_{1:t}\right)
\end{split}
\end{align}
\begin{equation}\label{sigmayy02}
\begin{split}
\det\Sigma_{YY}^{-1} &= \det B_t \det L_t^{-\top}\det L_t^{-1}\det R_t\det R_t^\top\\
&= \det B_t\left(\det L_t^{-1}\right)^2\left(\det R_t\right)^2.
\end{split}
\end{equation}
From the objective function \eqref{M1}, the posterior distribution of $\theta$ is 
\begin{equation}\label{posteriortheta}
p\left(\theta \mid y_{1:t}\right) \propto p\left(y_{1:t}\mid\theta\right)p\left(\theta\right) \propto \exp\left( -\frac{1}{2} y_{1:t} \Sigma_{YY}^{-1} y_{1:t} \right) \sqrt{\det \Sigma_{YY}^{-1}} p\left(\theta\right).
\end{equation}
Then, by taking natural logarithm on the posterior of $\theta$ and by using the useful solutions in equations \eqref{sigmayy01} and \eqref{sigmayy02}, we will have
\begin{align}\label{logposteriorL}
\ln L\left(\theta\right) &= -\frac{1}{2}y_{1:t}^\top\Sigma_{YY}^{-1}y_{1:t}+\frac{1}{2}\sum\ln\mbox{tr}\left(B_t\right)-\sum\ln\mbox{tr}\left(L_t\right)+\sum\ln\mbox{tr}\left(R_t\right) + \ln p\left(\theta\right).
\end{align}



\subsection{The Forecast Distribution}\label{sectionforecast}

From equation \eqref{M2}, a sequential way for estimating the forecast distribution is needed. Suppose it is 
\begin{equation}
y_{t}\mid y_{1:t-1},\theta \sim N\left( \bar{\mu}_{t},\bar{\sigma}_{t} \right). 
\end{equation}
Look back to the covariance matrices of observations that we found in the previous section 
\begin{equation}
\begin{split}
p(y_{1:t-1},\theta) &= N\left( 0,\Sigma_{YY}^{(t-1)} \right),\\
p(y_{t},y_{1:t-1},\theta) &= N\left( 0,\Sigma_{YY}^{(t)} \right),
\end{split}
\end{equation}
where the covariance matrix of the joint distribution is $\Sigma_{YY}^{(t)} = (I_{t}-A_{t}^{-1}B_{t})^{-1}B_{t}^{-1}$, $I_t$ is a $t\times t$ identity matrix. Then, by taking its inverse, we will get 
\begin{equation}
\begin{split}
\Sigma_{YY}^{(t) (-1)} &= B_{t}(I_{t}-A_{t}^{-1}B_{t}) \\
&= B_{t}(B_{t}^{-1}-A_{t}^{-1})B_{t} \\
&\triangleq \begin{bmatrix} 
B_t & 0 \\ 0 & B_1 \end{bmatrix}
\begin{bmatrix} 
Z_{t} & b_{t} \\
b_{t}^\top & K_{t}
\end{bmatrix} \begin{bmatrix} 
B_t & 0 \\ 0 & B_1\end{bmatrix}
\end{split}
\end{equation}
where $Z_{t}$ is a $t \times t$ matrix, $ b_{t} $ is a $t \times 1$ matrix and $K_{t}$ is a $1 \times 1$ matrix. Thus, by taking its inverse again, we will get 
\begin{equation} \Sigma_{YY}^{(t)}= \left[ \begin{matrix}
B_t^{-1} \left(Z_{t}-b_{t}K_{t}^{-1}b_{t}^\top\right)^{-1}B_t^{-1}  & - B_t^{-1}  Z_{t}^{-1}b_{t}\left(K_{t}-b_{t}^\top Z_{t}^{-1}b_{t}\right)^{-1}B_1^{-1} \\
-B_1^{-1}  K_{t}^{-1}b_{t}^\top \left(Z_{t}-b_{t}K_{t}^{-1}b_{t}^\top\right)^{-1}B_t^{-1}  & B_1^{-1}  \left(K_{t}-b_{t}^\top Z_{t}^{-1}b_{t}\right)^{-1}B_1^{-1} 
\end{matrix}\right].
\end{equation}
So, from the above covariance matrix, we can find the mean and variance of $p\left(y_{t}\mid y_{1:t-1},\theta\right)$ are 
\begin{align}
\bar{\mu}_{t} & =  B_1^{-1}K_{t}^{-1}b_{t}^\top B_{t-1}^{-1}y_{1:t-1} ,\\
\bar{\sigma}_{t}^2 & =  B_1^{-1}K_{t}B_1^{-1}  .
\end{align}




\subsection{The Estimation Distribution}\label{generalEstDistr}

From the joint distribution \eqref{generaljointmatrix}, one can find the best estimation with a given $\theta$ by
\begin{equation}\label{estimationdistribution}
\begin{split}
x_{1:t} \mid y_{1:t},\theta &\sim N \left( A_{t}^{-1}B_{t}y_{1:t}, A_{t}^{-1} \right) \\
&\sim N(L_t^{-\top}L_t^{-1}B_{t}y_{1:t-1},L^{-\top}L_t^{-1})\\
&\sim N(L_t^{-\top}W_t,L^{-\top}L_t^{-1}).
\end{split}
\end{equation}
%Consequently 
%\begin{align}
%x_{1:t} = L_t^{-\top}(W_t+Z_t),
%\end{align}
%where $Z_t \sim N(0, I(\varepsilon)_{t})$ is independent and identically distributed and drawn from a zero-mean normal distribution with variance $I(\varepsilon)_{t}$. 

For sole $x_{t}$, its joint distribution with $y_{1:t}$ is 
\begin{equation}\label{joinedXYgiventheta}
x_{t}, y_{1:t}\mid \theta \sim N\left( 0, \begin{bmatrix}
C_{t}^\top(A_{t}-B_{t}) ^{-1}C_{t} & C_{t}^\top (A_{t}-B_{t})^{-1}\\
(A_{t}-B_{t})^{-1}C_{t} & (I_t- A_{t}^{-1}B_{t}) ^{-1}B_{t}^{-1}
\end{bmatrix} \right),
\end{equation}
where $C_t^\top = \begin{bmatrix}0 & \cdots & 0 & 1\end{bmatrix}$ is $t\times 1$ vector. % that helps to extract the last element in the matrix. 
Thus, the filtering distribution of the state is 
\begin{equation}
x_{t}\mid y_{1:t},\theta \sim N\left( \mu_{t}^{(x)},\Var(x_{t}) \right),
\end{equation}
where, after simplifying, the mean and variance are  
\begin{align}\label{generalmux}
\mu_{t}^{(x)} & = C_{t}^\top A_{t}^{-1}B_{t}y_{1:t} ,\\
\Var(x_{t})& =C_{t}^\top A_{t}^{-1}C_{t}. \label{generalSigx}
\end{align}

Generally, researchers would like to find the combined estimation for $x_t$ and $\theta$ at time $t$ by
\begin{equation}
p(x_t, \theta \mid y_{1:t}) = p(x_t\mid y_{1:t},\theta)p(\theta\mid y_{1:t}).
\end{equation}
Differently, from the target equation \eqref{objecfun}, the state inference containing $N$ samples is a mixture Gaussian distribution in the following form 
\begin{equation}\label{mixtureGaussian}
p(x_t \mid y_{1:t}) = \int p(x_t\mid y_{1:t},\theta) p(\theta\mid y_{1:t})d\theta \dot{=} \frac{1}{N}\sum_{i=1}^{N}p\left(x_{t}\mid\theta^{(i)},y_{1:t}\right). 
\end{equation}
Suppose $x_t\mid y_{1:t},\theta_i \sim N\left( \mu_{ti}^{(x)},\Var(x_{ti}) \right)$ is found from equation \eqref{generalmux} and \eqref{generalSigx} for each $\theta_i$, then its mean is 
\begin{equation}\label{mixturemean}
\mu_t^{(x)} = \frac{1}{N} \sum_i \mu_{ti}^{(x)} 
\end{equation}
and  the unconditional variance of $x_t$, by law of total variance, is 
\begin{equation}\label{mixturevariance}
\begin{split}
\Var(x_t) &= \E\lbrack \Var(x_t\mid y_{1:t},\theta)\rbrack + \Var\lbrack \E(x_t\mid y_{1:t},\theta)\rbrack \\
&= \frac{1}{N} \sum_i \left( \mu_{ti}^{(x)}  \mu_{ti}^{(x)\top} +\Var(x_{ti})\right) -\frac{1}{N^2} \left(  \sum_i  \mu_{ti}^{(x)} \right) \left( \sum_i \mu_{ti}^{(x)} \right) ^\top.
\end{split}
\end{equation}

\section{Random Walk Metropolis-Hastings Algorithm}

Metropolis-Hastings algorithm is an important class of MCMC algorithms \citep{smith1993bayesian, tierney1994markov, gilks1995markov}.  This algorithm has been used extensively in physics but was little known to others until  \cite{muller1991generic, tierney1994markov} expound the value of this algorithm to statisticians. The algorithm is extremely powerful and versatile and has been included in a list of ``The Top 10 Algorithms''  with the greatest influence on the development and practice of science and engineering in the 20th century \citep{dongarra2000guest, medova2008bayesian}. 

Given essentially a probability distribution $\pi(\cdot)$ (the target distribution), MH algorithm provides a way to generate a Markov chain $x_1, x_2,\ldots, x_t$, who has the target distribution as a stationary distribution, for the uncertain parameters $x$ requiring only that this density can be calculated at $x$. Suppose that we can evaluate $\pi(x)$ for any $x$. The transition probabilities should satisfy the detailed balance condition
\begin{equation}
\pi\left(x^{(t)}\right)q\left(x', x^{(t)}\right) = \pi\left(x'\right)q\left(x^{(t)}, x'\right),
\end{equation}
which means that the transition from the current state $\pi(x^{(t)})$ to the new state $\pi(x')$ has the same probability as that from $\pi(x')$ to $\pi(x^{(t)})$. In sampling method, drawing $x_i$ first and then drawing $x_j$ should have the same probability as drawing $x_j$ and then drawing $x_i$. However, in most situations, the details balance condition is not satisfied. Therefore we introduce a function $\alpha(x,y)$ satisfying 
\begin{equation}
\pi\left(x'\right)q\left(x', x^{\left(t\right)}\right)\alpha\left(x',x^{\left(t\right)}\right) = \pi\left(x^{\left(t\right)}\right)q\left(x^{\left(t\right)}, x'\right)\alpha\left(x^{\left(t\right)},x'\right).
\end{equation}
In this way, a tentative new state $x'$ is generated from the proposal density $q\left(x';x^{\left(t\right)}\right)$ and it is accepted or rejected according to acceptance probability 
\begin{equation}\label{alphabalance}
\alpha=\frac{\pi\left(x'\right)}{\pi\left(x^{\left(t\right)}\right)}\frac{q\left(x^{\left(t\right)}, x'\right)}{q\left(x', x^{\left(t\right)}\right)}.
\end{equation}
If $\alpha \geq 1$, the new state is accepted. Otherwise, the new state is accepted with probability $\alpha$.

Here comes an issues of how to choose $q\left(\cdot\mid x^{(t)}\right)$. The most widely used subclass of MCMC algorithms is based on the \textit{random walk Metropolis} (RWM). The RWM updating scheme was first applied by \cite{metropolis1953equation} and proceeds as follows. Given a current value of the $d$-dimensional Markov chain $x^{(t)}$, a new value $x'$ is obtained by proposing a jump $\epsilon = \lvert x' - x^{(t)} \rvert  $ from the pre-specified Lebesgue density 
\begin{equation}\label{stepsizeep}
\tilde{\gamma}\left(\epsilon^\star;\lambda\right) = \frac{1}{\lambda^d}\gamma \left( \frac{\epsilon^\star}{\lambda} \right),
\end{equation}
with $\gamma(\epsilon) = \gamma(-\epsilon)$ for all $\epsilon$. Here, the positive $\lambda$ governs the overall distance of the proposed jump and plays a crucial role in determining the efficiency of any algorithm. In a random walk, the proposal density function $q(\cdot)$ can be chosen for some suitable normal distribution, and hence $q\left(x'\mid x^{\left(t\right)}\right)=N\left(x'\mid x^{\left(t\right)},\epsilon^2\right)$ and $q\left(x^{\left(t\right)}\mid x'\right)=N\left(x^{\left(t\right)}\mid x',\epsilon^2\right)$ cancel in the above equation \eqref{alphabalance} \citep{sherlock2016adaptive}. To decide whether to accept the new state, we compute the the probability of accepting the new state by 
\begin{equation}
\alpha=\min \left\lbrace 1,\frac{\pi\left(x'\right) q\left( x^{\left(t\right)}\mid x'\right) }{\pi\left(x^{\left(t\right)}\right)  q\left( x'\mid x^{\left(t\right)} \right) }  \right\rbrace= \min \left\lbrace 1,\frac{\pi\left(x'\right)  }{\pi\left(x^{\left(t\right)}\right) }  \right\rbrace.
\end{equation}
If the proposed value is accepted it becomes the next current value $x^{(t+1)}= x'$; otherwise the current value is left unchanged $x^{(t+1)} = x^{(t)}$ \citep{sherlock2010random}. 


\subsection{The Self-tuning Metropolis-Hastings Algorithm}

The self-tuning MH algorithm automatically tunes the step sizes for different parameters by one-variable-at-a-time random walk. Aiming at the target acceptance rates for each parameter, the algorithm efficiently and accurately explore the structure of the $d$-dimensional parameter space. 

By assuming the parameters are independent, the idea of this algorithm is that in each iteration, only one parameter is proposed and the others remain to be changed. After the step, take $n$ samples out of the total amount of iterations $N$ as new sequences. In Figure \ref{randomwalk}, examples of different proposing methods are compared. 
\begin{figure}[h]
\centering
 \begin{subfigure}[b]{0.32\textwidth}
 \begin{tikzpicture}
     \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggoneRW_Final.pdf}};
     \begin{scope}[
         x={(image.south east)},
         y={(image.north west)}
     ]
     \node [black, font=\bfseries] at (0.5,-0.05) {$x$};
     \node [black, font=\bfseries] at (-0.05,0.5) {$y$};
     \end{scope}
 \end{tikzpicture}
  \caption{\footnotesize One-variable-at-a-time random walk.}\label{MCMConevariableRW}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\begin{tikzpicture}
      \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggindRW_Final.pdf}};
      \begin{scope}[
          x={(image.south east)},
          y={(image.north west)}
      ]
      \node [black, font=\bfseries] at (0.5,-0.05) {$x$};
      %\node [black, font=\bfseries] at (-0.05,0.5) {$y$};
      \end{scope}
  \end{tikzpicture}
    \caption{\footnotesize Independent multi-variable-at-a-time random walk.}\label{MCMCMultivariableRW}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
  \begin{tikzpicture}
      \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggcorRW_Final.pdf}};
      \begin{scope}[
          x={(image.south east)},
          y={(image.north west)}
      ]
      \node [black, font=\bfseries] at (0.5,-0.05) {$x$};
      %\node [black, font=\bfseries] at (-0.05,0.5) {$y$};
      \end{scope}
  \end{tikzpicture}
  \caption{\footnotesize Correlated multi-variable-at-a-time random walk.}\label{MCMCCorrelatedRW}
\end{subfigure}
\caption{Examples of 2-Dimensional random walk Metropolis-Hastings algorithm. Figure \ref{MCMConevariableRW} is the trace of one-variable-at-a-time random walk. At each time, only one variable is changed and the other one stay constant. Figure \ref{MCMCMultivariableRW} and \ref{MCMCCorrelatedRW} present the traces by multi-variable-at-a-time random walk. In Figure \ref{MCMCMultivariableRW}, the proposal for each step is independent, but in Figure \ref{MCMCCorrelatedRW} the proposal are proposed correlated.}
\label{randomwalk}
\end{figure}

To gain the target acceptance rates $\alpha_i$, $(i = 1, \dots, d)$, the step size $s_i$ for each parameter is tuned automatically. The concept of the algorithm is if the proposal is accepted, we are more confident on the direction and step size that were made. In this scenario, the next moving step should be further. In another word, the step size $s_{t+1}$ in the next step is bigger than $s_t$. Otherwise, a conservative proposal is made with a shorter distance, which is $s_{t+1}\leq s_t$. 

Let $a$ and $b$ be non-negative numbers indicating the distances of a forward movement, the new step size $s_{t+1}$ from current $s_t$ is 
\begin{align}\ln s_{t+1} = 
\begin{cases}
\ln s_t + a & \mbox{with probability } \alpha \\
\ln s_t - b & \mbox{with probability } 1 - \alpha 
\end{cases},
\end{align}
where the logarithm guarantees the step size is positive. 
By taking its expectation  
\begin{align}
\E\lbrack\ln s_{t+1}\mid \ln s_t\rbrack = \alpha(\ln s_t+a) + (1-\alpha)(\ln s_t-b), 
\end{align}
and simplifying to 
\begin{align}
\mu= \alpha(\mu+a) + (1-\alpha)(\mu-b), 
\end{align}
one can find that 
\begin{equation}\label{autostepab}
a = \frac{1-\alpha}{\alpha}  b. 
\end{equation}
Thus, if the proposal is accepted, the step size $s_t$ is tuned to $s_{t+1}=s_te^a$, otherwise $s_{t+1}=s_t/e^b$. 

The complete one-variable-at-a-time MH is summarized in the following: 
\begin{algorithm}[h]
Initialization: Given an arbitrary positive step size $s_i^{(1)}$ for each parameter. Set up a value for $b$ and find $a$ by using the formula \eqref{autostepab}. Set up a target acceptance rate $\alpha_i$ for each parameter, where $i = 1,\dots, d$. \\
Run sampling algorithm: \For{$k$ from 1 to $N$}{
Randomly select a parameter $\theta_i^{(k)}$, propose a new one by $\theta_i'\sim N\left(\theta_i^{(k)}, \epsilon s_i^{(k)}\right)$ and leave the rest unchanged.\label{stRWMHselect}\\
Accept $\theta_i'$ with probability $\alpha=\min\left\lbrace  1,\frac{\pi\left(\theta'\right)q\left(\theta^{\left(k\right)},\theta'\right)}{\pi\left(\theta^{\left(k\right)}\right)q\left(\theta', \theta^{(k)}\right)}  \right\rbrace$. \\
If it is accepted, tune step size to $s_i^{(k+1)}=s_i^{(k)}e^a$, otherwise $s_i^{(k+1)}=s_i^{(k)}/e^b$. \\
Set $k=k+1$ and move to step \ref{stRWMHselect} until $N$.
}
Take $n$ samples out from $N$ with equal spaced index for each parameter being a new sequence. 
\caption{Self-tuning Random Walk Metropolis-Hastings Algorithm}\label{algoonevarible}
%\caption{One-variable-at-a-time Metropolis-Hastings Sampling Algorithm.}
\end{algorithm}


The advantage of the Algorithm \ref{algoonevarible} is that it returns a more accurate estimation for $\theta$ and is more reliable to learn the structure of the parameter space. However, if $\pi(\cdot)$ has an singular structure, the algorithm becomes time-consuming and low efficient. To solve the issue, the \textit{Delayed-Acceptance Metropolis-Hastings} (DA MH) algorithm is utilized to speed up the computation.





\subsection{Adaptive Delayed-Acceptance Metropolis-Hastings Algorithm}

The DA MH algorithm proposed in \citep{christen2005markov} is a two-stage Metropolis-Hastings algorithm in which, typically, proposed parameter values are accepted or rejected at the first stage based on a computationally cheap surrogate $\hat{\pi}(x)$ for the likelihood $\pi(x)$. In stage one, the quantity $\alpha_1$ is found by a standard MH acceptance formula 
\begin{equation}
\alpha_1=\min\left\lbrace  1,\frac{\hat{\pi}(x')q\left(x^{(t)}, x'\right)}{\hat{\pi}(x^{(t)})q\left(x', x^{(t)}\right)}  \right\rbrace ,
\end{equation}
where $\hat{\pi}(\cdot)$ is a cheap estimation for $x$ and a simple form is $\hat{\pi}(\cdot)=N\left(\cdot\mid \hat{x},\epsilon\right)$. Once $\alpha_1$ is accepted, the process goes into stage two and the acceptance probability $\alpha_2$ is
\begin{equation}\label{dahalpha2}
\alpha_2=\min \left\lbrace  1,\frac{\pi(x')\hat{\pi}\left(x^{(t)}\right) }{\pi\left(x^{(t)}\right)\hat{\pi}(x')} \right\rbrace,
\end{equation}
where the overall acceptance probability $\alpha_1\alpha_2$ ensures that detailed balance is satisfied with respect to $\pi(\cdot)$; however if a rejection occurs at stage one, the expensive evaluation of $\pi(x)$ at stage two is unnecessary.

For a symmetric proposal density kernel $q\left(x', x^{(t)}\right)$ such as is used in the random walk MH algorithm, the acceptance probability in stage one is simplified to
\begin{equation} \label{dahalpha1}
\alpha_1= \min \left\lbrace 1,\frac{\pi(x')}{\pi\left(x^{(t)}\right)}  \right\rbrace.
\end{equation}
If the true posterior is available, the delayed-acceptance Metropolis-Hastings algorithm is obtained by substituting this for the unbiased stochastic approximation in \eqref{dahalpha2} \citep{sherlock2015efficiency}.


To accelerate the MH algorithm, DA MH requires a cheap approximate estimation $\hat{\pi}(\cdot)$ in formula \eqref{dahalpha1}. Intuitively, the approximation should be efficient with respect to time and accuracy to the true posterior $\pi(\cdot)$. A sensible option is assuming the parameter distribution at each time $t$ is following a normal distribution with mean $m_t$ and covariance $C_t$. So the posterior density is given by 
\begin{equation}
\hat{\pi}(\theta\mid y_{1:t}) \propto \exp\left( -\frac{1}{2}(\theta-m_t)^\top C_t^{-1}(\theta-m_t)\right). 
\end{equation}
A lazy $C_t$ is chosen as an identity matrix, in which way all the parameters are independent. In terms of $m_t$, in most of circumstances, 0 is not an idea choice. To find an optimal or suboptimal $m_t$ and $C_t$, several algorithms have been discussed. \cite{stroud2018bayesian} use a second-order expansion of $l(\theta)$ at the mode and the mean and covariance become $m_t=\arg \max l(\theta)$ and $C_t = - \left[ \frac{\partial l(\theta)}{\partial \theta_i \partial \theta_j} \right]_{\theta=m_t}^{-1}$ respectively. The drawback of this estimation is a global optimum is not guaranteed. \cite{mathew2012bayesian} propose a fast adaptive MCMC sampling algorithm, which is a consist of two phases. In the learning phase, they use hybrid Gibbs sampler to learn the covariance structure of the variance components. In phase two, the covariance structure is used to formulate an effective proposal distribution for a MH algorithm. 


Likewise, we are suggesting that use a batch of data with length $L<t$ to learn the parameter space by using self-tuning random walk MH algorithm in the learning phase first. This algorithm tunes each parameter at its own optimal step size and explores the surface in different directions. When the process is done, we have a sense of the surface for $\theta\approx\hat{\theta}$ and its mean $\hat{\mu}\approx m_L$ and covariance $\hat{\Sigma}\approx C_L$ can be estimated. Then, we can move to the second phase: DA MH algorithm. The new $\theta'$ is proposed from  $N\left(\theta^{(t)}\mid m_L,C_L\right)$, which is in the following form 
\begin{equation}
\theta' = \theta^{(t)} + R\epsilon z,
\end{equation}
where $R^\top R = C_L$ is the Cholesky decomposition, $\epsilon$ is the tuned step size and $z\sim N(0,1)$ is Gaussian white noise. This proposing method reduces the impact of drawing $\theta'$ from a correlation space. 


%Adaptive Metropolis-Hastings algorithm was introduced in \citep{haario1999adaptive}. Because the choice of a suitable MCMC method and its proposal are crucial for the convergence of the Markov chain. 




\subsection{Efficiency of Metropolis-Hastings Algorithm}\label{effMHA}

In equation \eqref{stepsizeep}, the jump size $\epsilon$ determines the efficiency of RWM algorithm. For a general RWM, it is intuitively clear that we can make the algorithm arbitrarily poor by making $\epsilon$ either very large or very small \citep{sherlock2010random}. Assuming $\epsilon$ is extremely large, the proposal $x'\sim N\left(x^{(t)},\epsilon\right)$, for example, is taken a further distance from current value $x^{(t)}$. Therefore the algorithm will reject most of its proposed moves and stay where it was for a few iterations. On the other hand, if $\epsilon$ is extremely small, the algorithm will keep accepting the proposed $x'$ since $\alpha$ is always approximately be 1 because of the continuity of $\pi(x)$ and $q(\cdot)$ \citep{roberts2001optimal}. Thus, RWM takes a long time to explore the posterior space and converge to its stationary distribution. So, the balance between these two extreme situations must exist. This appropriate step size $\hat{\epsilon}$ is optimal, sometimes is suboptimal, the solution to gain a Markov chain. 

Figure \ref{largesmallstepsize} illustrates the performances of RWM with different $\epsilon$. From these figures, one can see that either too large or too small $\epsilon$ causes high correlation chains, indicating bad samples in sampling algorithm. An appropriate $\epsilon$ decorrelates samples and returns a stationary chain. That is considered highly efficient. 


%\begin{figure}[h]
%\centering
% \begin{subfigure}[b]{0.3\textwidth}
%     \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/largechain.pdf}
%     \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/largeacf.pdf}
%     \caption{With a large step size.}
%\end{subfigure}
%\begin{subfigure}[b]{0.3\textwidth}
%    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/smallchain.pdf}
%    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/smallacf.pdf}
%    \caption{With a small step size.}
%\end{subfigure}
%\begin{subfigure}[b]{0.3\textwidth} \
%    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/bestchain.pdf}
%    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/bestacf.pdf}
%    \caption{With an appropriate step size.}
%\end{subfigure}
%\caption{Metropolis algorithm sampling for a single parameter with (a) a large step size, (b) a small step size, (c) an appropriate step size. The upper plots show the sample chain and lower plots indicate the autocorrelation for each case.}
%\label{largesmallstepsize}
%\end{figure}


\begin{figure}[h]
\centering
 \begin{subfigure}[b]{0.32\textwidth}
   \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/gglargechain_Final.pdf}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/gglargeacf_Final.pdf}
     \caption{With a large step size}\label{MCMClargestep}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggsmallchain_Final.pdf}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggsmallacf_Final.pdf}
    \caption{With a small step size}\label{MCMCsmallstep}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggbestchain_Final.pdf}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggbestacf_Final.pdf}
    \caption{With a proper step size}\label{MCMCproperstep}
\end{subfigure}
\caption{Metropolis-Hastings sampler for a single parameter with: \ref{MCMClargestep} a large step size, \ref{MCMCsmallstep} a small step size, \ref{MCMCproperstep} an appropriate step size. The upper plots show the sample chains and lower plots indicate the autocorrelation values for each case.}
\label{largesmallstepsize}
\end{figure}


Plenty of work has been done in determining the efficiency of Metropolis-Hastings algorithm in recent years. \cite{gelman1996efficient} work with algorithms consisting of a single Metropolis move (not multi-variable-at-a-time), and obtain many interesting results for the $d$-dimensional spherical multivariate normal problem with symmetric proposal distributions, including that the optimal scale is approximately $2.4/\sqrt{d}$ times the scale of target distribution, which implies optimal acceptance rates of $0.44$ for $d = 1$ and $0.23$ for $d\rightarrow \infty$ \citep{gilks1995markov}. \cite{roberts2001optimal} evaluate scalings that are optimal (in the sense of integrated autocorrelation times) asymptotically in the number of components. They find that an acceptance rate of 0.234 is optimal in many random walk Metropolis situations, but their studies are also restricted to algorithms that consist of only a single step in each iteration, and in any case, they conclude that acceptance rates between 0.15 and 0.5 do not cost much efficiency. Other researchers, such as \citep{roberts1997weak, bedard2007weak, beskos2009optimal, sherlock2009optimal, sherlock2013optimal}, have been tackled for various shapes of target on choosing the optimal scale of the RWM proposal and led to the similar rule: choose the scale so that the acceptance rate is approximately 0.234. Although nearly all of the theoretical results are based upon limiting arguments in high dimension, the rule of thumb appears to be applicable even in relatively low dimensions \citep{sherlock2010random}. 



In terms of the step size $\epsilon$, it is pointed out that for a stochastic approximation procedure, its step size sequence $\left\lbrace \epsilon_i\right\rbrace$ should satisfy $\sum_{i=1}^\infty \epsilon_i=\infty $ and $\sum_{i=1}^\infty \epsilon_i^{1+\lambda}<\infty $ for some $\lambda>0$. The former condition somehow ensures that any point of $X$ can eventually be reached, while the second condition ensures that the noise is contained and does not prevent convergence \citep{andrieu2008tutorial}. \cite{sherlock2010random} tune various algorithms to attain target acceptance rates, and one of the algorithms tunes step sizes of univariate updates to attain the optimal efficiency of Markov chain at the acceptance rates between 0.4 and 0.45. Additionally, \cite{graves2011automatic} mentions that it is certain that one could use the actual arctangent relationship to try to choose a good $\epsilon$: in the univariate example, if $\alpha$ is the desired acceptance rate, then $\epsilon = 2\sigma / \tan \left(\pi/2\alpha\right)$, where $\sigma$ is the posterior standard deviation, will be obtained. In fact, some explorations infer a linear relationship between acceptance rate and step size, which is $\mathtt{logit}(\alpha) \approx 0.76-1.12\ln \epsilon/\sigma$, and the slope of the relationship is nearly equal to the constant -1.12 independently. 

However, in multi-variable-at-a-time RWM, one expects that the proper interpretation of $\sigma$ is not the posterior standard deviation but the average conditional standard deviation, which is presumably more difficult to estimate from a Metropolis algorithm. In a higher $d$-dimensional space, or propose multi-variable-at-a-time, suppose $\Sigma$ is known or could be estimated, then $X'$ can be proposed from $q\sim N\left(X,\epsilon^2\Sigma\right)$. Thus,the optimal step size $\epsilon$ is required. A concessive way of RWM in high dimension is proposing one-variable-at-a-time and treating them as separate one dimensional space individually. In any case, however, the behavior of RWM on a multivariate normal distribution is governed by its covariance matrix $\Sigma$, and it performs better than a fixed $N\left(X,\epsilon^2I_d\right)$ distribution \citep{roberts2001optimal}.


To explore the efficiency of a MCMC process, we introduce some notions first. For an arbitrary square integrable function $g$, \cite{roberts2001optimal} define its \textit{integrated autocorrelation time} by 
\begin{equation}
\tau_g = 1+ 2\sum_{i=1}^{\infty} \mathrm{Corr}\left( g(X_0),g(X_i) \right),
\end{equation}
where $X_0$ is assumed to be distributed according to $\pi$. Because central limit theorem, the variance of the estimator $\bar{g} = \sum_{i=1}^{n}g(X_i)/n$ for estimating $\E\lbrack g(X) \rbrack$ is approximately $\Var_\pi\lbrack g(X)\rbrack \times \tau_g/n$. The variance tells us the accuracy of the estimator $\bar{g}$. The smaller it is, the faster the chain converges. Therefore, they suggest that the efficiency of Markov chains (Eff) can be found by comparing the reciprocal of their integrated autocorrelation time, which is 
\begin{equation}
e_g(\sigma)\propto \left(\tau_g\Var_\pi\lbrack g(X)\rbrack \right)^{-1}. 
\end{equation}
However, the disadvantage of their method is that the measurement of efficiency is highly dependent on the function $g$. Instead, an alternative approach uses \textit{effective sample size} (ESS) \citep{kass1998markov, robert2004monte},  which is defined in \citep{gong2016practical} in the following form of  
\begin{equation}
\mbox{ESS} =  \frac{n}{1+2\sum_{k=1}^{\infty}\rho_k(X)} \approx \frac{n}{1+2\sum_{k=1}^{k_\text{\tiny cut}}\rho_k(X)}= \frac{n}{\tau}, 
\end{equation}
where $n$ is the amount of samples, $k_\mathtt{cut}$ is lag of the first $\rho_k<0.01$  or $0.05$ , and $\tau$ is the integrated autocorrelation time. Given a Markov chain having $n$ iterations, the ESS measures the size of \iid samples with the same standard error. Moreover, a wide support among both statisticians \citep{geyer1992practical} and physicists \citep{sokal1997monte} use the following cost of independent samples to evaluate the performance of MCMC, that is 
\begin{equation}
\frac{n}{\mbox{ESS}}\times \mbox{cost per step} = \tau \times  \mbox{cost per step}.
\end{equation} 

Being inspired by their research, we now define the Efficiency in Unit Time (EffUT)  and ESS in Unit Time (ESSUT) as follows: 
\begin{align}
\mbox{EffUT}   &= \frac{e_g}{T},\\
\mbox{ESSUT} &= \frac{\mbox{ESS}}{T},
\end{align} 
where $T$ represents the computation time, which is also known as running time. The computation time is the length of time, in minutes or hours, etc, required to perform a computational process. The best Markov chain with an appropriate step size $\epsilon$ should not only have a lower correlation, as illustrated in Figure \ref{largesmallstepsize}, but also have less time-consuming. The standard efficiency $e_g$ and ESS do not depend on the computation time, but EffUT and ESSUT do. The best-tuned step size gains the balance between the size of effective proposed samples and cost of time. 




\section{Simulation Studies}

In this section, we consider the model in regular and irregular spaced time difference separately. For an one dimensional state-space model, we consider the hidden state process $\left\lbrace x_t, t\geq 1\right\rbrace$ is a stationary and ergodic Markov process and transited by $F(x'\mid x)$. In this paper, we assume that the state of a system has an interpretation as the summary of the past one-step behavior of the system. The states are not observed directly but by another process $\left\lbrace y_t, t\geq 1\right\rbrace$, which is assumed depending on $\left\lbrace x_t\right\rbrace$ by the process $G(y\mid x)$ only and independent with each other. When observed on discrete time $T_1,\ldots,T_k$, the model is summarized on the directed acyclic following graph  
\begin{align}
\begin{matrix}
\mbox{State}  & x_0     &  \rightarrow& x_1   & \rightarrow \cdots  & x_k  & \rightarrow \cdots & x_t & \rightarrow \cdots\\
          & &       & \downarrow &         &\downarrow &        &\downarrow &   \\
\mbox{Observation}& && y_1               &          & y_k               &        & y_t               &   \\
\mbox{Time } & &       & T_1               &          & T_k               &        & T_t               &   \\
\end{matrix}
\end{align}
We define $\Delta_k = T_k-T_{k-1}$. If $\Delta_t$ is constant, we retrieve a standard  $\textit{AR(1)}$ model process with regular spaced time steps; if $\Delta_t$ is not constant, then the model becomes more complicated with irregular spaced time steps. (Note that we do not consider $x_0$ below: given an appropriate prior, $x_0$ can always be integrated out of the model probability.)

%If the transition processes $F$ and $G$ are linear and normal distributed, we call this model $\textit{Linear Gaussian State-Space Model}$. 


\subsection{Simulation on Regularly Sampled Time Series Data}

If the time steps are evenly spaced, the model can be written as a simple linear homogeneous state-space model: 
\begin{equation}
\begin{split}
y_t\mid x_t      &\sim N\left(x_t,\sigma^2\right) \\
x_t\mid x_{t-1} &\sim N\left(\phi x_{t-1},\tau^2\right),
\end{split}
\end{equation}
where $\sigma$ and $\tau$ are \iid  errors occurring in processes and $\phi$ is a static process parameter in the forward map. An initial value $x_1\sim N(0,L^2)$. 

The joint distribution is $p(x_{1:t},y_{1:t}) = p(y_t\mid x_t)p(x_t\mid x_{t-1})\cdots p(y_1\mid x_1)p(x_1)$. Given the form of the Gaussian density, the joint distribution becomes 
\begin{equation}
\left[ \begin{matrix} x\\y  \end{matrix}\bigg\rvert \theta \right]
\sim N\left(0, \Sigma  \right),
\end{equation}
where $\theta = \left\lbrace \phi,\sigma,\tau\right\rbrace$, and the precision matrix $\Sigma^{-1}$ is  
%\begin{equation}
%\begin{bmatrix}
%\frac{1}{L^2}+\frac{\phi^2}{\tau^2} & \frac{-\phi}{\tau^2} & \cdots & 0 & 0 & 0& \cdots & 0\\
%\frac{-\phi}{\tau^2}   & \frac{1+\phi^2}{\tau^2}+\frac{1}{\sigma^2}& \cdots & 0 & -\frac{1}{\sigma^2} &0 & \cdots & 0 \\
%0 & \frac{-\phi}{\tau^2}   &  \cdots & 0 & 0& -\frac{1}{\sigma^2} & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
%0 & 0   &  \cdots & \frac{1}{\tau^2}+\frac{1}{\sigma^2} & 0 & 0 & \cdots &-\frac{1}{\sigma^2}\\
%0 & -\frac{1}{\sigma^2}  & \cdots & 0 & \frac{1}{\sigma^2} & 0 & \cdots & 0 \\
%0& 0 & \cdots & 0 & 0 &  \frac{1}{\sigma^2} & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
%0 & 0& \cdots &-\frac{1}{\sigma^2} & 0 & 0 & \cdots &  \frac{1}{\sigma^2}
%\end{bmatrix},
%\end{equation}
\begin{equation}
\begin{bmatrix}
\frac{1}{L^2}+\frac{\phi^2}{\tau^2} +\frac{1}{\sigma^2} & -\frac{\phi}{\tau^2}  & \cdots & 0 & 0 & -\frac{1}{\sigma^2}  & 0 & \cdots & 0 & 0 \\
-\frac{\phi}{\tau^2}   & \frac{1+\phi^2}{\tau^2}+\frac{1}{\sigma^2}  & \cdots & 0 & 0 & 0 & -\frac{1}{\sigma^2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0  &  \cdots & \frac{1+\phi^2}{\tau^2} + \frac{1}{\sigma^2} & -\frac{\phi}{\tau^2} & 0 & 0 & \cdots & -\frac{1}{\sigma^2} & 0 \\
0 & 0   &  \cdots & -\frac{\phi}{\tau^2}  & \frac{1}{\tau^2}+\frac{1}{\sigma^2} & 0 & 0 & \cdots & 0 & -\frac{1}{\sigma^2}\\
-\frac{1}{\sigma^2}  & 0 & \cdots & 0 & 0 & \frac{1}{\sigma^2} & 0 & \cdots & 0 & 0\\
0&-\frac{1}{\sigma^2}   & \cdots & 0 & 0 & 0 &  \frac{1}{\sigma^2} & \cdots & 0& 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots &  \vdots\\
0 & 0& \cdots &-\frac{1}{\sigma^2} & 0 &  0 & 0 & \cdots &  \frac{1}{\sigma^2}  & 0\\ 
0 & 0& \cdots & 0 & -\frac{1}{\sigma^2} & 0 & 0 & \cdots & 0 &  \frac{1}{\sigma^2}
\end{bmatrix},
\end{equation}\label{precisionMatrix}
and denoted as $\Sigma^{-1}\triangleq\begin{bmatrix} A_t & -B_t \\ -B_t & B_t \end{bmatrix}$, 
%Its inverse is the covariance matrix 
%\begin{equation}
%\Sigma=\begin{bmatrix} (A_t-B_t)^{-1} &  (A_t-B_t)^{-1} \\ (A_t-B_t)^{-1} & (I_{t}-A_t^{-1}B_t)^{-1}B_t^{-1} \end{bmatrix} \triangleq \begin{bmatrix}
%\Sigma_{XX} & \Sigma_{XY}  \\ \Sigma_{YX} & \Sigma_{YY} 
%\end{bmatrix},
%\end{equation}
%where $B$ is a $t\times t$ diagonal matrix with elements $\frac{1}{\sigma^2}$. The covariance matrices $\Sigma_{XX} =  \left(A_t-B_t\right)^{-1}$ and $\Sigma_{YY}=\left(I_{t}-A_t^{-1}B_t\right)^{-1}B_t^{-1}$ are found. 
where $B_t$ is a diagonal matrix $\frac{1}{\sigma^2}I$ proportional to an identity matrix. 


\subsubsection*{Parameter Estimation}

In the formula \eqref{M1}, the parameter posterior is estimated with observation $y_{1:t}$. By using the Algorithm \ref{algoonevarible}, although it may take a longer time, we will achieve a precise estimation. Similarly with Section \ref{sectionlogParameter}, from the objective function, the posterior distribution of $\theta$ is 
%\begin{equation}
%p(\theta \mid Y) \propto p(Y\mid\theta)p(\theta) \propto \exp\left( {-\frac{1}{2} Y \Sigma_{YY}^{-1} Y } \right) \sqrt{\det \Sigma_{YY}^{-1}} p(\theta).
%\end{equation}
the same as equation \eqref{posteriortheta}. 
Then, by taking natural logarithm on the posterior of $\theta$ and by using the useful solutions in equations \eqref{sigmayy01} and \eqref{sigmayy02}, we will have
%\begin{equation}\label{linearlogL}
%\ln L(\theta) = -\frac{1}{2}Y^\top\Sigma_{YY}^{-1}Y+\frac{1}{2}\sum\ln\mbox{tr}(B_t)-\sum\ln\mbox{tr}(L_t)+\sum\ln\mbox{tr}(R_t) + \ln p(\theta).
%\end{equation}
the same log-likelihood function \eqref{logposteriorL}.

In a simple linear case, we are choosing the parameter $\theta = \left\lbrace \phi=0.9,\tau^2=0.5,\sigma^2=1\right\rbrace$ as the author did in \citep{lopes2011particle} and using $n=500$ dataset, setting initial $L=0$. Instead of inferring $\tau$ and $\sigma$, we are estimating $\nu_1 = \ln \tau^2$ and $\nu_2 = \ln \sigma^2$ in the RW-MH to avoid singular proposals. After the process, the parameters can be transformed back to original scale. Therefore the new parameter  $\theta^* =  \left\lbrace \phi,\nu_1,\nu_2\right\rbrace = \left\lbrace \phi,\ln\tau^2,\ln\sigma^2\right\rbrace$. 

By using Algorithm \ref{algoonevarible} and aiming the optimal acceptance rate at 0.44, after 10\,000 iterations we get the acceptance rates for each parameters are $\alpha_\phi = 0.4409, \alpha_{\nu_1}= 0.4289$ and $\alpha_{\nu_2}= 0.4505$, and the estimations are $\phi =0.8794, \nu_1= -0.6471$ and $\nu_2= -0.0639$ respectively. Thus, we have the cheap surrogate $\hat{\pi}(\cdot)$. Keep going to the DA MH with another 10\,000 iterations, the algorithm returns the best estimation with $\alpha_1=0.1896$ and $\alpha_2 = 0.8782$. In Figure \ref{linearmarginplots}, the trace plots illustrates that the Markov chain of $\hat{\theta}$ is stably fluctuating around the true $\theta$. 

\begin{figure}[h]
\centering
 \begin{subfigure}[b]{0.32\textwidth}
     \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/linear_phi.pdf}
     \caption{Trace plot of $\phi$}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/linear_tau2.pdf}
     \caption{Trace plot of $\tau^2$}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth} \
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/linear_sig2.pdf}
     \caption{Trace plot of $\sigma^2$}
\end{subfigure}
\caption{Linear simulation with true parameter $\theta = \{ \phi=0.9,\tau^2=0.5,\sigma^2=1\}$. By transforming back to the original scale, the estimation of $\hat{\theta}$ is $\{\phi = 0.8810, \tau^2 = 0.5247, \sigma^2= 0.9416\}$. }
\label{linearmarginplots}
\end{figure}



\subsubsection*{Recursive Forecast Distribution}\label{sectionlinearRecursive}

The calculation of log-posterior of the parameters requires finding out the forecast distribution of $p(y_{1:t}\mid y_{1:t-1},\theta)$. A general way is to use the joint distribution of $y_{t}$ and $y_{1:t-1}$, which is $p(y_{1:t}\mid \theta)\sim N(0,\Sigma_{YY})$, and following the procedure in Section \ref{sectionforecast} to work out the inverse matrix of a multivariate normal distribution. For example, one may find the inverse of the covariance matrix 
\begin{align}
\Sigma_{YY}^{-1} = B_t(I_t-A_t^{-1}B_t) =\frac{1}{\sigma^4}(\sigma^2 I_t-A_t^{-1}) \triangleq \frac{1}{\sigma^4} 
\begin{bmatrix} 
Z_{t} & b_{t} \\
b_{t}^\top & K_{t}
\end{bmatrix},
\end{align}
and the original form of this covariance is 
\begin{align} \Sigma_{YY} =\sigma^4 \begin{bmatrix}
\left(Z_t-b_tK_t^{-1}b_t^\top\right)^{-1} & -Z_t^{-1}b_t\left(K_t-b_t^\top Z_t^{-1}b_t\right)^{-1}\\
-K_t^{-1}b_t^\top \left(Z_t-b_tK_t^{-1}b_t^\top\right)^{-1} & \left(K_t-b_t^\top Z_t^{-1}b_t\right)^{-1}
\end{bmatrix}. 
\end{align}
By denoting $C_{t}^\top = \begin{bmatrix} 0 & \cdots & 0 & 1\end{bmatrix}$ and post-multiplying $\Sigma_{YY}^{-1}$, we will have  
\begin{equation}\label{beforeSMformula}
\Sigma_{YY}^{-1} C_{t}= \frac{1}{\sigma^4}\left(\sigma^2 I_t-A_t^{-1} \right)C_{t}= \frac{1}{\sigma^4} \begin{bmatrix} b_{t} \\ K_{t} \end{bmatrix}.
\end{equation} 


A recursive way of calculating $b_t$ and $K_t$ is to use the Sherman-Morrison-Woodbury formula. In the late 1940s and the 1950s,
%Sherman and Morrison\citep{sherman1950adjustment}, Woodbury \citep{woodbury1950inverting}, Bartlett \citep{bartlett1951inverse} and Bodewig \citep{bodewig1956matrix} 
\cite{sherman1950adjustment, woodbury1950inverting, bartlett1951inverse, bodewig1956matrix} 
discovered the following Theorem \ref{theoremSMW}. The original Sherman-Morrison-Woodbury (for short SMW) formula has been used to consider the inverse of matrices \citep{deng2011generalization}. In this paper, we will consider the more generalized case. 
\begin{theorem}\label{theoremSMW}
(Sherman-Morrison-Woodbury formula). Let $A \in B(H)$ and $G \in B(K)$ both be invertible, and $Y, Z \in B(K, H)$. Then, $A + YGZ^*$ is invertible if and only if $G^{-1} + Z^A^{-1}Y$ is invertible. In which case,
\begin{equation}\label{SMWformula}
\left(A+YGZ^*\right)^{-1}= A^{-1}-A^{-1}Y\left(G^{-1}+Z^A^{-1}Y\right)^{-1}Z^A^{-1}.
\end{equation}
A simple form of SMW formula is Sherman-Morrison formula represented in the following statement \citep{bartlett1951inverse}:
Suppose $A\in R^{n\times n}$ is an invertible square matrix and $u,v\in R^n$ are column vectors. Then, $A+uv\top$ is invertible $\iff 1+u^\top A^{-1}v\neq 0$. If $A+uv\top$ is invertible, then its inverse is given by
\begin{equation}\label{SMformula}
\left(A+uv^{T}\right)^{-1}=A^{-1}-{A^{-1}uv^{T}A^{-1} \over 1+v^{T}A^{-1}u}.
\end{equation}
\end{theorem}

By using the formula \eqref{SMformula}, one can update $K_{t}$ and $b_{t}$  in such a recursive way
\begin{align}\label{linearOUKreg}
K_{t}  &=\frac{\sigma^4}{\tau^2+\sigma^2+\phi^2(\sigma^2-K_{t-1})},\\
b_{t} &= \begin{bmatrix}
\frac{b_{t-1}\phi K_{t}}{\sigma^2} \\ \label{linearOUbreg} \frac{K_{t}(\sigma^2+\tau^2)-\sigma^4 }{\phi\sigma^2}
\end{bmatrix}. 
\end{align}
With the above formula, the recursive way of updating the mean and covariance is in the following formula: 
\begin{align}
\bar{\mu}_{t}       & = \frac{\phi}{\sigma^2}K_{t-1}\bar{\mu}_{t-1} + \phi \left(1 - \frac{K_{t-1}}{\sigma^2}\right)y_{t-1}, \\
\bar{\Sigma}_{t}  &= \sigma^4K_{t}^{-1},
\end{align}
%where $K_1=\frac{\sigma^4}{\sigma^2+\tau^2+L^2\phi^2}$. 
where $K_1=\frac{\sigma^4}{\sigma^2+L^2}$. 
For calculation details, we refer readers to Appendix \ref{linearcalculation}.


\subsubsection*{The Estimation Distribution}

%As discussed in Section \ref{generalEstDistr}, from the joint distribution of $x_{1:t}$ and $y_{1:t}$, one can find the best estimation with a given $\theta$ by
%\begin{align}
%x_{1:t} \mid y_{1:t},\theta \sim N\left(L_t^{-\top}W_t,L_t^{-\top}L_t^{-1}\right),
%\end{align}
%where $W_t = L_t^{-1}B_{t}y_{1:t-1}$. 
%%Consequently 
%\begin{align}
%x_{1:t} = L_t^{-\top}(W_t+Z_t),
%\end{align}
%where $Z_t \sim N\left(0, I(\varepsilon)_{t}\right)$ is independent and identically distributed and drawn from a zero-mean normal distribution with variance $ I(\varepsilon)_t$. Moreover, 
%The mixture Gaussian distribution $p(x_t \mid y_{1:t})$ can be found by 
%\begin{align}
%\mu_t^{(x)} &= \frac{1}{N} \sum_i \mu_{ti}^{(x)} \label{linearmu}  \\
%\Var(x_t) &= \frac{1}{N} \sum_i \left( \mu_{ti}^{(x)}  \mu_{ti}^{(x)\top} +\Var(x_{ti})\right) -\frac{1}{N^2} \left(  \sum_i  \mu_{ti}^{(x)} \right) \left( \sum_i \mu_{ti}^{(x)} \right) ^\top.\label{linearsigma} 
%\end{align}
%To find $\mu_{ti}^{(x)}$ and $\Var(x_{ti})$, we will use the joint distribution of $x_{t}$ and $y_{1:t}$, which is $p(x_{t}, y_{1:t}  \mid  \theta)= N(0,\Gamma)$ and 
%\begin{equation}
%\Gamma=\begin{bmatrix} C_{t}^\top(A_t-B_t)^{-1}C_{t} & C_{t}^\top(A_t-B_t)^{-1}\\(A_t-B_t)^{-1}C_{t} & (I_t-A_t^{-1}B_t)^{-1}B_t^{-1} \end{bmatrix}.
%\end{equation}


As discussed in Section \ref{generalEstDistr}, from the joint distribution of $x_{1:t}$ and $y_{1:t}$, one can find the estimation distribution \eqref{estimationdistribution}, a further joint distribution for $x_t,y_{1:t}$ \eqref{joinedXYgiventheta}, and the mixture Gaussian distribution \eqref{mixtureGaussian} with mean \eqref{mixturemean} and variance \eqref{mixturevariance}. Because of 
\begin{align}
C_{t}^\top A_{t}^{-1} = \begin{bmatrix} - b_{t}^\top & \sigma^2- K_{t} \end{bmatrix},
\end{align}
thus, for any given $\theta$, we have $x_{t}\mid y_{1:t},\theta \sim N\left( \mu_{t}^{(x)},\Var(x_t) \right)$, where
\begin{align}
\mu_{t}^{(x)} &  =  \frac{K_{t}\bar{\mu}_{t}}{\sigma^2}+\left(1-\frac{K_{t}}{\sigma^2}\right)y_{t} \\
\Var(x_t)&= \sigma^2-K_{t}.
\end{align}
By substituting them into the equation \eqref{mixturemean} and \eqref{mixturevariance}, the estimated $x_t$ is obtained. For calculation details, we refer readers to Appendix \ref{linearcalculation}.


\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/linearsimuXall.pdf}
     \caption{Estimation of $x_{1:t}$}\label{MCMClinearsimuXall}
\end{subfigure}
\begin{subfigure}[b]{0.45\textwidth}
    %\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/gglinearestXt.pdf}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/gglinearestXt2.pdf}
     \caption{Estimation of a single $x_t$}\label{MCMClinearsimuXt2}
\end{subfigure}
\caption{Linear simulation of $x_{1:t}$ and single $x_t$.In Figure \ref{MCMClinearsimuXall}, the dots is the true $x_{1:t}$ and the solid line is the estimation $\hat{x}_{1:t}$. In Figure \ref{MCMClinearsimuXt2}, the estimation $\hat{x}_t$ is very close to the true $x$. In fact, the true $x$ falls in the interval $\lbrack \hat{x}-\varepsilon,\hat{x}+\varepsilon\rbrack$.}
\label{linearmarginXt}
\end{figure}



\subsection{Simulation on Irregularly Sampled Time Series Data}

Irregularly sampled time series data is painful for scientists and researchers. In spatial data analysis, several satellites and buoy networks provide continuous observations of wind speed, sea surface temperature, ocean currents, etc. However, data was recorded with irregular time-step, with generally several data each day but also sometimes gaps of several days without any data. \cite{tandeo2011linear} adapt a continuous-time state-space model to analyze this kind of irregular time-step data, in which the state is supposed to be an Ornstein-Uhlenbeck process. 

The OU process is an adaptation of Brownian Motion, which models the movement of a free particle through a liquid and was first developed by \cite{einstein1956investigations}. 
%The Brownian motion is used to construct the Ornstein-Uhlenbeck (OU) process, which has become a popular tool for modeling interest rates and vehicle moving. The derivative of the Brownian motion $x_t$ does not exist at any point in time. Thus, if $x_t$ represents the position of a particle, we might be interested in obtaining its velocity, which is the derivative of the motion. The OU process is an alternative model to the Brownian motion that overcomes the preceding problem. 
By considering the velocity $u_t$ of a Brownian motion at time $t$, over a small time interval, two factors affect the change in velocity: the frictional resistance of the surrounding medium whose effect is proportional to $u_t$ and the random impact of neighboring particles whose effect can be represented by a standard Wiener process. Thus, because mass times velocity equals force, the process in a differential equation form is 
\begin{equation}
mdu_t = -\omega u_tdt+dW_t,
\end{equation}
where $\omega>0$ is called the friction coefficient and $m>0$ is the mass. If we define $\gamma = \omega /m$ and $\lambda = 1/m$, we obtain the OU process \citep{Schobel1999Stochastic},  %\citep{vaughan2015goodness}
which was introduced with the following differential equation:
\begin{equation}
du_t= -\gamma u_tdt+\lambda dW_t.
\end{equation}


The OU process is used to describe the velocity of a particle in a fluid and is encountered in statistical mechanics. It is the model of choice for random movement toward a concentration point. It is sometimes called a continuous-time Gauss Markov process, where a Gauss Markov process is a stochastic process that satisfies the requirements for both a Gaussian process and a Markov process. Because a Wiener process is both a Gaussian process and a Markov process, in addition to being a stationary independent increment process, it can be considered a Gauss-Markov process with independent increments \citep{kijima1997markov}. 

To apply OU process on irregularly sampled data, we assume that the latent process $\left\lbrace x_{1:t}\right\rbrace$ is a simple OU process, that is a stationary solution of the following stochastic differential equation : 
\begin{equation}\label{linearOUequation}
dx_t= -\gamma x_tdt+\lambda dW_t, 
\end{equation}
where $W_t$ is a standard Brownian motion, $\gamma>0$ represents the slowly evolving transfer between two neighbor data and $\lambda$ is the forward transition variability. It is not hard to find the solution of equation \eqref{linearOUequation} is 
\begin{equation}
x_t = x_{t-1}e^{-\gamma t} +\int_{0}^{t} \lambda e^{-\gamma (t-s)}dW_s. 
\end{equation}
For any arbitrary time step $t$, the general form of the process satisfies 
\begin{equation}
x_t = x_{t-1}e^{-\gamma \Delta_t} + \tau,
\end{equation}
where $\Delta_t = T_t-T_{t-1}$ is the time difference between two consecutive data points, $\tau$ is a Gaussian white noise with mean zero and variances $\frac{\lambda^2}{2\gamma}\left(1-e^{-2\gamma\Delta_t}\right)$. 

The observed $y_{1:t}$ is measured by 
\begin{equation}
y_t = Hx_t + \varepsilon,
\end{equation}
where $\varepsilon\sim N(0,\sigma)$ is a Gaussian white noise. 

To run the simulations, we generate an irregular time-lag sequence $\left\lbrace \Delta_t\right\rbrace$ first from an \textit{Inverse Gamma} distribution with parameters $\alpha=2, \beta=0.1$. Then, the following parameters were chosen for the numerical simulation: $\gamma = 0.5$, $\lambda^2 = 0.1$, $\sigma^2=1$. 



\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth,height=5cm]{Chapters/05MCMCOU/plots/simudataOUdataview.pdf}
\includegraphics[width=0.45\textwidth,,height=5cm]{Chapters/05MCMCOU/plots/simudataOUDelThist2.pdf}
\caption{Simulated data. The solid dots indicate the true state $x$ and cross dots indicate observation $y$. Irregular time lag $\Delta_t$ are generated from \textit{Inverse Gamma}(2,0.1) distribution.}
\label{simuOUreview}
\end{figure}


Similarly, we can get the joint distribution for $x_{1:t}$ and $y_{1:t}$ 
%\begin{equation}
%\begin{bmatrix} \begin{matrix} x\\y \end{matrix} \bigg\rvert \theta \end{bmatrix}
%\sim N\left(0, \Sigma  \right)
%\end{equation}
%from the precision matrix 
%\begin{equation}
%\begin{bmatrix}
%\frac{1}{L^2}+\frac{\phi_1^2}{\tau_1^2} +\frac{1}{\sigma^2}& \frac{-\phi_1}{\tau_1^2} & \cdots & 0 & 0 & 0& \cdots & 0\\ 
%\frac{-\phi_1}{\tau_1^2}   &\frac{1}{\tau_1^2}+\frac{\phi_2^2}{\tau_2^2}+\frac{1}{\sigma^2}& \cdots & 0 & -\frac{1}{\sigma^2} &0 & \cdots & 0 \\
%0 & \frac{-\phi_2}{\tau_2^2}   &  \cdots & 0 & 0& -\frac{1}{\sigma^2} & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
%0 & 0   &  \cdots & \frac{1}{\tau_t^2}+\frac{1}{\sigma^2} & 0 & 0 & \cdots &-\frac{1}{\sigma^2}\\
%0 & -\frac{1}{\sigma^2}  & \cdots & 0 & \frac{1}{\sigma^2} & 0 & \cdots & 0 \\
%0& 0 & \cdots & 0 & 0 &  \frac{1}{\sigma^2} & \cdots & 0\\
%\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
%0 & 0& \cdots &-\frac{1}{\sigma^2} & 0 & 0 & \cdots &  \frac{1}{\sigma^2}
%\end{bmatrix}
%\end{equation}
having a similar precision matrix shown in equation \eqref{precisionMatrix}, where $\phi_t = e^{-\gamma\Delta_t}, \tau^2_t = \frac{\lambda^2}{2\gamma}\left(1-e^{-2\gamma\Delta_t}\right)$, $\theta$ represents for the unknown parameters. 
%Denoted by $\Sigma^{-1}=\begin{bmatrix} A_t & -B_t \\ -B_t & B_t\end{bmatrix}$, covariance matrix is 
%\begin{equation}
%\Sigma=\begin{bmatrix} \left(A_t-B_t\right)^{-1} &  \left(A_t-B_t\right)^{-1} \\ \left(A_t-B_t\right)^{-1} & \left(I-A_t^{-1}B_t\right)^{-1}B_t^{-1} \end{bmatrix} \triangleq \begin{bmatrix}
%\Sigma_{XX} & \Sigma_{XY}  \\ \Sigma_{YX} & \Sigma_{YY} 
%\end{bmatrix},
%\end{equation}
%where $B_t$ is a $t\times t$ diagonal matrix with elements $\frac{1}{\sigma^2}$. The covariance matrices $\Sigma_{XX} =  \left(A_t-B_t\right)^{-1}$ and $\Sigma_{YY} =  \left(I-A_t^{-1}B_t\right)^{-1}B_t^{-1}$. 



\subsubsection*{Parameter Estimation}

%To use the Algorithm \ref{algoonevarible}, similarly with Section \ref{sectionlogParameter}, we need to find the posterior distribution of $\theta$ with observations $y_{1:t}$ fist, which in fact is 
%\begin{equation}
%p(\theta \mid Y) \propto p(Y\mid\theta)p(\theta) \propto \exp\left( -\frac{1}{2} Y \Sigma_{YY}^{-1} Y \right) \sqrt{\det \Sigma_{YY}^{-1}} p(\theta).
%\end{equation}
%By taking natural logarithm on the posterior of $\theta$ and by using the useful solutions in equations \eqref{sigmayy01} and \eqref{sigmayy02}, we have 
%\begin{equation}\label{simuOUlogL}
%\ln L(\theta) = -\frac{1}{2}Y^\top\Sigma_{YY}^{-1}Y+\frac{1}{2}\sum\ln\mbox{tr}(B_t)-\sum\ln\mbox{tr}(L_t)+\sum\ln\mbox{tr}(R_t) + \ln p(\theta).
%\end{equation}


By implementing he Algorithm \ref{algoonevarible}, similarly with Section \ref{sectionlogParameter}, from the objective function, the posterior distribution of $\theta$ is the same as equation \eqref{posteriortheta}. By taking natural logarithm on the posterior of $\theta$ and by using the useful solutions in equations \eqref{sigmayy01} and \eqref{sigmayy02}, we have the same log-likelihood function \eqref{logposteriorL}.

Because of all parameters are positive, we are estimating $\nu_1=\ln\lambda$, $\nu_2=\ln\gamma^2$ and $\nu_3=\ln\sigma^2$ instead. When the estimation process is done, we can transform them back to the original scale by taking exponential. 

After running the whole process, it gives us the best estimation of $\hat{\theta}$ is
$\{ \gamma=0.4841, \lambda^2=0.1032, \sigma^2=0.9276\}$. In Figure \ref{simuOUmarginplots}, we can see that the $\theta$ chains are skew to the true value with tails.
\begin{figure}[h]
\centering
 \begin{subfigure}[b]{0.3\textwidth}
     \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUtracegam.pdf}
     \caption{Trace plot of $\gamma$}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUtracelab2.pdf}
     \caption{Trace plot of $\lambda^2$}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUtracesig2.pdf}
     \caption{Trace plot of $\sigma^2$}
\end{subfigure}
\caption{Irregular time step OU process simulation. The estimation of $\hat{\theta}$ is $\{\gamma=0.4841, \lambda^2=0.1032, \sigma^2=0.9276\}$. In the plots, the horizontal dark lines are the true $\theta$. }
\label{simuOUmarginplots}
\end{figure}



\subsubsection*{Recursive Calculation and State Estimation}

Follow the procedure in Section \ref{sectionforecast} and do similar calculation with Section \ref{sectionlinearRecursive}, one can find the following recursive way to update $K_{t}$ and $b_{t}$: 
\begin{align} \label{linearOUKirreg}
K_{t}  &=\frac{\sigma^4}{\tau_t^2+\sigma^2+\phi_t^2(\sigma^2-K_{t-1})},\\
b_{t} &= \begin{bmatrix}
\frac{b_{t-1}\phi_t K_{t}}{\sigma^2} \\\label{linearOUbirreg} \frac{K_{t}(\sigma^2+\tau_t^2)-\sigma^4 }{\phi_t\sigma^2}
\end{bmatrix}. 
\end{align}
With the above formula, the recursive way of updating the mean and covariance are 
\begin{align} \label{linearOUmu}
\bar{\mu}_{t}       & = \frac{\phi_t}{\sigma^2}K_{t-1}\bar{\mu}_{t-1} + \phi_t \left(1 - \frac{K_{t-1}}{\sigma^2}\right)y_{t-1}, \\
\bar{\Sigma}_{t}  &= \sigma^4K_{t}^{-1}, \label{linearOUsigma}
\end{align}
%where $K_1=\frac{\sigma^4}{\sigma^2+\tau_1^2+L^2\phi_1^2}$. 
where $K_1=\frac{\sigma^4}{\sigma^2+L^2}$. 
%
%Additionally, as discussed in Section \ref{generalEstDistr}, the best estimation of $x_{1:t}$ with a given $\theta$ is 
%\begin{align}
%x_{1:t} \mid y_{1:t},\theta \sim N\left(L_t^{-\top}W_t,L_t^{-\top}L_t^{-1}\right),
%\end{align}
%where $W_t = L_t^{-1}B_{t}y_{1:t-1}$, and the mixture Gaussian distribution for $p(x_t \mid y_{1:t})$ is 
%\begin{align}
%\mu_t^{(x)} &= \frac{1}{N} \sum_i \mu_{ti}^{(x)}  \\
%\Var(x_t) &= \frac{1}{N} \sum_i \left( \mu_{ti}^{(x)}  \mu_{ti}^{(x)\top} +\Var(x_{ti})\right) -\frac{1}{N^2} \left(  \sum_i  \mu_{ti}^{(x)} \right) \left( \sum_i \mu_{ti}^{(x)} \right) ^\top.
%\end{align}


With a given $\theta$, the estimation is $x_{t}\mid y_{1:t},\theta \sim N\left(\mu_{t}^{(x)},\Var(x_t)\right)$, where
\begin{align}\label{linearOUmean}
\mu_{t}^{(x)} &  =  \frac{K_{t}\bar{\mu}_{t}}{\sigma^2}+\left(1-\frac{K_{t}}{\sigma^2}\right)y_{t} \\ \label{linearOUvar}
\Var(x_t)&= \sigma^2-K_{t}.
\end{align}
By substituting them into the equation \eqref{mixturemean} and \eqref{mixturevariance}, the estimated $x_t$ is obtained. 

Notice that the difference between equations \eqref{linearOUKreg}\eqref{linearOUbreg} and equations  \eqref{linearOUKirreg} \eqref{linearOUbirreg} is that in the latter ones the parameters are dependent on the time lag $\Delta_t$. 

\begin{figure}[h]
\centering
\begin{subfigure}[h]{0.45\textwidth}
\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUallX.pdf}
\caption{Batch method of estimating $x_{1:t}$}\label{MCMCOUallX}
\end{subfigure}
\begin{subfigure}[h]{0.45\textwidth}
%    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUXt.pdf}
\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUXt2.pdf}
\caption{Sequential method of estimating $x_t$}\label{MCMCOUallXt2}
\end{subfigure}
\caption{Irregular time step OU process simulation of $x_{1:t}$ and sole $x_t$. In Figure \ref{MCMCOUallX}, the dots is the true $x_{1:t}$ and the solid line is the estimation $\hat{x}_{1:t}$. In Figure \ref{MCMCOUallXt2}, the chain in solid line is the estimation $\hat{x}_t$; dotted line is the true value of $x$; dot-dash line on top is the observed value of $y$; dashed lines are the estimated error. }
\label{simuOUxt}
\end{figure}




\section{High Dimensional Ornstein-Uhlenbeck Process Application}\label{SectionHighDimensionalOU}

Tractors moving on an orchard are mounted with GPS units, which are recording data and transfer to the remote server. This data infers longitude, latitude, bearing, etc, with unevenly spaced time mark. However, one dimensional OU process containing either only position or velocity is not enough to infer a complex movement. 

\begin{figure}[h]
\centering
\begin{tikzpicture}
     \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/realdatapath_Final.pdf}};
     \begin{scope}[
         x={(image.south east)},
         y={(image.north west)}
     ]
     \node [black, font=\bfseries] at (0.5,-0.05) {Easting};
     \node [black, font=\bfseries,rotate=90] at (-0.05,0.5) {Northing};
     \end{scope}
\end{tikzpicture}
\begin{tikzpicture}
     \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/realdatahistdeltaT_Final.pdf}};
     \begin{scope}[
         x={(image.south east)},
         y={(image.north west)}
     ]
     \node [black, font=\bfseries] at (0.5,-0.05) {Lag};
     %\node [black, font=\bfseries,rotate=90] at (-0.05,0.5) {Frequency};
     \end{scope}
\end{tikzpicture}
\caption{Demonstration of line-based trajectory of a moving tractor. The time lags (right side figure) obtained from GPS units are irregular.}
\label{realdatareview}
\end{figure}

In this section, we are introducing an Ornstein-Uhlenbeck process (OU-process) model combing both position and velocity with the following equations  
\begin{equation}\label{OUprocess}
\begin{cases}
du_t = -\gamma u_t dt+ \lambda dW_t,\\
dx_t = u_t dt+\xi dW_t'.
\end{cases}
\end{equation}
The solution can be found by integrating $dt$ out, that gives us 
\begin{align}
\begin{cases}
u_t =u_{t-1}e^{-\gamma t} +\int_{0}^{t} \lambda e^{-\gamma (t-s)}dW_s,\\
x_t =x_{t-1} +\frac{u_{t-1}}{\gamma}\left(1- e^{-\gamma t}\right) + \int_{0}^{t} \frac{\lambda}{\gamma}e^{\gamma  s} \left(1-e^{-\gamma t}\right)dW_s + \int_{0}^{t}\xi dW_s'.
\end{cases}
\end{align}
As a result, the joint distribution is 
\begin{align}
\begin{bmatrix} x_t \\ u_t \end{bmatrix} &\sim N\left(
\begin{bmatrix}\mu_t^{(x)} \\ \mu_t^{(u)}  \end{bmatrix} , 
\begin{bmatrix}
\sigma_t^{(x)2} & \rho_t\sigma_t^{(x)} \sigma_t^{(u)} \\
\rho_t\sigma_t^{(x)} \sigma_t^{(u)} & \sigma_t^{(u)2}
\end{bmatrix} \right),
\end{align}
where $\mu_t^{(x)}$ and $\mu_t^{(u)} $ are from the forward map process 
\begin{align}
\begin{bmatrix}\mu_t^{(x)} \\ \mu_t^{(u)}  \end{bmatrix}  = 
\begin{bmatrix}
1 & \frac{1-e^{-\gamma \Delta_t}}{\gamma} \\ 0 &  e^{-\gamma \Delta_t}
\end{bmatrix}  \begin{bmatrix} x_{t-1}^{(x)} \\ u_{t-1}  \end{bmatrix} \triangleq \Phi \begin{bmatrix} x_{t-1}^{(x)} \\ u_{t-1}  \end{bmatrix},
\end{align}
and 
\begin{align}
\begin{cases}
\sigma_t^{(x)2} &=\frac{\lambda^2 \left(e^{2 \gamma\Delta_t}-1\right) \left(1 -e^{-\gamma\Delta_t}\right)^2}{2 \gamma ^3 } + \xi^2\Delta_t\\
\sigma_t^{(u)2} &= \frac{\lambda ^2 \left(1- e^{-2 \gamma\Delta_t}\right)}{2 \gamma } \\
\rho_t\sigma_t^{(x)}\sigma_t^{(u)} & =\frac{\lambda ^2 \left(e^{\gamma\Delta_t} -1\right) \left(1-e^{-2\gamma\Delta_t}\right)}{2 \gamma ^2}
\end{cases}
\end{align}
In the above equations $\Delta_t = T_t-T_{t-1}$ and initial values are $\Delta_1=0$, %$x_0\sim N\left(0,L_x^2\right), u_0\sim N(0,L_u^2)$, %% was x_0
$x_1\sim N\left(0,L_x^2\right), u_1\sim N(0,L_u^2)$, 
$\rho_t^2 = 1-\frac{\xi^2 \Delta_t}{\sigma_t^{(x)^2}}$. To be useful, we use $\frac{1}{1-\rho_t^2} =\frac{\sigma_t^{(x)2}}{\xi^2 \Delta_t}$ in the calculation. 

Furthermore, the independent observation process is 
\begin{equation}\label{obseq}
\begin{cases} y_t=x_t+\varepsilon_t,\\ v_t=u_t+\varepsilon'_t, \end{cases} 
\end{equation}
where $\varepsilon_t\sim N(0,\sigma),\varepsilon'_t\sim N(0,\tau)$ are normally distributed independent errors. Thus, the joint distribution of observations is 
\begin{align}\label{obmodel}
\begin{bmatrix} y_t \\ v_t \end{bmatrix} &\sim N\left(
\begin{bmatrix}x_t \\ u_t \end{bmatrix} , 
\begin{bmatrix}
\sigma^2 & 0\\
0 & \tau^2
\end{bmatrix} \right).
\end{align}
Consequently, the parameter $\theta$ of an entire Ornstein-Uhlenbeck process is a set of five parameters from both hidden status and observation process, which is represented as $\theta = \left\lbrace \gamma,\xi^2,\lambda^2,\sigma^2,\tau^2 \right\rbrace$. 


Starting from the joint distribution of $x_{0:t},u_{0:t}$ and $y_{1:t},v_{1:t}$ by given $\theta$, it can be found that
\begin{equation}\label{jointmatrix}
\begin{bmatrix} \begin{matrix} \tilde{X}\\ \tilde{Y}  \end{matrix} \biggr\rvert \theta \end{bmatrix}
\sim N\left(0, \tilde{\Sigma} \right),
\end{equation}
where $\tilde{X}$ represents for the hidden statues $\left\lbrace x,u\right\rbrace$, $\tilde{Y}$ represents for observed $\left\lbrace y,v\right\rbrace$, $\theta$ is the set of five parameters.  The inverse of the covariance matrix $\tilde{\Sigma}^{-1}$ is the precision matrix in the form of
\begin{align} \tilde{\Sigma}^{-1}=
\begin{bmatrix}
Q_{xx} & Q_{xu} & -\frac{1}{\sigma^2}I & 0\\
Q_{ux} & Q_{uu} & 0 &-\frac{1}{\tau^2}I \\
-\frac{1}{\sigma^2}I & 0 & \frac{1}{\sigma^2}I  & 0\\
 0  &  -\frac{1}{\tau^2}I  & 0 & \frac{1}{\tau^2}I 
\end{bmatrix}.
\end{align}
To make the covariance matrix a more beautiful form and convenient computing, $\tilde{X}$, $\tilde{Y}$ and $\tilde{\Sigma}$ can be rearranged in a time series order, that makes $X_{1:t} = \left\lbrace x_1,u_1,x_2,u_2,\ldots, x_t, u_t \right\rbrace$, $Y_{1:t} = \left\lbrace y_1,v_1,y_2,v_2,\ldots, y_t, v_t \right\rbrace$ and the new precision matrix $\Sigma^{-1}$ looks like 
\begin{align} \Sigma^{-1} &=
\begin{bmatrix}
\sigma_{11}^{(x)2}+\frac{1}{\sigma^2} & \sigma_{11}^{(xu)2} & \cdots & \sigma_{1t}^{(x)2} & \sigma_{1t}^{(xu)2}  &  -\frac{1}{\sigma^2} & 0 & \cdots & 0 & 0\\
\sigma_{11}^{(ux)2}   & \sigma_{11}^{(u)2} +\frac{1}{\tau^2} & \cdots & \sigma_{1t}^{(ux)2} & \sigma_{1t}^{(x)2}  &  0 & -\frac{1}{\tau^2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots &\ddots & \vdots & \vdots \\
\sigma_{t1}^{(x)2}   & \sigma_{t1}^{(xu)2} & \cdots & \sigma_{tt}^{(x)2} +\frac{1}{\sigma^2}  & \sigma_{tt}^{(xu)2}  &  0 & 0 & \cdots & -\frac{1}{\sigma^2} & 0 \\
\sigma_{t1}^{(ux)2}   & \sigma_{t1}^{(u)2} & \cdots & \sigma_{tt}^{(ux)2} & \sigma_{tt}^{(u)2} +\frac{1}{\tau^2}  &  0 & 0 & \cdots & 0 &-\frac{1}{\tau^2} \\
- \frac{1}{\sigma^2} & 0 & \cdots & 0 & 0 &  \frac{1}{\sigma^2} & 0 & \cdots & 0 & 0\\
0  & -\frac{1}{\tau^2}& \cdots & 0 & 0 &  0 &  \frac{1}{\tau^2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots &\ddots & \vdots & \vdots \\
0 & 0& \cdots & -\frac{1}{\sigma^2}  &0&  0 & 0 & \cdots & \frac{1}{\sigma^2} & 0 \\
0 & 0 & \cdots & 0 & -\frac{1}{\tau^2}   &  0 & 0 & \cdots & 0 & \frac{1}{\tau^2}
\end{bmatrix} \\ 
& \triangleq \begin{bmatrix}
A_t& -B_t \\ -B_t^\top & B_t
\end{bmatrix},
\end{align}
where $B_t$ is a $2t\times 2t$ diagonal matrix of observation errors at time $t$ in the form of $\begin{bmatrix}
\frac{1}{\sigma^2}& \cdot & \cdot &  \cdot  &  \cdot \\  \cdot & \frac{1}{\tau^2} & \cdot &  \cdot  &  \cdot  \\ 
\vdots & \vdots & \ddots & \vdots & \vdots \\
 \cdot  &  \cdot  & \cdot  & \frac{1}{\sigma^2}&  \cdot \\  \cdot  &  \cdot & \cdot  &  \cdot  & \frac{1}{\tau^2}
\end{bmatrix}$. 
In fact, the matrix $A_t$ is a $2t \times 2t$ bandwidth six sparse matrix at time $t$ in the process. Then, we may find the covariance matrix by calculating the inverse of the precision matrix as 
\begin{align}
\Sigma & %=\begin{bmatrix}
%\left(A_t-B_t^\top B_t^{-1}B_t\right) ^{-1} & -\left(A_t-B_t^\top B_t^{-1}B_t\right)^{-1}B_t^\top B_t^{-1}\\
%- B_t^{-1}B_t\left(A_t-B_t^\top B_t^{-1}B_t\right)^{-1} & \left(B_t-B_t^\top A_t^{-1}B_t\right) ^{-1}
%\end{bmatrix} 
%&= \begin{bmatrix}
%\left(A_t-B_t\right) ^{-1} & \left(A_t-B_t\right)^{-1}\\
%\left(A_t-B_t\right)^{-1} & \left(I_t- A_t^{-1}B_t\right) ^{-1}B_t^{-1}
%\end{bmatrix} \\
\triangleq \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY} \\
\Sigma_{YX}  &\Sigma_{YY} 
\end{bmatrix}.
\end{align}
A detailed structure of the covariance matrix $\Sigma_{XX} $ is presented in Appendix \ref{covMatrixdetails}. 

\subsection{The Posterior Distribution}

To find the log-posterior distribution of $X_{1:t}$ and $Y_{1:t}$, we start from the joint distribution. Similarly, the inverse of the covariance matrix is 
%\begin{align}
%\Sigma_{YY}^{-1} &= B_t(I_t-A_t^{-1}B_t)= B_tA_t^{-1}\Sigma_{XX}^{-1}.
%\end{align}
the same as equation \eqref{inverseYY}. Additionally, the posterior distribution and log-likelihood function are the same form as equations \eqref{posteriortheta} and \eqref{logposteriorL}. 
%By using Choleski decomposition and similar technical solution, second term in the integrated objective function is 
%\begin{align}
%p(\theta \mid Y) &\propto p(Y\mid\theta)p(\theta) \propto \exp\left( -\frac{1}{2} Y \Sigma_{YY}^{-1} Y \right) \sqrt{\det \Sigma_{YY}^{-1}} P(\theta).
%\end{align}
%Then, by taking natural logarithm on the posterior of $\theta$ and by using the useful solutions in equations \eqref{sigmayy01} and \eqref{sigmayy02}, we will have
%\begin{align}\label{logL}
%\ln L(\theta) &= -\frac{1}{2}Y^\top\Sigma_{YY}^{-1}Y+\frac{1}{2}\sum\ln\mbox{tr}(B_t)-\sum\ln\mbox{tr}(L_t)+\sum\ln\mbox{tr}(R_t).
%\end{align}




\subsection{The Forecast Distribution}

It is known that 
\begin{align}
p(Y_{1:t-1},\theta) &= N\left( 0,\Sigma_{YY}^{(t-1)} \right)\\
p(Y_{t},Y_{1:t-1},\theta) &= N\left( 0,\Sigma_{YY}^{(t)} \right)\\
p(Y_{t}\mid Y_{1:t},\theta) &= N\left( \bar{\mu}_{t},\bar{\Sigma}_{t} \right)
\end{align}
where the covariance matrix of the joint distribution is $\Sigma_{YY}^{(t)} = \left(I_{t}-A_{t}^{-1}B_{t}\right)^{-1}B_{t}^{-1}$. Then, by taking its inverse, one can obtain 
\begin{align}
\Sigma_{YY}^{(t) (-1)} = B_{t}(I_{t}-A_{t}^{-1}B_{t}).
\end{align}
To be clear, the matrix $B_{t}$ is short for the matrix $B_{t}(\sigma^2,\tau^2)$, which is $2t\times 2t$ diagonal matrix with elements $\frac{1}{\sigma^2},\frac{1}{\tau^2}$ repeating for $t$ times on its diagonal. For instance, the very simple $B_1(\sigma^2,\tau^2) = 
\begin{bmatrix}
\frac{1}{\sigma^2} & 0  \\
0 & \frac{1}{\tau^2}
\end{bmatrix}_{2\times 2}$ is a $2\times 2$ matrix. 

Because of $A_t$ is symmetric and invertible, $B_t$ is the diagonal matrix defined as above, therefore they have the following property 
\begin{align}
& A_tB_t=A_t^\top B_t^\top = \left(B_tA_t\right)^\top, \\
& A_t^{-1}B_t = A_t^{-\top}B_t^\top = \left(B_tA_t^{-1}\right)^\top. 
\end{align}
Followed up the form of $\Sigma_{YY}^{(t) (-1)}$, we can define that 
\begin{align}
\Sigma_{YY}^{(t) (-1)} \triangleq \begin{bmatrix} 
B_{t-1} & 0 \\ 0 & B_1 \end{bmatrix}
\begin{bmatrix} 
Z_{t} & b_{t} \\
b_{t}^\top & K_{t}
\end{bmatrix} \begin{bmatrix} 
B_{t-1} & 0 \\ 0 & B_1\end{bmatrix}
\end{align}
where $Z_{t}$ is a $2t \times 2t$ matrix, $ b_{t} $ is a $2t \times 2$ matrix and $K_{t}$ is a $2 \times 2$ matrix. Thus,by taking its inverse again, we will get 
\begin{align} 
\Sigma_{YY}^{\left(t\right)}= \begin{bmatrix}
B_{t-1}^{-1} \left(Z_{t}-b_{t}K_{t}^{-1}b_{t}^\top\right)^{-1}B_{t-1}^{-1}  & - B_{t-1}^{-1}  Z_{t}^{-1}b_{t}\left(K_{t}-b_{t}^\top Z_{t}^{-1}b_{t}\right)^{-1}B_1^{-1} \\
-B_1^{-1}  K_{t}^{-1}b_{t}^\top \left(Z_{t}-b_{t}K_{t}^{-1}b_{t}^\top\right)^{-1}B_{t-1}^{-1}  & B_1^{-1}  \left(K_{t}-b_{t}^\top Z_{t}^{-1}b_{t}\right)^{-1}B_1^{-1} 
\end{bmatrix}.
\end{align}

It is easy to find the relationship of $A_{t-1}$ and $A_{t}$ satisfies  
\begin{align} A_{t} = 
\begin{bmatrix}
A_{t-1} & \cdot & \cdot  \\ \cdot &\frac{1}{\sigma^2} &\cdot  \\ \cdot  & \cdot  & \frac{1}{\tau^2} 
\end{bmatrix} + U_{t}U_{t}^\top \triangleq M_{t}  + U_{t}U_{t}^\top,
\end{align}
where, in fact, $M_{t} = \begin{bmatrix}
A_{t-1} & \cdot & \cdot  \\ \cdot &\frac{1}{\sigma^2} &\cdot  \\ \cdot  & \cdot  & \frac{1}{\tau^2}
\end{bmatrix}  = \begin{bmatrix}
A_{t-1} & 0 \\ 0 & B_1
\end{bmatrix}$ 
and its inverse is $M_{t}^{-1} =\begin{bmatrix}
A_{t-1}^{-1} & 0 \\ 0 & B_1^{-1}
\end{bmatrix}$. By using the Sherman-Morrison-Woodbury formula, one can find the inverse of $A_{t}$ in such a recursive way that 
\begin{equation}
A_{t}^{-1} = \left(M_{t}+U_{t}U_{t}^\top\right)^{-1}= M_{t}^{-1}-M_{t}^{-1}U_{t}\left(I_t+U_{t}^\top M_{t}^{-1}U_{t}\right)^{-1}U_{t}^\top M_{t}^{-1}.
\end{equation}
Consequently, after being simplified, it gives us 
\begin{equation}\label{OUupdatingK}
K_{t} =B_1^{-1}D_{t} \left(I_t+ S_{t}^\top \left(B_1^{-1} - K_{t-1}\right)  S_{t} +D_{t}^\top B_1^{-1}D_{t}  \right)^{-1}  D_{t}^\top B_1^{-1},
\end{equation}
and
\begin{align}
b_{t} = \begin{bmatrix}
-b_{t-1} \\ B_1^{-1}-K_{t-1} 
\end{bmatrix}  S_{t} \left(I_t+ S_{t}^\top \left(B_1^{-1} - K_{t-1}\right)  S_{t} +D_{t}^\top B_1^{-1}D_{t}  \right)^{-1} D_{t}^\top B_1^{-1}, 
\end{align}
by which, $K_t$ and $b_t$ are updated in a recursive way. As a result, one can obtain the following recursive updating formula for the mean and covariance matrix 
\begin{align}
\begin{split}
\bar{\mu}_{t}&=\Phi_{t} K_{t-1}B_1\bar{\mu}_{t-1} + \Phi_{t} \left(I_t-K_{t-1}B_1\right)Y_{t-1}\\
\bar{\Sigma}_{t}&=\left(B_1K_{t}B_1\right)^{-1}
\end{split}
\end{align}
The matrix $K_{t}$ is updated via equation \eqref{OUupdatingK}, or updating its inverse in the following form makes the computation faster, that is 
\begin{align}
K_{t}^{-1} &= B_1D_{t}^{-\top}D_{t}^{-1}B_1 + B_1\Phi_{t} \left(B_1^{-1} - K_{t-1}\right) \Phi_{t}^\top B_1+ B_1,\\
\bar{\Sigma}_{t} &= D_{t}^{-\top}D_{t}^{-1}+ \Phi_{t} \left(B_1^{-1} - K_{t-1}\right) \Phi_{t}^\top + B_1^{-1}
\end{align}
and $K_1 =B_1^{-1} - A_1^{-1} = \begin{bmatrix}
\frac{\sigma^4}{\sigma^2 +L_x^2} & 0 \\ 0 &\frac{\tau^4}{\tau^2 +L_u^2}
\end{bmatrix} $. For calculation details, readers can refer to Appendix \ref{OUcalculation}. 


\subsection{The Estimation Distribution}

Because of the joint distribution \eqref{jointmatrix}, one can find the estimation with a given $\theta$ is in the same form as equation \eqref{estimationdistribution}. 
%\begin{equation}
%X_{1:t} \mid Y_{1:t},\theta \sim N\left(L_t^{-\top}W_t,L_t^{-\top}L_t^{-1}\right),
%\end{equation}
%thus
%\begin{align}
%X _{1:t}= L_t^{-\top}\left(W_t+Z_t\right).
%\end{align}
%where $Z_t \sim N\left(0, I(\sigma,\tau)_t\right)$.
Being explicitly, for $X_{t}$, the joint distribution with $Y_{1:t}$ updated to time $t$ is 
\begin{align}
X_{t}, Y_{1:t} \mid \theta \sim N\left( 0, \begin{bmatrix}
C_{t}^\top\left(A_{t}-B_{t}\right) ^{-1}C_{t} & C_{t}^\top \left(A_{t}-B_{t}\right)^{-1}\\
\left(A_{t}-B_{t}\right)^{-1}C_{t} & \left(I_t- A_{t}^{-1}B_{t}\right) ^{-1}B_{t}^{-1}
\end{bmatrix} \right),
\end{align}
where $C_{t}^\top=\begin{bmatrix}
0 & \cdots & 0 & 1 & 0 \\ 0 & \cdots & 0 & 0 & 1 
\end{bmatrix}$ is a $2\times 2t$ matrix. Thus,
\begin{align}
X_{t}\mid Y_{1:t},\theta \sim N\left(\mu_{t}^{\left(X\right)},\Sigma_{t}^{\left(X\right)}\right),
\end{align}
where
\begin{align}
\mu_{t}^{\left(X\right)} & = C_{t}^\top A_t^{-1}B_tY_t =C_{t}^\top L_t^{-\top}W_t,\\
\Sigma_{t}^{\left(X\right)} & =C_{t}^\top A_t^{-1}C_{t} =U_{t}^\top U_{t},
\end{align}
and $U_{t} = L_t^{-1} C_{t}$.
%The filtering distribution of the state given parameters is $p\left(X_t\mid Y_{1:t}, \theta \right)$. To find its form, one can use the joint distribution of $X_{t}$ and $Y_{1:t}$, which is $p\left(X_{t}, Y_{1:t}  \mid  \theta\right)\sim N\left(0,\Gamma\right)$, where
%\begin{equation}
%\Gamma=\begin{bmatrix} C_{t}^\top\left(A-B\right)^{-1}C_{t} & C_{t}^\top\left(A-B\right)^{-1}\\\left(A-B\right)^{-1}C_{t} & \left(I-A^{-1}B\right)^{-1}B^{-1} \end{bmatrix}.
%\end{equation}
The recursive updating formula is  
\begin{align}
\mu_{t}^{\left(X\right)}  &=  K_{t}B_1\bar{\mu}_{t} + \left(I_t- B_1K_{t}\right)Y_{t}  \\
\Sigma_{t}^{\left(X\right)}  &=B_1^{-1}-K_{t}.
\end{align}





\subsection{Prior Distribution for Parameters}

The well known \textit{Hierarchical Linear Model}, where the parameters vary at more than one level, is first introduced by \cite{lindley1972bayes, smith1973general}. Hierarchical Model can be used on data with many levels, although 2-level models are the most common ones. The state-space model in equations \eqref{statemodel1} and \eqref{statemodel2} is one of Hierarchical Linear Model if $G_t$ and $F_t$ are linear, and non-linear model if $G_t$ and $F_t$ are non-linear processes. Researchers have made a few discussions and work on these both linear and non-linear models. In this section, we only discuss on the prior for parameters in these models. 

Various informative and non-informative prior distributions have been suggested for scale parameters in hierarchical models. \cite{gelman2006prior} give a discussion on prior distributions for variance parameters in hierarchical models. General considerations include using invariance \citep{jeffries1961theory}, maximum entropy \citep{jaynes1983papers} and agreement with classical estimators \citep{box2011bayesian}. Regarding informative priors, the author suggests to distinguish them into three categories: (\romannum{1}) is traditional informative prior. A prior distribution giving numerical information is crucial to statistical modeling and it can be found from a literature review, an earlier data analysis or the property of the model itself. (\romannum{2}) is weakly informative prior. This genre prior is not supplying any controversial information but are strong enough to pull the data away from inappropriate inferences that are consistent with the likelihood. Some examples and brief discussions of weakly informative priors for logistic regression models are given in \citep{gelman2008weakly}. (\romannum{3}) is uniform prior, which allows the information from the likelihood to be interpreted probabilistically. 

\cite{stroud2007sequential} discuss a model with different structures in the errors. The two errors $\omega_t$ and $\varepsilon_t$ are assumed normally distributed as
\begin{align}
\omega_t &\sim N(0,\alpha Q),\\
\varepsilon_t &\sim N(0,\alpha R),
\end{align}
where the two matrices $R$ and $Q$ are known and $\alpha$ is an unknown scale factor to be estimated. (Note that the forward map will be deterministic if $Q=0$.) The density of the Gaussian state-space model therefore becomes 
\begin{align}
p(y_t\mid x_t,\alpha) &= N(F(x_t),\alpha R),\\
p(x_t\mid x_{t-1},\alpha) &= N(G(x_{t-1}),\alpha Q).
\end{align}
The parameter $\alpha$ is assumed \textit{Inverse Gamma} distribution. 

For the priors of all the parameters in an OU-process, shown in equation \eqref{OUprocess} and \eqref{obseq}, first of all, we should understand what meanings of these parameters are standing for. The reciprocal of $\gamma$ is typical velocity falling in the reasonable range of 0.1 to 100 $m/s$. $\xi$ is the error occurs in transition process, $\sigma$ and $\tau$ are errors in the forward map for position and velocity respectively. Generally, the error is a positive finite number. Considering prior distributions for these parameters, before looking at the data, we have an idea of ranges where these parameters are falling in. Conversely, we do not have any assumptions about the true value of $\lambda$, which means it could be anywhere. According to this assumption, the prior distributions are 
\begin{align}
\gamma   &\sim IG(10,0.5),\\
\xi^2        &\sim IG(5,2.5),\\
\sigma^2 &\sim IG(5,2.5),
\end{align}
where $IG(\alpha,\beta)$ represents the \textit{Inverse Gamma} distribution with two parameters $\alpha$ and $\beta$. 
\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/ggIGPDF_Final.pdf}
\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/ggIGCDF_Final.pdf}
\caption{Probability density function and cumulative distribution function of \textit{Inverse Gamma} with two parameters $\alpha$ and $\beta$. }
\label{IGPDFCDF}
\end{figure}


\subsection{Efficiency of Delayed-Acceptance Metropolis-Hastings Algorithm}

We have discussed the efficiency of Metropolis-Hastings (MH) algorithm and how it is affected by the step size. To explain it explicitly, here we give an example comparing Eff (efficiency), EffUT (efficiency in unit time), ESS (effective sample size) and ESSUT (effective sample size in unit time), which are calculated by using the dataset, which is demonstrated in Figure \ref{realdatareview}, and running 10\,000 iterations of DA MH. We are taking a sequence from 0.1 to 4 with equal-space of 0.3, so that $s=\left\lbrace 0.1,\dots,4\right\rbrace$, and to solve criterion formula with each of the value. Table \ref{effeutessessutexampletable} and Figure \ref{effeutessessutexamplefigure} show the compares the results of the calculation. 

The best step size found by Eff is 1, which is as the same as that found by ESS. Let $s=1$ and run 1\,000 iterations, the DA MH takes 36.35 seconds to get the Markov chain for $\theta$ and the acceptance rates $\alpha_1$ for approximating $\hat{\pi}(\cdot)$ and $\alpha_2$ for estimating the posterior distribution $\pi(\cdot)$ are 0.3097 and 0.8324 respectively. By using EffUT and ESSUT, the best step size is 2.5, which is bigger. One of the advantages of using this step size is the significant decreasing of the computation time to 5.10 seconds. It is because the surrogate $\hat{\pi}(\cdot)$ takes bad proposals out and only good ones are accepted to pass to the next level. It can be seen from the lower rates $\alpha_1$ in Table \ref{effeutessessutexampletable}. 
%\begin{table}[h]
%\centering
%\begin{tabular}{\mid c\mid c\mid c\mid c\mid c\mid c\mid }
%\hline
%          & Values      & Time (in seconds) & Step Size & $\alpha_1$ & $\alpha_2$ \\ \hline
%Eff      & 0.0532      & 182.55 & 1.0   & 0.3270    & 0.7011     \\ \hline
%EffUT    & 0.0005      & 41.16 & 2.2   & 0.0687   & 0.5555    \\ \hline
%ESS     & 1275.6400 & 123.13 & 1.3   & 0.2180     & 0.6573   \\ \hline
%ESSUT & 13.8781   & 29.31   & 2.5   & 0.0469   & 0.5090   \\ \hline
%\end{tabular}
%\caption{An example of Eff, EffUT, ESS and ESSUT found by using the same data.  }
%\label{effeutessessutexampletable}
%\end{table}
%\begin{figure}[h]
%\centering
%\includegraphics[width=8cm,height=4cm]{Chapters/05MCMCOU/plots/eff.pdf}
%\includegraphics[width=8cm,height=4cm]{Chapters/05MCMCOU/plots/eut.pdf}
%\includegraphics[width=8cm,height=4cm]{Chapters/05MCMCOU/plots/ess.pdf}
%\includegraphics[width=8cm,height=4cm]{Chapters/05MCMCOU/plots/essut.pdf}
%\caption{An example of Eff, EffUT, ESS and ESSUT found by using the same data. }
%\label{effeutessessutexamplefigure}
%\end{figure}
\begin{table}[h]
\centering
\caption{An example of Eff, EffUT, ESS and ESSUT found by running 10\,000 iterations with same data. The computation time is measured in seconds~$s$. }
\label{effeutessessutexampletable}
\begin{tabular}{|c|C{2cm}|C{2cm}|C{2cm}|C{2cm}|C{2cm}|}
\hline
          & Values     & Time & Step Size & $\alpha_1$ & $\alpha_2$ \\ \hline
Eff      & 0.0515     & 36.35 & 1.0   & 0.3097    & 0.8324    \\ \hline
EffUT  & 0.0031     & 5.10   & 2.5   & 0.0360   & 0.7861   \\ \hline
ESS     & 501.4248 & 36.35 & 1.0   & 0.3097    & 0.8324     \\ \hline
ESSUT & 29.8912   & 5.10   & 2.5   & 0.0360   & 0.7861    \\ \hline
\end{tabular}
\end{table}
\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggeff.pdf}
	\caption{Efficiency against different step sizes}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggeut.pdf}
	\caption{EffUT against different step sizes}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggess.pdf}
	\caption{ESS against different step sizes}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/ggessut.pdf}
	\caption{ESSUT against different step sizes}
\end{subfigure}
\caption{Influences of different step sizes on sampling efficiency (Eff), efficiency in unit time (EffUT), effective sample size (ESS) and effective sample size in unit time (ESSUT) found with the same data}
\label{effeutessessutexamplefigure}
\end{figure}


On the surface, a bigger step size causes lower acceptance rates $\alpha_1$ and it might not be a smart choice. However, on the other hand, one should notice the less time cost. To make it sensible, we are running the DA MH with different step sizes, as presented in Table \ref{effeutessessutexampletable},  for the same (or similar) amount of time. Because of the bigger step size takes less time than smaller one, so we achieve a longer chain. To be more clear, we take 1\,000 samples out from a longer chain, such as 8\,500, and calculate Eff, EffUT, ESS and ESSUT separately by the embedded function \textsf{IAT}, \citep{christen2010general}, and \textsf{ESS} of the package \textsf{LaplacesDemon} in \textit{R} and the above formulas . As we can see from the outcomes, by running the similar amount of time, the Markov chain with a bigger step size has a higher efficiency and effective sample size in unit time. More intuitively, the advantage of using larger step size is the sampling algorithm generates more representative samples per second. Figure \ref{1koutof8kfigures} is comparing different $\theta$ chains found by using different step sizes but running the same amount of time. As we can see that $\theta$ with the optimal step size has a lower correlated relationship. 
\begin{table}[h]
\centering
\caption{Comparison of Eff, EffUT, ESS and ESSUT values with different step size. The $1000^\star$ means taking 1\,000 samples from a longer chain, like 1\,000 out of 5\,000 sample chain. The computation time is measured in seconds~$s$.}
\label{stepsizecompare}
\begin{tabular}{|c|C{1.5cm}|C{1.5cm}|C{1.5cm}|C{1.5cm}|C{1.5cm}|C{1.5cm}|}
\hline
Step Size& Length & Time & Eff   & EffUT & ESS & ESSUT \\ \hline
1.0    &   1\,000        & 3.48   & 0.0619 & 0.0178   &  69.4549     & 19.9583   \\ \hline
\multirow{2}{*}{1.3}    &   1\,400        & 3.40   & 0.0547 & 0.0161   &  75.3706   & 22.1678 \\ \cline{2-7}
    &   $1\,000^\star$ & 3.40 & 0.0813 & 0.0239  & 72.5370  & 21.3344   \\ \hline
\multirow{2}{*}{2.2}     &   5\,000          &  3.31 & 0.0201 &  0.0061  &  96.6623    & 29.2031   \\ \cline{2-7}
    &   $1\,000^\star$ & 3.31  &  0.0941 & 0.0284 & 94.2254 &  28.4669 \\ \hline
\multirow{2}{*}{2.5}     &   7\,000          &  3.62  & 0.0161 &0.0044  & 112.3134   &  31.0258    \\ \cline{2-7}
  &   $1\,000^\star$ &  3.62 & \textbf{0.1095} &  \textbf{0.0302}  &  \textbf{\small 113.4063} & \textbf{31.3277} \\ \hline
\end{tabular}
\end{table}


%\begin{table}[h]
%\centering
%\begin{tabular}{|c|c|c|cc|c|c|}
%\hline
%Step Size& Length of Data  & Time (in seconds)  & Eff   & EffUT & ESS & ESSUT \\ \hline
%1.0    &   1\,000        & 3.57  & 0.0625 & 0.0175   &  61.6266     & 17.2623   \\ \hline
%1.3    &   1\,400        & 3.35  & 0.0463 & 0.0138   &  64.5524     & 19.26938  \\ \hline
%1.3    &   $1\,000^\star$ & 3.35 & 0.0638 & 0.0190   & 64.1237     & 19.1414   \\ \hline
%%2.2    &   5\,000          &  3.79&  0.0215 &  0.0057  & 109.5759    & 28.9118    \\ \hline
%%2.2    &   $1\,000^\star$ & 3.79 & 0.1025 & 0.0270  &  108.9920 & 28.7578 \\ \hline
%%2.5    &   7\,000          & 3.62  & 0.0157 & 0.0043  & 100.9355   & 27.8827    \\ \hline
%%2.5    &   $1\,000^\star$ & 3.62  & 0.1079 & 0.0298  &  97.1703 & 26.8426 \\ \hline
%2.2    &   5\,000          &  3.67 & 0.0214 &  0.0058  &  92.1885    & 25.1195   \\ \hline
%2.2    &   $1\,000^\star$ & 3.67  &  0.0963 & 0.0262 &  89.6414 & 24.4255 \\ \hline
%2.5    &   7\,000          &  3.70  & 0.0157 & 0.0043  & 103.9234   & 28.0874    \\ \hline
%2.5    &   $1\,000^\star$ &  3.70 & \textbf{0.1089} &  \textbf{0.0294}  &  \textbf{113.8122} & \textbf{30.7601} \\ \hline
%\end{tabular}
%\caption{Comparing Eff, EffUT, ESS and ESSUT values using different step size. The $1000^\star$ means taking 1\,000 samples from a longer chain, like 1\,000 out of 5\,000 sample chain. }
%\label{stepsizecompare}
%\end{table}



%
%\begin{table}[h]
%\centering
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%Step Size& Length of Data  & Time (in seconds)  & Eff   & EffUT & ESS & ESSUT \\ \hline
%1.0    &   1000               & 4.07  & 0.0517 & 0.0127   & 41.5770     & 10.2155   \\ \hline
%1.3    &   1300               & 4.11  & 0.0417 & 0.0101   & 49.4266     & 12.0259   \\ \hline
%1.3    &   $1000^\star$ & 4.11  & 0.0545 & 0.0133   & 49.8249     & 12.1228   \\ \hline
%2.2    &   5000               &  4.10 &  0.0181  & 0.0044  & 79.3274    & 19.3481    \\ \hline
%2.2    &   $1000^\star$ & 4.10 & \textbf{0.0893} & \textbf{0.0218}  &  \textbf{82.5684} & \textbf{20.1386}  \\ \hline
%2.5    &   8500               & 4.06  & 0.0096 & 0.0024   & 73.4414     & 18.0890    \\ \hline
%2.5    &   $1000^\star$ & 4.06  & 0.0779& 0.0191   &  71.4099  & 17.5887  \\ \hline
%\end{tabular}
%\caption{Comparing Eff, EffUT, ESS and ESSUT values using different step size. The $1000^\star$ means taking 1000 samples from a longer chain, like 1000 out of 5000 sample chain. }
%\label{stepsizecompare}
%\end{table}

\subsection{A Sliding Window State and Parameter Estimation Approach}

The length of data used in the algorithm really affects the computation time. The forecast distribution $p(Y_{t}\mid Y_{1:t-1},\theta)$ and estimation distribution $p(X_{t}\mid Y_{1:t},\theta)$ require finding the inverse of the covariance $\Sigma_{YY}^{(t+1)}$, however, which is time consuming if the sample size is big to generate a large sparse matrix. For a moving vehicle, one is more willing to get the estimation and moving status instantly rather than being delayed. Therefore a compromise solution is the fixed-length sliding window sequential filter. A fixed-lag sequential parameter learning method was proposed by \cite{polson2008practical} and named as \textit{Practical Filtering}. The authors rely on the approximation of 
\begin{equation}
p(x_{0:n-L},\theta\mid y_{0:n-1}) \approx p(x_{0:n-L},\theta \mid y_{0:n})
\end{equation}
for large $L$. The new observations coming after the $n$th data has little influence on $x_{0:n-L}$. 

Being inspired, we do not use the first $0$ to $n-1$ date and ignore the latest $n$th, on the contrary, use all the latest date with truncating the first few history ones. Suppose we are given a fixed-length $L$, up to time $t$ ($t>L$),  we estimate the $x_t$ by using all the retrospective observations to the point at $t-L+1$. In another word, the estimation distribution for the current state is 
\begin{equation}
p(X_{t}\mid Y_{t-L+1:t},\theta),
\end{equation}
where $t>L$. We name this method the \textit{Sliding Window Sequential Parameter Learning Filter}. 

The next question is how to choose an appropriate $L$. The length of data used in MH and DA MH algorithms has an influence on the efficiency and accuracy of parameter learning and state estimation. Being tested on real data set, there is no doubt that the more data be in use, the more accurate the estimation is, and lower efficient is in computation. In Table \ref{lengthofdatacompare}, one can see the pattern of parameters $\gamma,\xi,\tau$ follow the same trend with the choice of $L$ and $\sigma$ increases when $L$ decreases. Since estimation bias is inevitable, we are indeed to keep the bias as small as possible, and in the meantime, the higher efficiency and larger effective sample size are bonus items. In Figure \ref{compareLengthData}, we can see that the efficiency and effective sample size is not varying along with the sample size used in sampling algorithm, but in unit time, they are decreasing rapidly as data size increases. 
\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUlengtheff.pdf}
    \caption{Efficiency against data length}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUlengtheffut.pdf}
    \caption{EffUT against data length}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUlengthess.pdf}
    \caption{ESS against data length}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
    \includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/simudataOUlengthessut.pdf}
    \caption{ESSUT against data length}
\end{subfigure}
\caption{Comparison of efficiency (Eff), efficiency in unit time (EffUT), effective sample size (ESS) and effective sample size in unit time (ESSUT) against the different length of data. Increasing data length does not significantly improve the efficiency and ESSUT.}\label{compareLengthData}
\end{figure}
In addition, from a practical point of view, the observation error $\sigma$ should be kept at a reasonable level, let's say $50cm$, and the computation time should be as less as possible. To reach that level, $L=100$ is an appropriate choice. For a one-dimensional linear model, $L$ can be chosen larger and that does not change too much. If the data up to time $t$ is less than or equal to the chosen $L$, the whole data set is used in learning $\theta$ and estimating $X_t$. 


For the true posterior, the algorithm requires a cheap estimation $\hat{\pi}(\cdot)$, which is found by one-variable-at-a-time Metropolis-Hastings algorithm. The advantage is getting a precise estimation of the parameter structure, and disadvantage is, obviously, lower efficiency. Luckily, we find that it is not necessary to run this MH every time when estimate a new state from $x_{t-1}$ to $x_t$. In fact, in the DA MH process, the cheap $\hat{\pi}$ does not vary too much in the filtering process with new data coming into the dataset. We may use this property in the algorithm. First of all, we use all available data from $1$ to $t$ with length up to $L$ to learn the structure of $\theta$ and find out the cheap approximation $\hat{\pi}$. Then, use DA MH to estimate the true posterior $\pi$ for $\theta$ and $x_t$. After that, extend dataset to $1:t+1$ if $t\leq L$ or shift the data window to $2:t+1$ if $t>L$ and run DA MH again to estimate $\theta$ and $x_{t+1}$. From figures \ref{batchwindowkeyfeature} and \ref{batchwindowparameter}, we can see that the main features and parameters in the estimating process between batch method and sliding window method have not significant differences. 


To avoid estimation bias, which is caused by sampling degeneracy, we are introducing a \textit{threshold} criterion and a \textit{cutting-off} value. In a certain circumstance, the cheap $\hat{\pi}(\cdot)$ is not accurate and is replaced by a new one $\hat{\pi}_{\mbox{\scriptsize new}}(\cdot)$. The \textit{cutting-off} procedure stops the algorithm when a large $\Delta_t$ occurs in the progress. A large time gap indicates a break of the vehicle at a time point and it causes irregularity and bias. A smart way is to stop the process and to wait for new data coming in. By running testings on real data, the \textit{threshold} is chosen $\alpha_2<0.7$ and the \textit{cutting-off} value is set at $\Delta_t\geq 300$. For each time, if the acceptance rate $\alpha_2$ is less than $0.7$, we update the mean of $\hat{\pi}$ and remain the covariance unchanged. 


In fact, the mean of the estimation may vary upon the data but the covariance matrix does not change too much, as is shown in Figure \ref{ParameterEvolutionVisualization}. Actually, these two values are on researchers' choices. Figures \ref{comparenotanupDAL} and \ref{comparenotanupfeatures} compare the performances of using and not using the \textit{threshold} criterion to update the mean of the parameters. We can see that by using the \textit{threshold} criterion, we effectively avoid estimation bias and obtain more effective samples. 

 
\begin{figure}[ht]
\centering
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdataestbiaslogDAnoupdate.pdf}
	\caption{$\ln DA$ surfaces of not-updating-mean}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdataestbiaslogDAupdate.pdf}
	\caption{$\ln DA$ surfaces of updating-mean}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdataestbiaslogLnoupdate.pdf}
	\caption{$\ln L$ surfaces of not-updating-mean}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdataestbiaslogLupdate.pdf}
	\caption{$\ln L$ surfaces of updating-mean}
\end{subfigure}
\caption{Comparing $\ln DA$ and $\ln L$ surfaces between not-updating-mean and updating-mean methods. It is obviously that the updating-mean method has higher dense log-surfaces, which contain more effective samples.} \label{comparenotanupDAL}
\end{figure}

\begin{figure}[ht]
\centering
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdatacomparea1notupandup2.pdf}
   \caption{Comparing $\alpha_1$}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdatacomparea2notupandup2.pdf}
   \caption{Comparing $\alpha_2$}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdatacompareeffutnotupandup2.pdf}
   \caption{Comparing EffUT}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdatacompareessutnotupandup2.pdf}
   \caption{Comparing ESSUT}
\end{subfigure}
\caption{Comparing acceptance rates $\alpha_1$, $\alpha_2$, EffUT and ESSUT between not-updating-mean and updating-mean methods. Black solid dots $\bullet$ indicate values obtained from not-updating-mean method and black solid triangular $\blacktriangle$ indicate values obtained from updating-mean method. The acceptance rates of the updating-mean method are more stable and effective samples are larger in unit computation time. }\label{comparenotanupfeatures}
\end{figure}


Consequently, a complete form of this algorithm is summarized in the following Algorithm \ref{algorithmslidingwindow}: 
\begin{algorithm}[ht]
\SetAlgoLined 
Initialization: Set up $L$, \textit{threshold} and  \textit{cutting-off} criteria. \\
Learning phase: Estimate $\theta$ with $p\left(\theta\mid Y_{1:\min \left\lbrace t,L\right\rbrace } \right) \propto p\left(Y_{1:\min \left\lbrace t,L\right\rbrace } \mid \theta \right)p\left(\theta \right)$ by one-variable-at-a-time Random Walk Metropolis-Hastings algorithm gaining the target acceptance rates and find out the structure of $\theta\sim N\left(\mu,\Sigma\right)$ and the approximation $\hat{\pi}\left(\cdot\right)$. \label{algorithmlearningsurface}\\
Estimation phase: draw samples for $\theta$ and $X_{ \max\left\lbrace 1,t-L+1 \right\rbrace :\min \left\lbrace t,L\right\rbrace }$ given $Y_{ \max\left\lbrace 1,t-L+1 \right\rbrace :\min \left\lbrace t,L\right\rbrace }$: \For{$i$ from 1 to $N$}{ \label{algorithmestimaiton}
Propose $\theta_i^*$ from $N\left(\theta_i\mid\mu,\Sigma\right)$, accept it with probability $\alpha_1=\min\left\lbrace  1,\frac{\hat{\pi}\left(\theta_i^*\right)q\left(\theta_i, \theta_i^*\right)}{\hat{\pi}\left(\theta_i\right)q\left(\theta_i^*, \theta_i\right)}  \right\rbrace$ and go to next step; otherwise go to step \ref{algorithmDA}.\label{algorithmDA}\\
Accept $\theta_i^*$ with probability $\alpha_2=\min \left\lbrace  1,\frac{\pi\left(\theta_i^*\right)\hat{\pi}\left(\theta_i\right) }{\pi\left(\theta_i\right)\hat{\pi}\left(\theta_i^*\right)} \right\rbrace$ and go to next step; otherwise go to step \ref{algorithmDA}. \\
Calculate $\mu_i^{\left(t\right)},\Sigma_i^{\left(t\right)}$ for $X_t$ and $\mu_i^{\left(t+s\right)},\Sigma_i^{\left(t+s\right)}$ for $X_{t+s}$.\\
}
Calculate $\mu_X^{\left(t\right)} = \frac{1}{N} \sum_i \mu_i^{\left(t\right)}$, $\Var\lbrack X^{\left(t\right)}\rbrack = \frac{1}{N} \sum_i \left(\mu_i^{\left(t\right)} \mu_i^{\left(t\right)\top} +\Sigma_i\right) -\frac{1}{N^2} \left(\sum_i  \mu_i^{\left(t\right)}\right) \left(\sum_i \mu_i^{\left(t\right)}\right)^\top$ and $\mu_X^{\left(t+s\right)}$, $\Var\lbrack X^{\left(t+s\right)}\rbrack$ with the same formula.  \\
Check \textit{threshold} and  \textit{cutting-off} criteria. \uIf{\textit{threshold} is TRUE}{Update $\theta\sim N\left(\mu,\Sigma\right)$}\uElseIf{ \textit{cutting-off} is TRUE}{Stop process. }
\Else{ Go to next step.}
Shift the window by setting $t = t+1$ and go back to step \ref{algorithmestimaiton}.
 \caption{Sliding Window Adaptive MCMC}\label{algorithmslidingwindow}
\end{algorithm}


\clearpage

\subsection{Application on 2-Dimensional GPS Data}

An application of the Algorithm \ref{algorithmslidingwindow} is to track the position of a moving tractor on a farm. The original GPS dataset is plotted in Figure \ref{realGPSdataset}. In a 2-dimensional trajectory filtering problem, we use the same parameter $\theta=\lbrace \gamma,\xi^2,\lambda^2,\sigma^2,\tau^2 \rbrace$ for both easting and northing directions. The observations on these two directions are denoted as $Y_E$ and $Y_N$ respectively. The hidden states on easting and northing directions are $X_E$ and $X_N$. 


To speed up the estimation, we should get an idea of what the parameter space looks like by running step \ref{algorithmlearningsurface} of the algorithm with a subset of observations. By setting $L=100$ and running 5\,000 iterations, we find 5\,000 samples for $\theta$ in 59 seconds. For each parameter of $\theta$, we take 1\,000 sub-samples out as a new sequence. The new $\theta^*$ is representative for the parameter space. Then the traces and correlation are derived from $\theta^*$. Meanwhile, the acceptance rates for each parameter are $\alpha_\gamma = 0.453,\alpha_{\xi^2}=0.433, \alpha_{\lambda^2}=0.435, \alpha_{\sigma^2}=0.414, \alpha_{\tau^2}=0.4490$ respectively. Hence, the structure of $\hat{\theta}\sim N\left( m_t,C_t\right)$, which is depicted in Figure \ref{realdatacorMatrix}, is obtained. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Chapters/05MCMCOU/plots/realdatalearningcorMatrix.pdf}
	\caption{Visualization of the parameters correlation matrix, which is found in the learning phase. }\label{realdatacorMatrix}
\end{figure}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\textwidth]{Chapters/05MCMCOU/plots/realdatalearninggam.pdf}
    \caption{Trace plot of $\gamma$}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\linewidth]{Chapters/05MCMCOU/plots/realdatalearningxi2.pdf}
 	\caption{Trace plot of $\xi^2$}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\linewidth]{Chapters/05MCMCOU/plots/realdatalearninglab2.pdf}
	\caption{Trace plot of $\lambda^2$}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\linewidth]{Chapters/05MCMCOU/plots/realdatalearningsig2.pdf}
	\caption{Trace plot of $\sigma^2$}
\end{subfigure}
\begin{subfigure}[t]{0.45\textwidth}
	\includegraphics[width=\linewidth]{Chapters/05MCMCOU/plots/realdatalearningtau2.pdf}
	\caption{Trace plot of $\tau^2$}
\end{subfigure}
\caption{Trace plots of $\theta$ after taking 1\,000 burn-in samples out from 5\,000 from the learning phase.}
\end{figure}



\clearpage

Since a cheap surrogate $\hat{\pi}(\cdot)$ for the true $\pi(\cdot)$ is found in step \ref{algorithmlearningsurface} (the learning phase), it is time to move on to the estimation phase. Algorithm \ref{algorithmslidingwindow} takes fixed $L$ length data from $\{Y_E,Y_N\}_{1:L}$ to $\{Y_E,Y_N\}_{t-L+1:t}$ until an irregular large time lag meets the cutting-off criterion. In the implementation, the first cutting-off occurs after the $648$-th point. The first $100$ estimates $\{X_E,X_N\}_{1:100}$ were found in the learning phase and $\{X_E,X_N\}_{101:648}$ were found sequentially in the estimation phase with approximate 9 seconds per 10\,000 iterations for each $\{X_E,X_N\}_s, s\in \lbrack 101,648\rbrack$. The outcome is depicted in Figure \ref{MCMCfirstportionestimation}. 


\begin{figure}[h]
\centering
%\includegraphics[width=0.8\textwidth]{Chapters/05MCMCOU/plots/realdatabatchPosition2whole.pdf}
%\includegraphics[width=0.8\textwidth]{Chapters/05MCMCOU/plots/realdatabatchVelocity2.pdf}
\begin{tikzpicture}
	\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.9\textwidth]{Chapters/05MCMCOU/plots/realdatabatchPosition3.pdf}};
	\begin{scope}[
	x={(image.south east)},
	y={(image.north west)}
	]
	\node [black, font=\bfseries] at (0.5,0) {Easting};
	\node [black, font=\bfseries,rotate=90] at (0,0.5) {Northing};
	\end{scope}
\end{tikzpicture}
\caption{Estimations of $Z$ found by combined batch and sequential methods. The red line is the estimation by batch method and the green line is the sequential MCMC filtering estimation. Black dots are the measurements.}\label{MCMCfirstportionestimation}
\end{figure}

The means of uncertainties in the estimation for each direction are about 0.5 meters. Figure \ref{MCMCErrorFill} depicts uncertainties of the estimation before the first cutting-off procedure activated. The shaded blue filling indicates that there are larger uncertainties at turning points. In the estimation phase, Algorithm \ref{algorithmslidingwindow} is able to estimate $X_t$ and to predict $X_{t+s}$. However, when $s$ goes along time $t$, the uncertainty becomes larger. When a new observation $X_{t+1}$ comes into the data stream, the uncertainty shrinks. 



\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/tracErrorFillX.pdf}};
	\begin{scope}[
	x={(image.south east)},
	y={(image.north west)}
	]
	\node [black, font=\bfseries] at (0.5,0) {$t$};
	\node [black, font=\bfseries,rotate=90] at (0,0.5) {Easting};
	\end{scope}
	\end{tikzpicture}
		\begin{tikzpicture}
	\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/tracErrorFillY.pdf}};
	\begin{scope}[
	x={(image.south east)},
	y={(image.north west)}
	]
	\node [black, font=\bfseries] at (0.5,0) {$t$};
	\node [black, font=\bfseries,rotate=90] at (0,0.5) {Northing};
	\end{scope}
	\end{tikzpicture}
	\caption{Uncertainties on easting and northing directions before the first cutting-off procedure. The means of uncertainties on each direction are about 0.5 meters. }\label{MCMCErrorFill}
\end{figure}



%\begin{figure}[h]
%\centering
%\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/realdataEstXYwithEr.pdf}
%\includegraphics[width=0.45\textwidth]{Chapters/05MCMCOU/plots/realdataestwitherror.pdf}
%\caption{Zoom in on estimations. For each estimation $\hat{X}_i (i=1,\dots,t)$, there is an error circle around it. }
%\end{figure}


After a cutting-off procedure is activated, the adaptive MCMC algorithm goes off-line and accumulates measurements until there are enough, for example 100, for continued estimation \footnote{Alternatively, a ``hot start'' is possible in which the priors are the posteriors of the previous estimation phase and no learning phase is required.}. In the application, we can see that the first 100 observations are used for parameter estimation in the learning phase. Once this step is done, the sequential estimation phase goes on-line for filtering calculation. Because there is a large time lag between the 648-th and 649-th points, the algorithm goes off-line again to accumulate data in the learning phase, and then goes back to on-line for filtering estimation. 

Figure \ref{MCMCwholeestimation} gives the whole estimated trajectory by the proposed sliding window MCMC algorithm. There are two main learning phase occur on the entire dataset. The first learning phase uses the first 100 data and the second learning phase uses the data from 649 to 748. With the information obtained from the learning phase, two sequential estimation phases estimate the data from 101 to 648 and from 749 to 1121 respectively. However, when the third large time lag occurs after the 1121-th point, the algorithm has insufficient observations to run a third learning phase. In this figure, the on-line/off-line switching points are colored in yellow. 


\begin{figure}[h]
	\centering
	\begin{tikzpicture}
	\node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=0.9\textwidth]{Chapters/05MCMCOU/plots/realdatabatchPositionwhole2.pdf}};
	\begin{scope}[
	x={(image.south east)},
	y={(image.north west)}
	]
	\node [black, font=\bfseries] at (0.5,0) {Easting};
	\node [black, font=\bfseries,rotate=90] at (0,0.5) {Northing};
	\node [black, font=\bfseries] at (0.68,0.68) {1};
	\node [black, font=\bfseries] at (0.15,0.75) {2};
	\node [black, font=\bfseries] at (0.94,0.83) {3};
	\node [black, font=\bfseries] at (0.64,0.08) {4};
	\end{scope}
	\end{tikzpicture}
	\caption{Two learning phases are colored by red and two sequential estimation phases are colored by green. The algorithm is not able to estimate the data from 1121 till the end because of the lack of observations. Point 1 is the first point of the data stream. Points 2 and 3 are the switching points. Point 4 is the last point of the data stream. }\label{MCMCwholeestimation}
\end{figure}



\section{Discussion and Future Work}


In this chapter, an adaptive MCMC algorithm is proposed for estimating combined state and parameter in a homogeneous linear state-space model. The whole process is split into two phases: learning phase and estimation phase. In the learning phase, a self-tuning one-variable-at-a-time random walk Metropolis-Hastings algorithm is used to learn the structure of the parameter space. After getting a cheap surrogate for the expensive posterior distribution, it is then used in a delayed-acceptance algorithm in the estimation phase. 

Note that in the learning phase, we determine an approximation for the posterior distribution to be used in the delayed-acceptance MH algorithm. This is quite different to population MCMC \citep{laskey2003population}, in which multiple chains are used to determine a better proposal distribution. This does, however, suggest that multiple chains could be used to improve the learning phase. 


In on-line mode, the algorithm is adaptive to maintain sampling efficiency and uses a sliding window approach to maintain sampling speed. At the end of this chapter, the algorithm is applied to on-line estimation on a 2-dimensional GPS dataset. 


The advantage of this algorithm is that it is easy to understand and to implement in practice. In contrast, Particle Learning algorithm is highly efficient, however, the sufficient statistics are not available at all times. 

The sliding window adaptive MCMC algorithm should be contrasted with the V-spline algorithm proposed in Chapter \ref{ChapterTS}. The sliding window adaptive MCMC algorithm is a filtering algorithm that is designed for fast estimation. The V-spline is a smoothing algorithm that uses all the data for entire trajectory estimation and parameter optimization can be time consuming. On the other hand, the V-spline has piecewise continuous second derivatives, whereas the forward map \eqref{OUprocess} built into our sliding window adaptive MCMC algorithm implies sample paths are not twice differentiable.

The gradient boosting V-spline, discussed in Chapter \ref{ChapterFuture}, is potentially a much faster algorithm that could also be employed in on-line mode. Like the V-spline, the forward map used in the adaptive MCMC algorithm could also incorporate vehicle operating characteristics. However, it would be important to maintain the efficiency of the MCMC sampler in higher dimensions. 

