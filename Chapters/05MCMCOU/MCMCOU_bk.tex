


\section{Combined State and Parameters Estimation of Sequential Monte Carlo Algorithm}

To work out the best estimations for $x$ and $u$, one has to solve the target function
\begin{align}\label{objecfun}
p(X\mid Y) = \int p(X\mid Y,\theta)p(\theta\mid Y)d\theta.
\end{align} 
The main work need to be done is finding an efficient way to sort out the integration in the above equation. Several methods can be used, such as cross validation, Expectation Maximization algorithm, Gibbs sampling and Metropolis-Hastings algorithm and so on. A Monte Carlo method is popular in research area solving this problem. Monte Carlo method is an algorithm that relies on repeated random sampling to obtain numerical results. To compute an integration of $\int f(x)dx$, one has to sampling as many independent $x_i \mbox{ } (i = 1,\cdots, N)$ as possible and numerically to find $\frac{1}{N}\sum_i f(x_i)$ to approximate the target function. 

In our target function, we have to sampling $\theta$ and use a numerical way to calculate its integration. Here are two ways of solving the sampling problem sequentially: 
\begin{align*}
\begin{cases}
\mbox{M1}: & p(\theta\mid Y_{1:t},Y_{t+1}) \propto p(Y_{1:t},Y_{t+1}\mid\theta)p(\theta)\\
\mbox{M2}: & p(\theta\mid Y_{1:t},Y_{t+1}) \propto p(Y_{t+1}\mid\theta,Y_{1:t})p(\theta\mid Y_{1:t})
\end{cases}.
\end{align*}


NOTES: add more..........


\subsection{General Linear Space}
In one dimensional state space model, we consider the hidden state process $\{x_t, t\geq 1\}$ is a stationary and ergodic Markov process and transited by $f(x'\mid x)$. In this paper, we assume that the current state $x_t$ only depends on the previous one step $x_{t-1}$, which is known as $\textit{AR(1)}$ model. As indicated by its name, the states are not observed directly but by another process $\{y_t, t\geq 1\}$, which is assumed depending on $\{x_t\}$ by the process $g(y\mid x)$ only and independent with each other. If the transition processes $f$ and $g$ are linear and normal distributed, we call this model $\textit{Linear Gaussian Model}$, that can be written as
\begin{align*}
y_t\mid x_t      &\sim N(\gamma x_t,\sigma^2) \\
x_t\mid x_{t-1} &\sim N(\phi x_{t-1},\tau^2),
\end{align*}
where $\sigma$ and $\tau$ are errors occurring in processes, $\gamma$ and $\phi$ are static process parameters. 

In a simple scenario, by assuming $\gamma=1$ gives us the joint distribution for $x_{0:t}$ and $y_{1:t}$ as following
\begin{equation*}
\begin{bmatrix} x\\y \end{bmatrix}
\sim N\left(0, \Sigma  \right),
\end{equation*}
where $\Sigma^{-1}$ looks like
\begin{equation*}
\begin{bmatrix}
\frac{1}{L^2}+\frac{\phi^2}{\tau^2} & \frac{-\phi}{\tau^2} & \cdots & 0 & 0 & 0& \cdots & 0\\\\
\frac{-\phi}{\tau^2}   & \frac{1+\phi^2}{\tau^2}+\frac{1}{\sigma^2}& \cdots & 0 & -\frac{1}{\sigma^2} &0 & \cdots & 0 \\
0 & \frac{-\phi}{\tau^2}   &  \cdots & 0 & 0& -\frac{1}{\sigma^2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0   &  \cdots & \frac{1}{\tau^2}+\frac{1}{\sigma^2} & 0 & 0 & \cdots &-\frac{1}{\sigma^2}\\
0 & -\frac{1}{\sigma^2}  & \cdots & 0 & \frac{1}{\sigma^2} & 0 & \cdots & 0 \\
0& 0 & \cdots & 0 & 0 &  \frac{1}{\sigma^2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0& \cdots &-\frac{1}{\sigma^2} & 0 & 0 & \cdots &  \frac{1}{\sigma^2}
\end{bmatrix},
\end{equation*}
is the general procedure matrix denoted as $\Sigma^{-1}=\begin{bmatrix} A & -B \\ -B & B \end{bmatrix}$. Its inverse is 
\begin{equation}
\Sigma=\begin{bmatrix} (A-B)^{-1} &  (A-B)^{-1} \\ (A-B)^{-1} & (I-A^{-1}B)^{-1}B^{-1} \end{bmatrix} \triangleq \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY}  \\ \Sigma_{YX} & \Sigma_{YY} 
\end{bmatrix}
\end{equation}
the covariance matrix, where $B$ is a $t\times t$ diagonal matrix with elements $\frac{1}{\sigma^2}$. The covariance matrices $\Sigma_{XX} =  (A-B)^{-1}$ and $\Sigma_{YY} =  (I-A^{-1}B)^{-1}B^{-1}$ are easily found. Here the parameter $\theta$ represents for the unknown parameters $\phi,\sigma,\tau$. 

To find a recursive way of calculating the log likelihood posteriors, we introduce the Sherman-Morrison-Woodbury formula here first. In the late 1940s and the 1950s, Sherman and Morrison\cite{sherman1950adjustment}, Woodbury \cite{woodbury1950inverting}, Bartlett \cite{bartlett1951inverse} and Bodewig \cite{bodewig1959matrix} discovered the following result. The original Sherman-Morrison-Woodbury (for short SMW) formula has been used to consider the inverse of matrices \cite{deng2011generalization}. In this paper, we will consider the more generalized case. 

Theorem 1.1 (Sherman-Morrison-Woodbury). Let $A \in B(H)$ and $G \in B(K)$ both be invertible, and $Y, Z \in B(K, H)$. Then $A + YGZ^*$ is invertible if and only if $G^{-1} + Z^∗A^{-1}Y$ is invertible. In which case,
\begin{equation}
(A+YGZ^*)^{-1}= A^{-1}-A^{-1}Y(G^{-1}+Z^∗A^{-1}Y)^{-1}Z^∗A^{-1}.
\end{equation}

%\begin{itemize}
%\item [1] Chun Yuan Deng, A generalization of the Sherman-Morrison-Woodbury formula.
%\item [3] J. Sherman, W.J. Morrison, Adjustment of an inverse matrix corresponding to a change in one element of a given matrix, Ann. Math. Statist. 21 (1950)
%124–127.
%\item [4] M.A. Woodbury, Inverting Modifed Matrices, Technical Report 42, Statistical Research Group, Princeton University, Princeton, NJ, 1950.
%\item [5] M.S. Bartlett, An inverse matrix adjustment arising in discriminant analysis, Ann. Math. Statist. 22 (1951) 107–111.
%\item [6] E. Bodewig, Matrix Calculus, North-Holland, Amsterdam, 1959.
%\end{itemize}

A simple form of SMW formula is Sherman-Morrison formula represented in the following statement \cite{bartlett1951inverse}:
Suppose $A\in R^{n\times n}$ is an invertible square matrix and $u,v\in R^n$ are column vectors. Then $A+uv\top$ is invertible $\iff 1+u^\top A^{-1}v\neq 0$. If $A+uv\top$ is invertible, then its inverse is given by
\begin{equation}
(A+uv^{T})^{-1}=A^{-1}-{A^{-1}uv^{T}A^{-1} \over 1+v^{T}A^{-1}u}.
\end{equation}



\subsubsection{The Forecast Distribution $p(y_{t+1}\mid y_{1:t},\theta)$}

The joint distribution for $y_{t+1}$ and $y_{1:t}$ is $p(y_{1:t+1}\mid \theta)\sim N(0,\Sigma_{YY})$, where $\Sigma_{YY} = (I-A^{-1}B)^{-1}B^{-1}$ is the covariance matrix given above. One may find the inverse of the covariance matrix 
\begin{align*}
\Sigma_{YY}^{-1} = B(I-A^{-1}B) =\frac{1}{\sigma^4}(\sigma^2 I-A^{-1}) \triangleq \frac{1}{\sigma^4} \left[\begin{matrix} 
Z_{t+1} & b_{t+1} \\
b_{t+1}^\top & K_{t+1}
\end{matrix} \right].
\end{align*}
Therefore, the original form of this covariance is 
\begin{align*} \Sigma_{YY} =\sigma^4 \left[ \begin{matrix}
(Z-bK^{-1}b^\top)^{-1} & -Z^{-1}b(K-b^\top Z^{-1}b)^{-1}\\
-K^{-1}b^\top (Z-bK^{-1}b^\top)^{-1} & (K-b^\top Z^{-1}b)^{-1}
\end{matrix}\right].
\end{align*}
For sake of simplicity, here we are using $Z$ to represent the $t\times t$ matrix $Z_{t+1}$, $b$ to represent the $t \times 1$ vector  $b_{t+1}$  and $K$ to represent the $1\times 1$ constant $K_{t+1}$. 


By denoting $C_{t+1} = \left[ \begin{array}{c} 0\\\vdots \\ 0 \\ 1\end{array} \right]$ and multiplying $\Sigma_{YY}^{-1}$, it gives us
\begin{align*}
\Sigma_{YY}^{-1} C_{t+1}= \frac{1}{\sigma^4}(\sigma^2 I-A^{-1}) C_{t+1}= \frac{1}{\sigma^4} \left[\begin{matrix} b_{t+1} \\ K_{t+1} \end{matrix} \right].
\end{align*} 
In order to find $b$ and $K$ easily, one has to use Sherman-Morrison formula in the following way, that
\begin{equation}
A_{t+1}^{-1}C_{t+1} = \left( I - \frac{M_{t+1}^{-1}u_{t+1}u_{t+1}^\top }{1+u_{t+1}^\top M_{t+1}^{-1} u_{t+1}} \right)M_{t+1}^{-1}C_{t+1},
\end{equation}
in which
\begin{align*}
M_{t+1}^{-1}C_{t+1}    &=\left[ \begin{array}{cc} A_{t}^{-1} & 0 \\ 0 & \sigma^2 \end{array} \right]C_{t+1}=\sigma^2 C_{t+1},\\
u_{t+1}^\top C_{t+1} & = \left[ \begin{array}{ccccc} 0 & \cdots & 0 &\frac{-\phi}{\tau} & \frac{1}{\tau} \end{array} \right] \left[ \begin{array}{c} 0 \\ \vdots \\ 0\\ 1 \end{array} \right]= \frac{1}{\tau}.
\end{align*}
Then the above equation becomes
\begin{equation}
A_{t+1}^{-1}C_{t+1} = \sigma^2 C_{t+1}-\frac{M_{t+1}^{-1} u_{t+1} \frac{\sigma^2}{\tau}}{1+u^\top M_{t+1}^{-1} u}.
\end{equation}
Moreover,
\begin{align*}
M_{t+1}^{-1} u_{t+1} &=\left[ \begin{array}{cc} A_{t}^{-1} & 0 \\ 0 & \sigma^2 \end{array} \right] \left[ \begin{array}{c}  0 \\ \vdots \\0 \\ -\frac{\phi}{\tau} \\\frac{1}{\tau} \end{array} \right] =
\left[ \begin{array}{cc} A_{t}^{-1} & 0 \\ 0 & \sigma^2 \end{array} \right] \left[ \begin{array}{c} -\frac{\phi}{\tau} C_t \\ \frac{1}{\tau}\end{array} \right] = \left[ \begin{array}{c} -\frac{\phi}{\tau} A_{t}^{-1}C_t \\ \frac{\sigma^2}{\tau}\end{array} \right] ,\\
u^\top M_{t+1}^{-1}  u &=\left[ \begin{array}{ccccc}  0 & \cdots & 0 & -\frac{\phi}{\tau} & \frac{1}{\tau} \end{array} \right] \left[ \begin{array}{c} -\frac{\phi}{\tau} A_{t}^{-1}C_t \\ \frac{\sigma^2}{\tau} \end{array} \right] =  \left[ \begin{array}{cc} -\frac{\phi}{\tau} C_t^\top & \frac{1}{\tau}\end{array} \right]  \left[ \begin{array}{c} -\frac{\phi}{\tau} A_{t}^{-1}C_t \\ \frac{\sigma^2}{\tau}\end{array} \right] =\frac{\phi^2}{\tau^2} C_t^\top A_{t}^{-1}C_t+\frac{\sigma^2}{\tau^2}.
\end{align*}
Thus
\begin{align}
\begin{split}
A_{t+1}^{-1}C_{t+1} &= \left[ \begin{array}{c} -b_{t+1} \\ \sigma^2-K_{t+1}\end{array} \right] = \sigma^2C_{t+1}-\frac{1}{1+\frac{\phi^2}{\tau^2} C_t^\top A_{t}^{-1}C_t+\frac{\sigma^2}{\tau^2}} \left[\begin{array}{c} -\frac{\phi\sigma^2}{\tau^2} A_{t}^{-1}C_t \\\frac{\sigma^4}{\tau^2} \end{array}\right] \\
&= \sigma^2C_{t+1}-\frac{1}{\tau^2+\phi^2C_t^\top A_{t}^{-1}C_t+\sigma^2} \left[\begin{array}{c} -\phi\sigma^2 A_{t}^{-1}C_t \\ \sigma^4 \end{array}\right]
\end{split}
\end{align}
and
\begin{equation*}
\sigma^2-K_{t+1} = \sigma^2 - \frac{\sigma^4}{\tau^2+\phi^2C_t^\top A_{t}^{-1}C_t+\sigma^2} = \sigma^2 - \frac{\sigma^4}{\tau^2+\sigma^2+\phi^2(\sigma^2-K_t)},
\end{equation*}
therefore
\begin{align}
K_{t+1}  &=\frac{\sigma^4}{\tau^2+\sigma^2+\phi^2(\sigma^2-K_t)},
\end{align}
and
\begin{align}b_{t+1} = 
\begin{bmatrix}
\frac{b_t\phi K_{t+1}}{\sigma^2} \\ \frac{K_{t+1}(\sigma^2+\tau^2)-\sigma^4 }{\phi\sigma^2}
\end{bmatrix},
\end{align}

\begin{align*}
\bar{\mu}_{t+1}      &= 0-\sigma^4 K^{-1}b^\top (Z-bK^{-1}b^\top)^{-1} \sigma^{-4} (Z-bK^{-1}b^\top) y_{1:t} \\
					 & =-K^{-1}b^\top y_{1:t} \\
					 & = \frac{\phi}{\sigma^2}K_t\bar{\mu}_t + \phi (1 - \frac{K_t}{\sigma^2})y_t, \\
\bar{\Sigma}_{t+1} &= \sigma^4(K-b^\top Z^{-1}b)^{-1}- \sigma^4K^{-1}b^\top (Z-bK^{-1}b^\top)^{-1} (Z-bK^{-1}b^\top)Z^{-1}b(K-b^\top Z^{-1}b)^{-1}\\
                     & = \sigma^4(I-K^{-1}b^\top Z^{-1}b)(K-b^\top Z^{-1}b)^{-1} \\
                     & = \sigma^4K_{t+1}^{-1},
\end{align*}
where $K_1=\frac{\sigma^4}{\frac{\phi^2}{\tau^2}+\frac{1}{L^2}}$.



\subsubsection{The Estimation Distribution $p(x_{t+1}\mid y_{1:t+1},\theta)$}

The joint distribution for $x_{t+1}$ and $y_{1:t+1}$ is $p(x_{t+1}, y_{1:t+1}  \mid  \theta)\sim N(0,\Gamma)$, where
\begin{equation*}
\Gamma=\begin{bmatrix} C_{t+1}^\top(A-B)^{-1}C_{t+1} & C_{t+1}^\top(A-B)^{-1}\\(A-B)^{-1}C_{t+1} & (I-A^{-1}B)^{-1}B^{-1} \end{bmatrix},
\end{equation*}
where $C_{t+1}^\top$ is a $1\times t+1$ vector $\left[ \begin{array}{cccc} 0 &\cdots & 0 & 1\end{array} \right]_{1 \times t+1}$ retrieving the last column of a matrix. Because of 
\begin{align*}
C_{t+1}^\top A_{t+1}^{-1} = \left[\begin{matrix} - b_{t+1}^\top & \sigma^2- K_{t+1} \end{matrix} \right],
\end{align*}
thus $x_{t+1}\mid y_{1:t+1}\sim N(\bar{\mu}_{t+1}^{(x)},\bar{\sigma}_{t+1}^{(x)2})$, where
\begin{align*}
\bar{\mu}_{t+1}^{(x)}       &= \phi \hat{x}_{t} +  C_{t+1}^\top (A-B)^{-1}B (I-A^{-1}B)y_{1:t+1}\\
                      &= \phi \hat{x}_{t} +  C_{t+1}^\top A^{-1}B y_{1:t+1} \\ &= \phi \hat{x}_{t} +  \frac{1}{\sigma^2}C_{t+1}^\top A^{-1} y_{1:t+1}\\
                      &=0+  \frac{1}{\sigma^2}\left[\begin{matrix} - b_{t+1}^\top & \sigma^2- K_{t+1} \end{matrix} \right]  \left[\begin{matrix} y_{1:t} \\ y_{y+1} \end{matrix} \right] \\
                      &= - \frac{1}{\sigma^2}b_{t+1}^\top y_{1:t}+(1-\frac{K_{t+1}}{\sigma^2})y_{t+1}\\
                      &=\frac{K_{t+1}\bar{\mu}_t}{\sigma^2}+(1-\frac{K_{t+1}}{\sigma^2})y_{t+1} \\
\bar{\sigma}_{t+1}^{(x)2}&=C_{t+1}^\top(A-B)^{-1}C_{t+1}-  C_{t+1}^\top(A-B)^{-1}  B(I-A^{-1}B) (A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top(A-B)^{-1}C_{t+1} -  C_{t+1}^\top A^{-1}B(A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top A^{-1}C_{t+1} \\ &= \sigma^2-K_{t+1}.
\end{align*}



\subsubsection{Approximations of The Parameters Posterior}

Because of the covariance  $\Sigma_{YY} =  (I-A^{-1}B)^{-1}B^{-1}$, therefore the inverse is 
\begin{align*}
\Sigma_{YY}^{-1} &= B(I-A^{-1}B)= BA^{-1}\Sigma_{XX}^{-1}.
\end{align*}
Given the Choleski decomposition $LL^\top = A$, we have
\begin{align*}
\Sigma_{YY}^{-1} &=BL^{-\top}L^{-1}\Sigma_{XX}^{-1}\\
&=(L^{-1}B)^\top(L^{-1}\Sigma_{XX}^{-1})\\
&=\mbox{solve}(L,B)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}).
\end{align*}
More usefully, by given another Choleski decomposition $RR^\top=A-B=\Sigma_{XX}^{-1}$,
\begin{align}\label{sigmayy01}
\begin{split}
Y^\top \Sigma_{YY}^{-1} Y &= \mbox{solve}(L,BY)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
&\triangleq W^\top \mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
\end{split}
\end{align}
\begin{align}\label{sigmayy02}
\begin{split}
\det\Sigma_{YY}^{-1} &= \det B \det L^{-\top}\det L^{-1}\det R\det R^\top\\
&= \det B(\det L^{-1})^2(\det R)^2.
\end{split}
\end{align}

From the objective function, the posterior distribution of $\theta$ is 
\begin{align*}
p(\theta \mid Y) &\propto p(Y\mid\theta)p(\theta) \propto e^{-\frac{1}{2} Y \Sigma_{YY}^{-1} Y } \sqrt{\det \Sigma_{YY}^{-1}} p(\theta).
\end{align*}
Then by taking natural logarithm on the posterior of $\theta$ and using the useful solutions in equations (\ref{sigmayy01}) and (\ref{sigmayy02}), we will have
\begin{align}\label{logL}
\ln L(\theta) &= -\frac{1}{2}Y^\top\Sigma_{YY}^{-1}Y+\frac{1}{2}\sum\ln\mbox{tr}(B)-\sum\ln\mbox{tr}(L)+\sum\ln\mbox{tr}(R) + \ln p(\theta).
\end{align}



%
%\subsection{Particle Learning Testing}
%A state space model is
%\begin{align*}
%y_t&=F_tx_t+\epsilon_t,\\
%x_t&=\phi_tx_{t-1}+w_t,\\
%x_0&\sim N(m_0,c_0),
%\end{align*}
%where $\epsilon_t\sim N(0,\sigma)$ and $w_t\sim N(0,W)$. $x_t$ are hidden status and $y_t$ are observations. Assuming that $V=W=1$, $F_t=1$, the initial value $x_0=0$. $\phi_t$ is the  parameter to be estimated. 
%
%$s_{t}^x$ is the sufficient statistics of the states and $s_{t}$ is the sufficient statistics of the parameter $\theta$ at time $t$. The Particle Learning algorithm runs as follows:
%
%\begin{itemize}
%\item Step 1. Resample $\{\tilde{z}_t^{(i)}\}_{i=1}^N=(\tilde{x}_t^{(i)},\tilde{s}_t^{(i)},\tilde{\theta}^{(i)})$ from $z_t^{(i)}=(x_t,s_t,\theta)^{(i)}$ with weight $w\propto p(y_{t+1}|s_{t}^x,\theta)$. It is found that \begin{equation*}
%p(y_{t+1}|s_{t}^x,\theta) \propto \exp \left(-\frac{1}{2}\frac{(y_{t+1}-\theta x_{t})^2}{\sigma^2} \right).
%\end{equation*}
%
%\item Step 2. Propagate $\tilde{x}_t^{(i)}$ to $x_{t+1}^{(i)}$ via $p(x_{t+1}|\tilde{z}_t^{(i)},y^{t+1})$. 
%\begin{align*}
%p(x_{t+1}|\tilde{z}_t^{(i)},y^{t+1}) &= p(x_{t+1}|s_t^x,\phi,y^{t+1}) \propto p(x_{t+1},y^{t+1}|s_t^x,\phi)\\
%&\propto p(x_{t+1}|s_t^x,\phi)p(y_{t+1}|x_{t+1},s_t^x,\phi) \\
%&=N(x_{t+1}|\phi x_t,1)N(y_{t+1}|x_{t+1},1)\\
%&\sim N(\frac{1}{2}(y_{t+1}+\phi x_t),\frac{1}{\sqrt{2}})
%\end{align*}
%
%\item Step 3. Update sufficient statistics $s_t$.
%\begin{align*}
%s_{t+1,1} &= x_{t+1} \\
%s_{t+1,2} &= x_tx_{t+1}+s_{t,2} = x_ts_{t,1}+s_{t,2} \\
%s_{t+1,3} &= x_{t}^2 + s_{t,3} = s_{t,1}^2 + s_{t,3} .
%\end{align*}
%
%\item Step 4. Sample $\theta$ from $p(\theta | s_t)$.
%\begin{align*}
%p(\theta | x^{t+1},y^{t+1}) & \propto p(x^{t+1},y^{t+1}|\theta)p(\theta)\propto p(x^{t+1}|\theta)p(\theta)\\
%&\sim N\left( \phi | \frac{s_{t+1,2}}{s_{t+1,3}},\frac{1}{s_{t+1,3}} \right).
%\end{align*}
%
%\item Step 5. Update $s_{t+1}^x = x_{t+1}$.
%\end{itemize}
%
%Particle learning is used to estimate one parameter $\phi$.
%\begin{figure}[h]
%\centering
%\includegraphics[width=14cm,height=9cm]{PFforphi}
%\end{figure}


\subsection{High Dimension Parameters Space of OU-Process}

The Brownian motion is used to construct the Ornstein Uhlenbeck (OU) process, which has become a popular tool for modeling interest rates and vehicle moving. The derivative of the Brownian motion $x_t$ does not exist at any point in time. Thus, if $x_t$ represents the position of a particle, we might be interested in
obtaining its velocity, which is the derivative of the motion. The OU process is an alternative model to the Brownian motion that overcomes the preceding problem.
It does this by considering the velocity $u_t$ of a Brownian motion at time $t$. Over a small time interval, two factors affect the change in velocity: the frictional
resistance of the surrounding medium whose effect is proportional to $u_t$ and the random impact of neighboring particles whose effect can be represented
by a standard Wiener process. Thus, because mass times velocity equals force, we have that
\begin{equation*}
mdu_t = -\omega u_tdt+dW_t,
\end{equation*}
where $\omega>0$ is called the friction coefficient and $m>0$ is the mass. If we define $\gamma = \omega /m$ and $\lambda = 1/m$, we obtain the OU process with the following differential
equation:
\begin{equation}
du_t= -\gamma u_tdt+\lambda dW_t.
\end{equation}

The OU process is used to describe the velocity of a particle in a fluid and is encountered in statistical mechanics. It is the model of choice for random movement
toward a concentration point. It is sometimes called a continuous-time Gauss Markov process, where a Gauss Markov process is a stochastic process
that satisfies the requirements for both a Gaussian process and a Markov process. Because a Wiener process is both a Gaussian process and a Markov process, in addition to being a stationary independent increment process, it can be considered a Gauss-Markov process with independent increments \cite{kijima1997markov}.

An OU-process model combing states and velocity is in the form of
\begin{align}
\begin{cases}
du_t = -\gamma u_t dt+ \lambda dW_t,\\
dx_t = u_t dt+\xi dW_t'.
\end{cases}
\end{align}
The solution can be found by integrating $dt$ out, that gives us 
\begin{align}
\begin{cases}
u_t &=u_{t-1}e^{-\gamma t} +\int_{0}^{t} \lambda e^{-\gamma (t-s)}dW_s,\\
x_t &=x_{t-1} +\frac{u_{t-1}}{\gamma}(1- e^{-\gamma t}) + \int_{0}^{t} \frac{\lambda}{\gamma}e^{\gamma  s} \left(1-e^{-\gamma t}\right)dW_s + \int_{0}^{t}\xi dW_s'.
\end{cases}
\end{align}
Therefore,  the joint distribution is 
\begin{align}
\begin{bmatrix} x_t \\ u_t \end{bmatrix} &\sim N\left(
\begin{bmatrix}\mu_t^{(x)} \\ \mu_t^{(u)}  \end{bmatrix} , 
\begin{bmatrix}
\sigma_t^{(x)2} & \rho_t\sigma_t^{(x)} \sigma_t^{(u)} \\
\rho_t\sigma_t^{(x)} \sigma_t^{(u)} & \sigma_t^{(u)2}
\end{bmatrix} \right),
\end{align}
where $\mu_t^{(x)}$ and $\mu_t^{(u)} $ are from the forward map process 
\begin{align}
\begin{bmatrix}\mu_t^{(x)} \\ \mu_t^{(u)}  \end{bmatrix}  = 
\begin{bmatrix}
1 & \frac{1-e^{-\gamma \Delta_t}}{\gamma} \\ 0 &  e^{-\gamma \Delta_t}
\end{bmatrix}  \begin{bmatrix} x_{t-1}^{(x)} \\ u_{t-1}  \end{bmatrix} \triangleq \Phi \begin{bmatrix} x_{t-1}^{(x)} \\ u_{t-1}  \end{bmatrix},
\end{align}
and 
\begin{align*}
\begin{cases}
\sigma_t^{(x)2} &=\frac{\lambda^2 \left(e^{2 \gamma\Delta_t}-1\right) \left(1 -e^{-\gamma\Delta_t}\right)^2}{2 \gamma ^3 } + \xi^2\Delta_t\\
\sigma_t^{(u)2} &= \frac{\lambda ^2 \left(1- e^{-2 \gamma\Delta_t}\right)}{2 \gamma } \\
\rho_t\sigma_t^{(x)}\sigma_t^{(u)} & =\frac{\lambda ^2 \left(e^{\gamma\Delta_t} -1\right) \left(1-e^{-2\gamma\Delta_t}\right)}{2 \gamma ^2}
\end{cases}
\end{align*}
In the above equations, $\Delta_t = T_t-T_{t-1}, \Delta_1=0$, $x_0\sim N(0,L_x^2), u_0\sim N(0,L_u^2)$, $\rho_t^2 = 1-\frac{\xi^2 \Delta_t}{\sigma_t^{(x)^2}}$. To be useful, $1-\rho_t^2 =\frac{\sigma_t^{(x)^2}}{\xi^2 \Delta_t}$. 

Moreover, the independent observation processes are
\begin{align*}\begin{cases} y_t=x_t+\epsilon_t,\\ v_t=u_t+\epsilon'_t, \end{cases} \end{align*}
where $\epsilon_t\sim N(0,\sigma),\epsilon'_t\sim N(0,\tau)$ are normally distributed independent errors. Thus, the joint distribution of observations is 
\begin{align}\label{obmodel}
\begin{bmatrix} y_t \\ v_t \end{bmatrix} &\sim N\left(
\begin{bmatrix}x_t \\ u_t \end{bmatrix} , 
\begin{bmatrix}
\sigma^2 & 0\\
0 & \tau^2
\end{bmatrix} \right).
\end{align}
Consequently, the parameter $\theta$ of an entire Ornstein-Uhlenbeck process is a set of five parameters from both hidden status and observation process, which is represented as $\theta = \{\gamma,\xi^2,\lambda,\sigma^2,\tau^2 \}$. 


Starting from the joint distribution of $x_{0:t},u_{0:t}$ and $y_{1:t},v_{1:t}$ by given $\theta$, it can be found that
\begin{equation}\label{jointmatrix}
\begin{bmatrix} \begin{matrix} \tilde{X}\\ \tilde{Y}  \end{matrix} \biggr\rvert \theta \end{bmatrix}
\sim N\left(0, \tilde{\Sigma} \right),
\end{equation}
where $\tilde{X}$ represents for the hidden statues $\{x,u\}$, $\tilde{Y}$ represents for observed $\{y,v\}$, $\theta$ is the set of five parameters.  The inverse of the covariance matrix $\tilde{\Sigma}^{-1}$ is the procedure matrix in the form of
\begin{align*} \tilde{\Sigma}^{-1}=
\begin{bmatrix}
Q_{xx} & Q_{xu} & -\frac{1}{\sigma^2}I & 0\\
Q_{ux} & Q_{uu} & 0 &-\frac{1}{\tau^2}I \\
-\frac{1}{\sigma^2}I & 0 & \frac{1}{\sigma^2}I  & 0\\
 0  &  -\frac{1}{\tau^2}I  & 0 & \frac{1}{\tau^2}I 
\end{bmatrix}.
\end{align*}
To make the covariance matrix a more beautiful form and convenient computing, $\tilde{X}$, $\tilde{Y}$ and $\tilde{\Sigma}$ can be rearranged in a time series order, that makes $X = \{x_1,u_1,x_2,u_2,\cdots, x_t, u_t \}$, $Y = \{y_1,v_1,y_2,v_2,\cdots, y_t, v_t \}$ and the new procedure matrix $\Sigma^{-1}$ looks like 
\begin{align*} \Sigma^{-1}=
\begin{bmatrix}
\sigma_{11}^{(x)2}+\frac{1}{\sigma^2} & \sigma_{11}^{(xu)2} & \cdots & \sigma_{1t}^{(x)2} & \sigma_{1t}^{(xu)2}  &  -\frac{1}{\sigma^2} & 0 & \cdots & 0 & 0\\
\sigma_{11}^{(ux)2}   & \sigma_{11}^{(u)2} +\frac{1}{\tau^2} & \cdots & \sigma_{1t}^{(ux)2} & \sigma_{1t}^{(x)2}  &  0 & -\frac{1}{\tau^2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots &\ddots & \vdots & \vdots \\
\sigma_{t1}^{(x)2}   & \sigma_{t1}^{(xu)2} & \cdots & \sigma_{tt}^{(x)2} +\frac{1}{\sigma^2}  & \sigma_{tt}^{(xu)2}  &  0 & 0 & \cdots & -\frac{1}{\sigma^2} & 0 \\
\sigma_{t1}^{(ux)2}   & \sigma_{t1}^{(u)2} & \cdots & \sigma_{tt}^{(ux)2} & \sigma_{tt}^{(u)2} +\frac{1}{\tau^2}  &  0 & 0 & \cdots & 0 &-\frac{1}{\tau^2} \\
- \frac{1}{\sigma^2} & 0 & \cdots & 0 & 0 &  \frac{1}{\sigma^2} & 0 & \cdots & 0 & 0\\
0  & -\frac{1}{\tau^2}& \cdots & 0 & 0 &  0 &  \frac{1}{\tau^2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots &\ddots & \vdots & \vdots \\
0 & 0& \cdots & -\frac{1}{\sigma^2}  &0&  0 & 0 & \cdots & \frac{1}{\sigma^2} & 0 \\
0 & 0 & \cdots & 0 & -\frac{1}{\tau^2}   &  0 & 0 & \cdots & 0 & \frac{1}{\tau^2}
\end{bmatrix} \triangleq \begin{bmatrix}
A_t& -B_t \\ -B_t^\top & B_t
\end{bmatrix},
\end{align*}
where $B_t$ is a $2t\times 2t$ diagonal matrix of observation errors at time $t$ in the form of $\begin{bmatrix}
\frac{1}{\sigma^2}& \cdot & \cdot &  \cdot  &  \cdot \\  \cdot & \frac{1}{\tau^2} & \cdot &  \cdot  &  \cdot  \\ 
\vdots & \vdots & \ddots & \vdots & \vdots \\
 \cdot  &  \cdot  & \cdot  & \frac{1}{\sigma^2}&  \cdot \\  \cdot  &  \cdot & \cdot  &  \cdot  & \frac{1}{\tau^2}
\end{bmatrix}$. 
In fact, the matrix $A_t$ is a $2t \times 2t$ bandwidth six sparse matrix at time $t$ in the process. Temporally, we are using $A$ and $B$ to represent the matrices  $A_t$ and $B_t$ here. Then we may find the covariance matrix by calculating the inverse of the procedure matrix as 
\begin{align*}
\Sigma &= \begin{bmatrix}
(A-B^\top B^{-1}B) ^{-1} & -(A-B^\top B^{-1}B)^{-1}B^\top B^{-1}\\
- B^{-1}B(A-B^\top B^{-1}B)^{-1} & (B-B^\top A^{-1}B) ^{-1}
\end{bmatrix} \\
&= \begin{bmatrix}
(A-B) ^{-1} & (A-B)^{-1}\\
(A-B)^{-1} & (I- A^{-1}B) ^{-1}B^{-1}
\end{bmatrix} \\
&\triangleq \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY} \\
\Sigma_{YX}  &\Sigma_{YY} 
\end{bmatrix}.
\end{align*}



\subsubsection{The Forecast Distribution $p(Y_{t+1}| Y_{1:t},\theta)$}

We are now using the capital letter $Y$ to represent the joint $\{y,v\}$ and $Y_{1:t} = \{y_1,v_1,y_2,v_2,\cdots, y_t, v_t \}$, $Y_{t+1} = \{y_{t+1}, v_{t+1}\}$. It is known that 
\begin{align*}
p(Y_{1:t},\theta) &\sim N\left( 0,\Sigma_{YY}^{(t)} \right)\\
p(Y_{t+1},Y_{1:t},\theta) &\sim N\left( 0,\Sigma_{YY}^{(t+1)} \right)\\
p(Y_{t+1}\mid Y_{1:t},\theta) &\sim N\left( \bar{\mu}_{t+1},\bar{\Sigma}_{t+1} \right)
\end{align*}
where the covariance matrix of the joint distribution is $\Sigma_{YY}^{(t+1)} = (I_{t+1}-A_{t+1}^{-1}B_{t+1})^{-1}B_{t+1}^{-1}$. Then, by taking its inverse, we will get
\begin{align*}
\Sigma_{YY}^{(t+1) (-1)} = B_{t+1}(I_{t+1}-A_{t+1}^{-1}B_{t+1}).
\end{align*}
To be clear, the matrix $B_t$ is short for the matrix $B_t(\sigma^2,\tau^2)$, which is $2t\times 2t$ diagonal matrix with elements $\frac{1}{\sigma^2},\frac{1}{\tau^2}$ repeating for $t$ times on its diagonal. For instance, the very simple $B_1(\sigma^2,\tau^2) = 
\begin{bmatrix}
\frac{1}{\sigma^2} & 0  \\
0 & \frac{1}{\tau^2}
\end{bmatrix}_{2\times 2}$ is a $2\times 2$ matrix. 

Because of $A$ is symmetric and invertible, $B$ is the diagonal matrix defined as above, then they have the following property 
\begin{align*}
& AB=A^\top B^\top = (BA)^\top, \\
& A^{-1}B = A^{-\top}B^\top = (BA^{-1})^\top. 
\end{align*}
Followed up the form of $\Sigma_{YY}^{(t+1) (-1)}$, we can find out that 
\begin{align*}
\Sigma_{YY}^{(t+1) (-1)} &= B_{t+1}(I_{t+1}-A_{t+1}^{-1}B_{t+1}) \\
&= B_{t+1}(B_{t+1}^{-1}-A_{t+1}^{-1})B_{t+1} \\
&\triangleq \begin{bmatrix} 
B_t & 0 \\ 0 & B_1 \end{bmatrix}
\begin{bmatrix} 
Z_{t+1} & b_{t+1} \\
b_{t+1}^\top & K_{t+1}
\end{bmatrix} \begin{bmatrix} 
B_t & 0 \\ 0 & B_1\end{bmatrix}
\end{align*}
where $Z_{t+1}$ is a $2t \times 2t$ matrix, $ b_{t+1} $ is a $2t \times 2$ matrix and $K_{t+1}$ is a $2 \times 2$ matrix. Thus by taking its inverse again, we will get 
\begin{align*} \Sigma_{YY}^{(t+1)}= \left[ \begin{matrix}
B_t^{-1} (Z_{t+1}-b_{t+1}K_{t+1}^{-1}b_{t+1}^\top)^{-1}B_t^{-1}  & - B_t^{-1}  Z_{t+1}^{-1}b_{t+1}(K_{t+1}-b_{t+1}^\top Z_{t+1}^{-1}b_{t+1})^{-1}B_1^{-1} \\
-B_1^{-1}  K_{t+1}^{-1}b_{t+1}^\top (Z_{t+1}-b_{t+1}K_{t+1}^{-1}b_{t+1}^\top)^{-1}B_t^{-1}  & B_1^{-1}  (K_{t+1}-b_{t+1}^\top Z_{t+1}^{-1}b_{t+1})^{-1}B_1^{-1} 
\end{matrix}\right].
\end{align*}

It is easy to find the relationship between $A_{t+1}$ and  $A_{t}$ in the Sherman-Morrison-Woodbury form is 
\begin{align*} A_{t+1} = 
\begin{bmatrix}
A_t & \cdot & \cdot  \\ \cdot &\frac{1}{\sigma^2} &\cdot  \\ \cdot  & \cdot  & \frac{1}{\tau^2} 
\end{bmatrix} + U_{t+1}U_{t+1}^\top \triangleq M_{t+1}  + U_{t+1}U_{t+1}^\top,
\end{align*}
where $M_{t+1} = \begin{bmatrix}
A_t & \cdot & \cdot  \\ \cdot &\frac{1}{\sigma^2} &\cdot  \\ \cdot  & \cdot  & \frac{1}{\tau^2}
\end{bmatrix}  = \begin{bmatrix}
A_t & 0 \\ 0 & B_1
\end{bmatrix}$ 
and its inverse is $M_{t+1}^{-1} =\begin{bmatrix}
A_t^{-1} & 0 \\ 0 & B_1^{-1}
\end{bmatrix}$. Additionallly, $U$ is a $2t+2 \times 2$ matrix in the following form 
\begin{align*}
U_{t+1} = \frac{1}{\sqrt{ 1-\rho_{t+1}^2} } \begin{bmatrix}
\mathbf{0}_{2t-2} & \mathbf{0}_{2t-2}  \\ \frac{1}{\sigma_{t+1}^{(x)}}& 0 \\
\frac{1-e^{-\gamma \Delta_{t+1}}}{\gamma \sigma_{t+1}^{(x)}}-\frac{\rho_{t+1} e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}} & \frac{\sqrt{1-\rho_{t+1}^2}e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}} \\
-\frac{1}{\sigma_{t+1}^{(x)}} & 0 \\
\frac{\rho_{t+1}}{\sigma_{t+1}^{(u)}} & -\frac{\sqrt{1-\rho_{t+1}^2}}{\sigma_{t+1}^{(u)}}
\end{bmatrix} \triangleq  \begin{bmatrix}
C_t S_{t+1} \\ D_{t+1}
\end{bmatrix},
\end{align*}
denoted by $S_{t+1} = \frac{1}{\sqrt{ 1-\rho_{t+1}^2} } \begin{bmatrix}
\frac{1}{\sigma_{t+1}^{(x)}}& 0 \\
\frac{1-e^{-\gamma \Delta_{t+1}}}{\gamma \sigma_{t+1}^{(x)}}-\frac{\rho_{t+1} e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}} & \frac{\sqrt{1-\rho_{t+1}^2}e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}}
\end{bmatrix}$, $D_{t+1} =  \frac{1}{\sqrt{ 1-\rho_{t+1}^2} }\begin{bmatrix}
-\frac{1}{\sigma_{t+1}^{(x)}} & 0 \\
\frac{\rho_{t+1}}{\sigma_{t+1}^{(u)}} & -\frac{\sqrt{1-\rho_{t+1}^2}}{\sigma_{t+1}^{(u)}}
\end{bmatrix}$ and $C_{t+1} = \begin{bmatrix} 0 & 0 \\ \vdots & \vdots \\ 0 & 0 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix}
\mathbf{0}_t \\ I_{2} \end{bmatrix}$. 


By post-multiplying $\Sigma_{YY}^{(t+1)(-1)}$ with $C_{t+1}$, it gives us
\begin{align*}
\Sigma_{YY}^{(t+1)(-1)} C_{t+1} &=  B_{t+1} (I_{t+1} -A_{t+1} ^{-1}B_{t+1} ) C_{t+1}  \\ 
&= \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix} \left( \begin{bmatrix} B_t^{-1} & 0 \\ 0 & B_1^{-1} \end{bmatrix}  -A_{t+1}^{-1} \right) \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}   C_{t+1}\\
&= \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}\begin{bmatrix} Z_{t+1} & b_{t+1} \\ b_{t+1}^\top  & K_{t+1} \end{bmatrix}  \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}   C_{t+1}\\
& = \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}\begin{bmatrix} b_{t+1}B_1\\ K_{t+1}B_1 \end{bmatrix}.
\end{align*}
and the property of $A_{t+1}^{-1}$ is 
\begin{equation*}
A_{t+1}^{-1}C_{t+1} = \begin{bmatrix}
-b_{t+1} \\ B_1^{-1} - K_{t+1}
\end{bmatrix}.
\end{equation*}
Moreover, by pre-multiplying $C_{t+1}^\top$ on the left side of the above equation, we will have 
\begin{align}\label{OUKtp1}
C_{t+1}^\top A_{t+1}^{-1}C_{t+1}  &= B_1^{-1} -K_{t+1},\\
K_{t+1} &= B_1^{-1} - C_{t+1}^\top A_{t+1}^{-1}C_{t+1}.
\end{align}


We may use Sherman-Morrison-Woodbury formula to find the inverse of $A_{t+1}$ in a recursive way, which is 
\begin{equation}
A_{t+1}^{-1} = (M_{t+1}+U_{t+1}U_{t+1}^\top)^{-1}= M_{t+1}^{-1}-M_{t+1}^{-1}U_{t+1}(I+U_{t+1}^\top M_{t+1}^{-1}U_{t+1})^{-1}U_{t+1}^\top M_{t+1}^{-1}.
\end{equation}
Consequently, it is easy to find that $M_{t+1}^{-1}C_{t+1} =\begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix} $ and 
\begin{align*}
A_{t+1}^{-1}C_{t+1} &= \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} & 0 \\ 0 & B_1^{-1} \end{bmatrix} \begin{bmatrix} C_tS_{t+1} \\ D \end{bmatrix}
(I+U_{t+1}^\top M_{t+1}^{-1}U_{t+1})^{-1}  \begin{bmatrix}
S_{t+1}^\top C_t^\top & D_{t+1}^\top \end{bmatrix} \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  \\ 
% ===
& = \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} C_tS_{t+1} \\B_1^{-1}D_{t+1} \end{bmatrix} 
(I+U_{t+1}^\top M_{t+1}^{-1}U_{t+1})^{-1}  D_{t+1}^\top B_1^{-1} \\ 
% ===
& = \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} C_tS_{t+1} \\B_1^{-1}D_{t+1} \end{bmatrix} 
(I+ S_{t+1}^\top C_t^\top A_t^{-1} C_t S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1} \\ 
% ===
& = \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} C_tS_{t+1} \\B_1^{-1}D_{t+1} \end{bmatrix} 
(I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1}.
\end{align*}
Thus, by using the equation (\ref{OUKtp1}), we will get 
\begin{equation}
K_{t+1} =B_1^{-1}D_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1},
\end{equation}
and
\begin{align*}
b_{t+1} &= A_t^{-1}C_t S_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1} D_{t+1}^\top B_1^{-1} \\
&= \begin{bmatrix}
-b_t \\ B_1^{-1}-K_t 
\end{bmatrix}  S_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1} D_{t+1}^\top B_1^{-1}.
\end{align*}
To achieve the recursive updating formula, firstly we need to find the form of $b_{t+1}^\top B_t^2 Y_{1:t}$. In fact, it is 
\begin{align*}
b_{t+1}^\top B_t Y_{1:t} &= B_1^{-\top}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
\begin{bmatrix}
-b_t^\top  & B_1^{-1}-K_t 
\end{bmatrix} B_t \begin{bmatrix}
Y_{1:t-1} \\ Y_t
\end{bmatrix}\\
&= B_1^{-\top}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
\left(  -b_t^\top  B_{t-1}  Y_{1:t-1} + (B_1^{-1}-K_t )  B_1  Y_t      \right) \\ 
&= B_1^{-\top}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
\left(  K_t B_1\bar{\mu}_t+ (I-K_t B_1)  Y_t      \right), \\
\end{align*}
By using equation (\ref{recursiveKp1}) and simplifying the above equation, one can achieve a recursive updating form of the mean, which is 
\begin{align*}
%\bar{\mu}_{t+1} = -B_1K_{t+1}^{-1} B_1^{-1}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
%\left(  K_t B_1 \bar{\mu}_t+ (I-K_tB_1)Y_t      \right).
\bar{\mu}_{t+1} &= -B_1K_{t+1}^{-1}b_{t+1}^\top B_t Y_{1:t} \\
&= -D_{t+1}^{-\top}S_{t+1}^\top (K_tB_1\bar{\mu}_t + (I-K_tB_1)Y_t) \\ 
&= -D_{t+1}^{-\top}S_{t+1}^\top ( Y_t +   K_tB_1(\bar{\mu}_t-Y_t)),
\end{align*}
where by simplifying $D^{-\top}S^\top$, one may find 
\begin{align*}
D_{t+1}^{-\top}S_{t+1}^\top = \begin{bmatrix}
-1 & -\frac{1-e^{-\gamma \Delta_{t+1}}}{\gamma} \\ 0 & - e^{-\gamma \Delta_{t+1}}
\end{bmatrix} = -\Phi_{t+1},
\end{align*}
which is the negative of forward process. Then the final form of recursive updating formula are 
\begin{align}
\begin{cases}
\bar{\mu}_{t+1}&=\Phi_{t+1} K_tB_1\bar{\mu}_t + \Phi_{t+1} (I-K_tB_1)Y_t\\
\bar{\Sigma}_{t+1}&=\left( B_1K_{t+1}B_1  \right)^{-1}
\end{cases}.
\end{align}
The matrix $K_{t+1}$ is updated via 
\begin{equation}\label{recursiveKp1}
K_{t+1} =B_1^{-1}D_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1},
\end{equation}
or updating its inverse in the following form makes the computation faster, that is 
\begin{align*}
\begin{cases}
K_{t+1}^{-1} &= B_1D_{t+1}^{-\top}D_{t+1}^{-1}B_1 + B_1\Phi_{t+1} (B_1^{-1} - K_t) \Phi_{t+1}^\top B_1+ B_1,\\
\bar{\Sigma}_{t+1}&= D_{t+1}^{-\top}D_{t+1}^{-1}+ \Phi_{t+1} (B_1^{-1} - K_t) \Phi_{t+1}^\top + B_1^{-1}
\end{cases}
\end{align*}
and $K_1 =B_1^{-1} - A_1^{-1} = \begin{bmatrix}
\frac{\sigma^4}{\sigma^2 +L_x^2} & 0 \\ 0 &\frac{\tau^4}{\tau^2 +L_u^2}
\end{bmatrix} $.

\subsubsection{The Estimation Distribution $p(X_{t+1}| Y_{1:t+1},\theta)$}

%
%Because of the joint distribution (\ref{jointmatrix}), one can find the best estimation with a given $\theta$ by
%\begin{align*}
%X \mid Y,\theta &\sim N \left( A^{-1}BY, A^{-1} \right) \\
%&\sim N(L^{-\top}L^{-1}BY,L^{-\top}L^{-1})\\
%&\sim N(L^{-\top}W,L^{-\top}L^{-1}),
%\end{align*}
%thus
%\begin{align*}
%\hat{X} = L^{-\top}(W+Z),
%\end{align*}
%where $Z \sim N(0, I(\sigma,\tau))$.
%
%
%For $x_{t+1}$, the joint distribution with $Y$ updated to stage $t+1$ is 
%\begin{align*}
%x_{t+1}, Y\mid \theta \sim N\left( 0, \begin{bmatrix}
%C_{t+1}^\top(A-B) ^{-1}C_{t+1} & C_{t+1}^\top (A-B)^{-1}\\
%(A-B)^{-1}C_{t+1} & (I- A^{-1}B) ^{-1}B^{-1}
%\end{bmatrix} \right),
%\end{align*}
%where $C_{t+1}^\top = \begin{bmatrix}
%0 & \cdots & 0 & 1 & 0 \\
%0 & \cdots & 0 & 0 & 1
%\end{bmatrix}$ is a $2 \times 2(t+1)$ matrix. Thus
%\begin{align*}
%x_{t+1}\mid Y,\theta \sim N(\bar{\mu}_{t+1}^{(x)},\bar{\Sigma}_{t+1}^{(x)}),
%\end{align*}
%where
%\begin{align*}
%\bar{\mu}_{t+1}^{(x)} & = C_{t+1}^\top A^{-1}BY =C_{t+1}^\top L^{-\top}W,\\
%\bar{\Sigma}_{t+1}^{(x)} & =C_{t+1}^\top A^{-1}C_{t+1} =U_{t+1}^\top U_{t+1},
%\end{align*}
%and $U_{t+1} = L^{-1} C_{t+1} = \mbox{solve}(L,C_{t+1})$.

The filtering distribution of the state given parameters is $p(X_t\mid Y_{1:t}, \theta )$. To find its form, one can use the joint distribution of $X_{t+1}$ and $Y_{1:t+1}$, which is $p(X_{t+1}, Y_{1:t+1}  \mid  \theta)\sim N(0,\Gamma)$, where
\begin{equation*}
\Gamma=\begin{bmatrix} C_{t+1}^\top(A-B)^{-1}C_{t+1} & C_{t+1}^\top(A-B)^{-1}\\(A-B)^{-1}C_{t+1} & (I-A^{-1}B)^{-1}B^{-1} \end{bmatrix}.
\end{equation*}
Because of 
\begin{align*}
C_{t+1}^\top A_{t+1}^{-1} = \left[\begin{matrix} - b_{t+1}^\top & B_1^{-1}- K_{t+1} \end{matrix} \right],
\end{align*}
then $X_{t+1}\mid Y_{1:t+1},\theta \sim N(\bar{\mu}_{t+1}^{(X)},\bar{\sigma}_{t+1}^{(X)2})$, where
\begin{align*}
\bar{\mu}_{t+1}^{(X)}       &= \Phi \hat{x}_{t} +  C_{t+1}^\top (A-B)^{-1}B (I-A^{-1}B)Y_{1:t+1}\\
                      &= \Phi \hat{x}_{t} +  C_{t+1}^\top A^{-1}B Y_{1:t+1} \\ 
                      &=0+ \begin{bmatrix} - b_{t+1}^\top & B_1^{-1}-K_{t+1} \end{bmatrix}\begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix} \begin{bmatrix} Y_{1:t} \\ Y_{y+1} \end{bmatrix} \\
                      &=   -b^\top B_t Y_{1:t} + (I - B_1K_{t+1})Y_{t+1} \\
                      & =  K_{t+1}B_1\bar{\mu}_{t+1 } + (I - B_1K_{t+1})Y_{t+1}  \\
\bar{\sigma}_{t+1}^{(X)2}&=C_{t+1}^\top(A-B)^{-1}C_{t+1}-  C_{t+1}^\top(A-B)^{-1}  B(I-A^{-1}B) (A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top(A-B)^{-1}C_{t+1} -  C_{t+1}^\top A^{-1}B(A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top A^{-1}C_{t+1} \\ &=B_1^{-1}-K_{t+1}.
\end{align*}

\subsubsection{Approximations of The Parameters Posterior}

From the joint distribution of $X$ and $Y$, we can find that  
\begin{align*}
\Sigma_{YY}^{-1} &= B(I-A^{-1}B)= BA^{-1}\Sigma_{XX}^{-1}.
\end{align*}
Given the Choleski decomposition $LL^\top = A$, we have
\begin{align*}
\Sigma_{YY}^{-1} &=BL^{-\top}L^{-1}\Sigma_{XX}^{-1}\\
&=(L^{-1}B)^\top(L^{-1}\Sigma_{XX}^{-1})\\
&=\mbox{solve}(L,B)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}).
\end{align*}
More usefully, by given another Choleski decomposition $RR^\top=A-B=\Sigma_{XX}^{-1}$,
\begin{align}\label{sigmayy01}
\begin{split}
Y^\top \Sigma_{YY}^{-1} Y &= \mbox{solve}(L,BY)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
&\triangleq W^\top \mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
\end{split}
\end{align}
\begin{align}\label{sigmayy02}
\begin{split}
\det\Sigma_{YY}^{-1} &= \det B \det L^{-\top}\det L^{-1}\det R\det R^\top\\
&= \det B(\det L^{-1})^2(\det R)^2.
\end{split}
\end{align}

From the objective function, the second term in the integral is
\begin{align*}
p(\theta \mid Y) &\propto p(Y\mid\theta)p(\theta) \propto e^{-\frac{1}{2} Y \Sigma_{YY}^{-1} Y } \sqrt{\det \Sigma_{YY}^{-1}} P(\theta).
\end{align*}
Then by taking natural logarithm on the posterior of $\theta$ and using the useful solutions in equations (\ref{sigmayy01}) and (\ref{sigmayy02}), we will have
\begin{align}\label{logL}
\ln L(\theta) &= -\frac{1}{2}Y^\top\Sigma_{YY}^{-1}Y+\frac{1}{2}\sum\ln\mbox{tr}(B)-\sum\ln\mbox{tr}(L)+\sum\ln\mbox{tr}(R).
\end{align}





\section{Prior Distribution for Variance Parameters}

The well known Hierarchical Linear Model, where the parameters vary at more then one level, was firstly introduced by Lindley and Smith in 1972 and 1973 \cite{lindley1972bayes} \cite{smith1973general}. An extension of these models is   non-linear Hierarchical Model. Hierarchical Model can be used on data with many levels, although 2-level models are the most common ones. The state space model in equations (\ref{statemodel1}) and (\ref{statemodel2}) is one of Hierarchical Linear Model if $G_t$ and $F_t$ are linear and non-linear model if $G_t$ and $F_t$ are non-linear processes. Researchers have made a few discussions and works on these both linear and non-linear models. In this section, we only discuss on the prior for variance parameters in these models. 

Jonathan and Thomas in \cite{stroud2007sequential} have discussed a model, which is slightly different with a Gaussian state-space model in equations (\ref{statemodel1}) and (\ref{statemodel2}) from section one. The two errors $\omega_t$ and $\epsilon_t$ are assumed normally distributed as
\begin{align*}
\omega_t &\sim N(0,\alpha Q),\\
\epsilon_t &\sim N(0,\alpha R),
\end{align*}
where the two matrices $R$ and $Q$ are known and $\alpha$ is an unknown scale factor to be estimated. (Note that a perfect model is obtained by setting $Q= 0$.) Therefore, the density of Gaussian state-space model is
\begin{align*}
p(y_t\mid x_t,\alpha) &= N(F(x_t),\alpha R),\\
p(x_t\mid x_{t-1},\alpha) &= N(G(x_{t-1}),\alpha Q).
\end{align*}
The parameter $\alpha$ is assumed \textit{Inverse Gamma} distribution. 

Various non-informative and weakly-informative prior distributions have been suggested for
scale parameters in hierarchical models. Andrew Gelman gave a discussion on prior distributions for variance parameters in hierarchical models in 2006 \cite{gelman2006prior}. General considerations include using invariance \cite{jeffries1961theory}, maximum entropy \cite{jaynes1983papers} and agreement with classical estimators \cite{box2011bayesian}. 


\subsection{Priors Discussion}

$http://andrewgelman.com/2007/07/18/informative_and/$

Informative and noninformative priors
Posted by Andrew on	18 July 2007, 8:04 am
Neal writes,

As I start your Bayesian stuff, can I ask you the same question I asked Boris a few years ago, namely, as you note, noninf priors simply represent the situation where we know very little and want the data to speak (so in the end not too far from the classical view). Can you point me to any social science (closer to ps is better) where people actually update, so that the prior in a second study is the posterior of the first (whether or not the two studies done by same person or not).

Equivalently – point me to a study which uses non-inf priors. (as more than a toy – i know the piece by gill and his student).

Btw do you know the old piece by Harry Roberts, saying that as a scientist all we can report is the likelihood, and that everyone should put their own prior in and then produce their own posterior. so all articles would just be a computer program which takes as input my prior and produces my posterior given the likelihood surface estimated by the author?

My reply: now I like weakly informative priors. But that’s new since our books. Regarding informative priors in applied research, we can distinguish three categories:

(1) Prior distributions giving numerical information that is crucial to estimation of the model. This would be a traditional informative prior, which might come from a literature review or explicitly from an earlier data analysis.

(2) Prior distributions that are not supplying any controversial information but are strong enough to pull the data away from inappropriate inferences that are consistent with the likelihood. This might be called a weakly informative prior.

(3) Prior distributions that are uniform, or nearly so, and basically allow the information from the likelihood to be interpreted probabilistically. These are noninformative priors, or maybe, in some cases, weakly informative.

I have examples of (1), (2), and (3) in my own applied research. Category (3) is the most common for me, but an example of (2) is my 1990 paper with King on seats-votes curves, where we fit a mixture model and used an informative prior to constrain the locations, scales, and masses of the three components. An example of (3) is my 1996 paper with Bois and Jiang where we used an informative prior distribution for several parameters in a toxicology model. We were careful to parameterize the model so that these priors made sense, and the model also had an interesting two-level structure which we discuss in that paper and also in Section 9.1 of Bayesian Data Analysis.

Regarding your question about models where people actually update: we did this in our radon analysis (see here) where the posterior distribution from a national data analysis (based on data from over 80,000 houses) gives inference for each county in the U.S., which is in turn used as the prior distribution for the radon level in your house, which in turn can be updated if you have information from a measurement in your house.

One of the convenient things about doing applied statistics is that eventually I can come up with an example for everything from my own experience. (This also makes it fun to write books.)

Regarding your last comment: yes, there is an idea that a Bayesian wants everyone else to be non-Bayesian so that he or she can do cleaner analyses. I discuss that idea in this talk from 2003 which I’ve been too lazy to write up as a paper.

\subsection{Discussion two}
$http://andrewgelman.com/2007/05/11/weakly_informat/$

Weakly informative priors
Posted by Andrew on	11 May 2007, 1:01 pm

Bayesians traditionally consider prior distributions that (a) represent the actual state of subject-matter knowledge, or (b) are completely or essentially noninformative. We consider an alternative strategy: choosing priors that convey some generally useful information but clearly less than we actually have for the particular problem under study. We give some examples, including the Cauchy (0, 2.5) prior distribution for logistic regression coefficients, and then briefly discuss the major unsolved problem in Bayesian inference: the construction of models that are structured enough to learn from data but weak enough to learn from data.

I’m speaking Monday on this at Jun Liu’s workshop on Monte Carlo methods at Harvard (my talk is 9:45-10:30am Monday at 104 Harvard Hall).

Here’s the presentation. I think this is potentially a huge advance in how we think about Bayesian models.


