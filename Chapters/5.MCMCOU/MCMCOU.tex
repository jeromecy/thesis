


\section{State Space Models}
State space models are the natural form of system models relying on the general concept of state. If we describe a system as an operator mapping from the space of inputs to the space of outputs, then we may need the entire input-output history of the system together with the planned input in order to compute the future output values \cite{hangos2006analysis}. In an alternative way, by using new information at time $t$ containing all the past information up to the current state and initial conditions to get the current output is possible, that is known as a sequential method. A genetic state space model consists of two sets of equations: state equation and output equation. The state equation describes the evolution of the true input and state variables sequentially as a function and passes the variable one after one, generally, with some noises. The output equation catches the input values and interprets it out by an algebraic equation. A general state space model looks like the following form
\begin{align}\label{statemodel1}
\mbox{State equation } x_t &= G_t(x_{t-1})+w_t,\\
\label{statemodel2}
\mbox{Output equation } y_t &=F_t(x_t)+\epsilon_t
\end{align}
with an initial state $x_0$, where $\epsilon_t$ and $w_t$ are noises passing through the process $G_t$ and $F_t$. $x_t$ are true status variables and $y_t$ are output values. Many researchers have been interested in this model and its application because of its good property. It can be used to model univariate or multivariate time series, also in the presence of non-stationarity, structural changes, and irregular patterns \cite{petris2009dynamic}.

The most simple and important system is given by Gaussian linear state space models, also known by dynamic linear models (DLM), which defines a very general class of non-stationary time series models.  Firstly, the model is linear, which means $G_t$ and $F_t$ are linear processes and satisfying linearity property. Secondly, the it is specified by a normal prior distribution for the $p$-dimensional state vector at initial state $t=0$, 
\begin{align*}
x_0 \sim N_p(m_,0,C_0)
\end{align*} 
and two independent zero mean normal distributed noises $\epsilon_t \sim N_p(0,V_t)$ and $w_t \sim N_p(0,W_t)$ \cite{petris2009dynamic}. The well known Kalman Filter is a particular algorithm that is used to solve state space models in the linear case. This was first derived by Kalman \cite{kalman1960new}.

In a nonlinear state space model, the process $G_t$ and $F_t$ are no longer linear functions and the situation becomes more complicated. Here gives a simple nonlinear example of such a model, which has been used extensively in the literature for benchmarking numerical filtering techniques \cite{kitagawa1996monte} \cite{west1993mixture} \cite{gordon1993novel} assuming the sequence is Markovian.
\begin{align*}
x_t &= \frac{x_{t-1}}{2} +25\frac{x_{t-1}}{1+x_{t-1}^2}+8\cos(1.2t)+u_t\\
y_t &= \frac{x_{t}^2}{20}+v_t,
\end{align*}
where $u_t \sim N(0,\sigma_u^2)$, $v_t \sim N(0,\sigma_v^2)$, $\sigma_u^2=10$ and $\sigma_t^2=1$ are considered fixed and known. The initial state $x_0\sim N(0,10)$. The assumption Markovian keeps the current state $x_t$ only depending on the previous one step $x_{t-1}$ and the observed $y_t$ depending on $x_t$. A state-space is shown in the diagram below:
\begin{align*}
{\displaystyle {\begin{array}{cccccccccc}\cdots &\to &x_{t-1}&\to &x_{t}&\to &x_{t+1}&\to &\cdots &{\text{truth}}\\ \cdots &&\downarrow &&\downarrow &&\downarrow &&\cdots &\\ \cdots&&y_{t-1}&&y_{t}&&y_{t+1}&&\cdots &{\text{observation}}\end{array}}}
\end{align*}

In applications, the process function $G_t$ and $F_t$ contain unknown parameters to be estimated \cite{de1988likelihood} and the target is to estimate the true states on sequential observations $y_t, \cdots, y_t$. Then it becomes to estimate a joint density of $p(x_{1:t},\theta \mid y_{1:t})$, where $x_{1:t} = \{x_1, x_2, \cdots, x_t \}$ are the hidden states and $y_{1:t} = \{y_1, y_2, \cdots, y_t \}$ are the observed outcomes and $\theta$ is a set of unknown parameters. 


\section{Sequential Monte Carlo Method}

The use of Monte Carlo methods for non-linear filtering can be traced back to the pioneering contributions of Handschin and Mayne (1969) \cite{handschin1969monte} and Handschin
(1970) \cite{handschin1970monte}. These researchers tried to use an importance sampling paradigm to approximate the target distributions. Later on, an importance sampling algorithms were implemented sequentially in the non-linear filtering context. This algorithm is called sequential importance sampling, often abbreviated SIS, and has been known since the early 1970s. Limited by the power of computers and  suffering from sample impoverishment or weight degeneracy, the SIS didn't develop very well until 1993. A particle filter algorithm was proposed to allow rejuvenation of the set of samples by duplicating the samples with high importance weights and, on the contrary, removing samples with low weights \cite{cappe2009inference}. Since then, sequential Monte Carlo (SMC) methods have been applied in many different fields including but not limited to computer vision, signal processing, control, econometrics, finance, robotics, and statistics \cite{smcmip2011}  \cite{ristic2004beyond}.


\subsection{Filtering Problem and Estimation}
Sequential Monte Carlo method, also known as particle filter, is a technique based on sampling and importance sampling methods to find the best state estimation given by Gordon in 1993 \cite{gordon1993novel} and was the first successful application of sequential Monte Carlo techniques to the field of non-linear filtering \cite{cappe2009inference}. In the state space model, a generic particle filter estimates the posterior distribution of the hidden states using the observation measurement process. The filtering problem is to estimate sequentially the values of the hidden states $x_k$ given the values of the observation process $y_{1:k}$ at any time step $k$. In another word, it is to find the value of $p(x_k \mid  y_{1:k})$. The process is divided into two steps: prediction and updating. In the prediction step, the assumption of Markov Chain is the current status $x_k$ only depends on the previous one $x_{k-1}$. Then we can calculate the probability of $x_k$ by 
\begin{align*}
p(x_k\mid y_{1:k-1})=&\int p(x_k,x_{k-1}\mid y_{1:k-1}) dx_{k-1}\\
=&\int p(x_k\mid x_{k-1},y_{1:k-1}) p(x_{k-1}\mid y_{1:k-1})dx_{k-1}\\
=&\int p(x_k\mid x_{k-1}) p(x_{k-1}\mid y_{1:k-1})dx_{k-1}.
\end{align*}
In the updating step, once $p(x_k\mid y_{1:k-1})$ is known, $p(x_k\mid y_{1:k})$ can be found by
\begin{align*}
p(x_k\mid y_{1:k})=&\frac{p(y_k\mid x_k,y_{1:k-1})p(x_{k}\mid y_{1:k-1})}{p(y_k\mid  y_{1:k-1})} \\
=&\frac{p(y_k\mid x_k)p(x_{k}\mid y_{1:k-1})}{p(y_k\mid  y_{1:k-1})},
\end{align*}
where the normalization $p(y_k\mid  y_{1:k-1})=\int p(y_k\mid x_k)p(x_k\mid  y_{1:k-1}) dx_k$ \cite{arulampalam2002tutorial}.

Imagine that the stat space is partitioned as many parts, in which the particles are filled according to some probability measure. The higher probability, the denser the particles are concentrated. Suppose the particles $x_1, \cdots, x_N$ are drawn from the target probability density function $p(x)$, then these particles are used to estimate the expectation and variance of $f(x)$ by
\begin{align*}
E(f(x)) &= \int_a^bf(x)p(x)dx\\
Var(f(x)) &= E(f(x)-E(f(x)))^2p(x)dx.
\end{align*}
Back to our target, the posterior distribution or density is empirically represented by a weighted sum of samples $x_1, \cdots, x_N$ 
\begin{align*}
\hat{p}(x_n\mid y_{1:k})=\frac{1}{N}\sum_{i=1}^N\delta (x_n-x_n^{(i)})\approx p(x_n\mid y_{1:k}),
\end{align*}
where $f(x)=\delta (x_n-x_n^{(i)})$ is Dirac delta function. When $N$ is sufficiently large, $\hat{p}(x_n\mid y_{1:k})$ approximates the true posterior $p(x_n\mid y_{1:k})$. By this approximation, the filtering problem becomes to get the expectation of current status
\begin{align*}
E(f(x_n)) &\approx \int f(x_n)\hat{p}(x_n\mid y_{1:k})dx_n \\
 & =\frac{1}{N} \sum_{i=1}^N\int f(x_n) \delta (x_n-x_n^{(i)}) dx_n\\
 & = \frac{1}{N}\sum_{i=1}^Nf(x_n^{(i)}).
\end{align*}
The expectation is the mean of the status of all particles $x_1, \cdots, x_N$. 

However, the posterior distribution is unknown and impossible to sample from the true posterior. So some sampling methods are introduced.

\subsection{Sampling Methods}

\subsubsection{Importance sampling}

It is common to sample from an easy-to-implement distribution, the so-called proposal distribution $q(x\mid y)$, hence
\begin{align*}
E(f(x)) &= \int f(x_k)\frac{p(x_k\mid y_{1:k})}{q(x_k\mid y_{1:k})} q(x_k\mid y_{1:k})dx_x\\
&= \int f(x_k)\frac{p(x_k)p(y_{1:k}\mid x_k)}{p(y_{1:k})q(x_k\mid y_{1:k})} q(x_k\mid y_{1:k})dx_x\\
&= \int f(x_k)\frac{W_k(x_k)}{p(y_{1:k})} q(x_k\mid y_{1:k})dx_x,
\end{align*}
where $W_k(x_k)=\frac{p(x_k)p(y_{1:k}\mid x_k)}{q(x_k\mid y_{1:k})} \propto \frac{p(x_k\mid y_{1:k})}{q(x_k\mid y_{1:k})}$. Because $p(y_{1:k})=\int p(y_{1:k}\mid x_k)p(x_k)dx_k$, so the above equation can be rewritten as
\begin{align*}
E(f(x)) &= \frac{1}{p(y_{1:k})}\int f(x_k)W_k(x_k)q(x_k\mid y_{1:k})dx_k\\
&= \frac{ \int f(x_k)W_k(x_k)q(x_k\mid y_{1:k})dx_k }{\int p(y_{1:k}\mid x_k)p(x_k)dx_k} \\
&= \frac{ \int f(x_k)W_k(x_k)q(x_k\mid y_{1:k})dx_k }{\int W_k(x_k)q(x_k\mid y_{1:k})dx_k} \\
&= \frac{E_{q(x_k\mid y_{1:k})}[W_k(x_k)f(x_k)]}{E_{q(x_k\mid y_{1:k})}[W_k(x_k)]}.
\end{align*}
To solve the above equation, we can use Monte Carlo method by drawing samples $\{x_k^{(i)}\}$ from $q(x_k\mid y_{1:k})$ and get their expectation, which approximate by
\begin{align*}
E(f(x_k)) &\approx \frac{\frac{1}{N} \sum_{i=1}^{N} W_k(x_k^{(i)})f(x_k^{(i)})} {\frac{1}{N} \sum_{i=1}^{N} W_k(x_k^{(i)})}\\
&= \sum_{i=1}^{N} \tilde{W}_k(x_k^{(i)})f(x_k^{(i)}),
\end{align*}
where $\tilde{W}_k(x_k^{(i)}) = \frac{ W_k(x_k^{(i)})}{\sum_{i=1}^NW_k(x_k^{(i)})}$ is factorized weight. Each particles has its own weighted value, so the expectation is a weighted mean. However, the drawback of this method is that the computation is quite expensive. A smarter way is to update $W_k^{(i)}$ recursively. Suppose the proposal distribution 
\begin{align*}
q(x_{0:k}\mid y_{1:k}) = q(x_{0:k-1}\mid y_{1:k-1}) q(x_k\mid  x_{0:k-1},y_{1:k}),
\end{align*}
then the recursive form of the posterior distribution is 
\begin{align*}
p(x_{0:k}\mid y_{1:k}) &= \frac{p(y_k\mid x_{0:k},y_{1:k-1})p(x_{0:k}\mid y_{1:k-1})}{p(y_k\mid y_{1:k-1})}\\
&= \frac{p(y_k\mid x_{0:k},y_{1:k-1}) p(x_k\mid x_{0:k-1},y_{1:k-1}) p(x_{0:k-1}\mid y_{1:k-1} ) }{p(y_k\mid y_{1:k-1})}\\
&= \frac{p(y_k\mid x_k) p(x_k\mid x_{k-1}) p(x_{0:k-1}\mid y_{1:k-1} ) }{p(y_k\mid y_{1:k-1})}\\
&\propto p(y_k\mid x_k) p(x_k\mid x_{k-1}) p(x_{0:k-1}\mid y_{1:k-1} ),
\end{align*}
the recursive form of the weights are
\begin{align*}
W_k^{(i)} &\propto \frac{p(x_{0:k}^{(i)}\mid y_{1:k})}{q(x_{0:k}^{(i)}\mid y_{1:k})}\\
&= \frac{ p(y_{1:k}\mid x_{0:k}^{(i)}) p(x_{k}^{(i)}\mid x_{k-1}^{(i)})  p(x_{0:k-1}^{(i)}\mid y_{1:k-1})}   { q(x_{k}^{(i)}\mid x_{0:k-1}^{(i)},y_{k})  q(x_{0:k-1}^{(i)}\mid y_{1:k-1}) } \\
&= W_{k-1}^{(i)} \frac{ p(y_{1:k}\mid x_{0:k}^{(i)}) p(x_{k}^{(i)}\mid x_{k-1}^{(i)}) }   {q(x_{k}^{(i)}\mid x_{0:k-1}^{(i)},y_{k})}.
\end{align*}

\subsubsection{Sequential Importance Sampling and Resampling}
 
In practice, we are interested in the current filtered estimate $p(x_k\mid y_{1:k})$ instead of $p(x_{0:k}\mid y_{1:k})$. Provided 
\begin{align*}
q(x_k\mid  x_{0:k-1},y_{1:k})=q(x_k\mid  x_{k-1},y_k),
\end{align*}
the importance weights $W_k^{(i)}$ can be updated recursively
\begin{align*}
W_k^{(i)} &\propto W_{k-1}^{(i)} \frac{ p(y_k\mid x_k^{(i)}) p(x_{k}^{(i)}\mid x_{k-1}^{(i)}) }   {q(x_{k}^{(i)}\mid x_{k-1}^{(i)},y_{k})}.
\end{align*}

The problem of SIS filter is that the distribution of importance weights becomes more and more skewed as time increases. Hence, after some iterations, only very few particles have non-zero importance weights. This phenomenon is called \textit{weight degeneracy} or \textit{sample impoverishment} \cite{smcmip2011}.

The effective sample size $\textit{N}_{\textit{eff}}$ is suggested to monitor how bad the degeneration is, which is
\begin{align*}
\textit{N}_{\textit{eff}}=\frac{N}{1+var(w_k^{*(i)})},
\end{align*}
where $w_k^{*(i)}=\frac{p(x_k^{(i)}\mid y_{1:k})}{q(x_k^{(i)}\mid x_{k-1}^{(i)},y_{1:k})}$. The more different between the biggest weight and smallest weight, the worse the degeneration is. In practice, the effective sample size is approximated by
\begin{align*}
\hat{N}_{\textit{eff}}\approx \frac{1}{\sum_{i=1}^{N}(w_k^{(i)})^2}.
\end{align*}
If the value of $\textit{N}_{\textit{eff}}$ is less than some threshold, some procedure should be used to avoid a worse degeneration. There are two ways one can do: choose an appropriate PDF for importance sampling, or use re-sampling after SIS. 

The idea of resampling is keeping the same size of particles, replacing the low weights particles with new ones. As discussed before, 
\begin{align*}
p(x_k\mid y_{1:k})=\sum_{i=1}^Nw_k^{(i)} \delta (x_k-x_k^{(i)}).
\end{align*}
After resampling, it becomes
\begin{align*}
\tilde{p}(x_k\mid y_{1:k})=\sum_{j=1}^N\frac{1}{N} \delta (x_k-x_k^{(j)})= \sum_{i=1}^N\frac{n_i}{N} \delta (x_k-x_k^{(i)}),
\end{align*}
where $n_i$ represents how many times the new particles $x_k^{(j)}$ were duplicated from$x_k^{(i)}$. 

Then the process of SIS particle filter with re-sampling is:
\begin{itemize}
\item Initial particles when $k=0$. For $i=1, \cdots, N$, draw samples $\{x_0^{(i)}\}$ from $p(x_0)$.
\item For $k=1,2,\cdots$, run the process recursively
\begin{itemize}
\item Importance sampling: draw sample $\{\tilde{x}_k^{(i)}\}_{i=1}^N$ from $q(x_k\mid y_{1:k})$, calculate their weights $\tilde{w}_k^{(i)}$ and normalize them.
\item Re-sampling: Re-sample $\{\tilde{x}_k^{(i)}, \tilde{w}_k^{(i)}\}$ and get a new set $\{x_k^{(i)},\frac{1}{N}\}$.
\item Output the status at time $k$: $\hat{x}_k=\sum_{i=1}^{N}\tilde{x}_k^{(i)}\tilde{w}_k^{(i)}$.
\end{itemize}
\end{itemize}

In SIR, if we choose
\begin{align*}
q(x_k^{(i)}\mid x_{k-1}^{(i)},y_k) = p(x_k^{(i)}\mid x_{k-1}^{(i)}),
\end{align*}
the weights become
\begin{align*}
w_k^{(i)}&\propto w_{k-1}^{(i)}\frac{ p(y_k\mid x_{k}^{(i)}) p(x_k^{(i)}\mid x_{k-1}^{(i)}) }{q(x_k^{(i)}\mid x_{k-1}^{(i)},y_k) }\\
&\propto w_{k-1}^{(i)}p(y_k\mid x_{k}^{(i)}).
\end{align*}
Because $w_{k-1}^{(i)}=\frac{1}{N}$, thus we have $w_k^{(i)} \propto p(y_k\mid x_{k}^{(i)})$ and
\begin{align*}
w=\frac{1}{\sqrt{2\pi\Sigma}}exp\left(-\frac{1}{2} (y_{true}-y)\Sigma^{-1}(y_{true}-y)\right).
\end{align*}


\subsubsection{Delayed Acceptance Metropolis-Hastings Algorithm}

Importance sampling works well only if the proposal density $q(x)$ is similar to $p(x)$. In large and complex problems it is difficult to create a single density $q(x)$ that has this property \cite{mackay2003information}. Here, we introduce the Metropolis-Hastings algorithm, which makes use of a proposal density $q(x)$ depending on the current state $x_t$ instead. We assume that we can evaluate $p(\theta)$ for any $\theta$. The transition probabilities should satisfy the detailed balance condition
\begin{equation*}
\pi(\theta)p(\theta'\mid \theta) = \pi(\theta')p(\theta\mid \theta'),
\end{equation*}
that means transition from $\pi(\theta)$ to $\pi(\theta')$ has the same probability as that 
from $\pi(\theta')$ to $\pi(\theta)$. In sampling method, drawing $\theta_i$ first and then drawing $\theta_j$ should have the same probability as drawing $\theta_j$ and then drawing $\theta_i$. However, in most situations, the details balance condition is not satisfied. Therefore, we introduce a function $\alpha(x,y)$ satisfying 
\begin{equation*}
p(\theta_i)q(\theta_i, \theta_j)\alpha(\theta_i,\theta_j) = p(\theta_j)q(\theta_j, \theta_i)\alpha(\theta_j,\theta_i).
\end{equation*}
A tentative new state $\theta'$ is generated from the proposal density $q(\theta';\theta^{(t)})$. 
To decide whether to accept the new state, we compute the quantity
\begin{equation*}
\alpha=\frac{p(\theta')}{p(\theta^{(t)})}\frac{q(\theta^{(t)};\theta')}{q(\theta';\theta^{(t)})}.
\end{equation*}
If $\alpha \geq 1$, then the new state is accepted. Otherwise, the new state is accepted with probability $\alpha$. A drawback of MH algorithm is a large time consuming in calculating $p(\theta)$ if it's in a irregular structure. A delayed acceptance MH algorithm introduces a cheap approximation $\hat{p}(\theta)$ for $p(\theta)$ in two stages. In stage one, the quantity $\alpha_1$ is found by a standard MH acceptance formula 
\begin{equation}
\alpha_1=\min\left\lbrace  1,\frac{\hat{p}(\theta^*)q(\theta\mid \theta^*)}{\hat{p}(\theta)q(\theta^*\mid \theta)}  \right\rbrace ,
\end{equation}
where $\hat{p}(\cdot)$ is a cheap estimation for $\theta$ and a simple form is $\hat{p}(\cdot)=N(\cdot\mid \hat{\theta},\sigma)$. Once $\alpha_1$ is accepted, the process goes into stage two and the acceptance probability $\alpha_2$ is
\begin{equation}
\alpha_2=\min \left\lbrace  1,\frac{p(\theta^*)\hat{p}(\theta) }{p(\theta)\hat{p}(\theta^*)} \right\rbrace,
\end{equation}
where the overall acceptance probability $\alpha_1\alpha_2$ ensures that detailed balance is satisfied with respect to $p(\cdot)$; however if a rejection occurs at stage one then the expensive evaluation of $p(\theta)$ at stage two is unnecessary.

In a random walk, the proposal density function $q(\cdot)$ can be chosen for some suitable normal distribution, and hence $q(\theta^*\mid \theta)=N(\theta^*\mid \theta,\epsilon^2)$ and $q(\theta\mid \theta^*)=N(\theta\mid \theta^*,\epsilon^2)$ cancel in the Delayed Acceptance MH process stage one \cite{sherlock2016adaptive}. Then in our case, the proposal $\theta' \sim N(\theta^{(t)}, \sigma)$ and the density $q$ is symmetric, so it becomes
\begin{equation}
\alpha=\frac{p^*(\theta')}{p^*(\theta^{(t)})}=\frac{p(y_{1:t}\mid \theta')p(\theta')}{p(y_{1:t}\mid \theta^{(t)})p(\theta^{(t)})}.
\end{equation}


\section{Bayesian Parameter Estimation}

The state transition density and the conditional likelihood function depend not only upon the dynamic state $x_t$, but also on a static parameter vector $\theta$, which will be stressed by use of the notations $f(x_t \mid x_{t-1},\theta)$ and $g(y_t\mid x_t,\theta)$. To estimate $\theta$, we would consider a Bayesian method in the following two situations: off-line, estimating the parameters by a batch of data, and on-line, by an instant updated sequential data stream. Specifically, the advantage of Bayesian than maximum likelihood method is that the unknown parameter is considered random and assigned a suitable prior distribution, which is addressed from the experiences of researchers or a learning process and easily to be implemented in the algorithm of machine learning.

Generally, in the Bayesian setting, we choose a suitable prior density $p(\theta)$ for $\theta$ and compute the joint posterior density $p(x_{0:t},\theta \mid y_{0:t})$ in the off-line case, or the sequence of posterior densities $\{ p(x_{0:n},\theta \mid y_{0:n})\}$ in the on-line setting \cite{kantas2009overview}.



\subsection{Off-line Methods}

In the off-line setting, the parameters can be estimated with non-sequential Monte Carlo methods, such as Markov Chain Monte Carlo \cite{robert2004monte}. However, it is recognized that the sequential MC methods have some significant advantages in some certain cases, like \cite{cappe2009inference} and \cite{del2006sequential}. Additionally, it is difficult to design an efficient MCMC sampling algorithm for a nonlinear non-Gaussian state space model. A Particle  MCMC method is proposed by \cite{andrieu2010particle}, which is a new class of MCMC techniques relying on Standard MC methods to build efficient high dimensional proposal distributions. 

PMMH jointly updates $\theta$ and $x_{0:t}$ for state space models. It proposes a new $\theta^*$ from a proposal density function $q(\theta^* \mid \theta)$, and then generates $x^*_{0:t}$ by running bootstrap particle filter with $\theta^*$. The acceptance ratio of this sampler is
\begin{align*}
\alpha &= \min \left\lbrace 1,\frac{ p(x_{0:t}^*, \theta^* \mid y_{0:t} ) q((x_{0:t},\theta) \mid  (x_{0:t}^*,\theta^*)   ) }{  p(x_{0:t}, \theta \mid y_{0:t} ) q((x_{0:t}^*,\theta^*) \mid  (x_{0:t},\theta) ) }  \right\rbrace \\
           &= \min \left\lbrace 1,\frac{p_{\theta^*} (y_{0:t}) p(\theta^*)q(\theta\mid\theta^*) }{  p_{\theta} (y_{0:t}) p(\theta)q(\theta^*\mid\theta)  }  \right\rbrace .
\end{align*}
The PMMH sampler is an approximation of the ideal MMH sampler for sampling from $p(x^t,\theta\mid y^t)$. Apparently, the higher number of particles $N$ the better the mixing properties of the algorithm, in contrast, the lower efficiency of computation.

%
%The data likelihood is
%\begin{align*}
%l_{1:t}(y^t\mid \theta) = \int p(y^t,x^t\mid\theta)dx^t
%\end{align*}
%and the prediction likelihood is 
%\begin{align*}
%l_{t+1}(y_{t+1}\mid y^t,\theta) &= \int p(y_{t+1},x_{t+1}\mid y^t,\theta)dx_{t+1} \propto \sum_{i=1}^N\omega_{t+1}^{(i)},
%\end{align*}
%where $\omega_{t+1}^{(i)} = g(y_{t+1}\mid x_t^{(i)},\theta)$ are the unnormalized importance weights of particles. 
%
%If the dimension of $\theta$ is small, one can directly use  the particle approximation to the likelihood $l_{t+1}(y^t\mid\theta)$ for instance evaluated on a grid of values of $\theta$. If the dimension is large, a natural option is using iterative optimization algorithms, such as EM. The most natural sequential Monte Carlo approximation equation is given by
%\begin{align*}
%\hat{\tau}_{t\mid 0:t}(y^t,\theta) =  \sum_{i=1}^{N}\omega_t^{(i)}\sum_{t=0}^{t-1}s_t(x_t^{(i)},x_{t+1}^{(i)}).
%\end{align*}
%Such approximations have been used with reasonable success either using MC versions of EM algorithm or stochastic gradient procedures. However, when the number of observations $t$ becomes large, the number $N$ of particles should be increased to ensure the convergence of the optimization procedure. 

%\propto \sum_{i=1}^N\omega^{(i)}\int g(y_{t+1}\mid x_{t+1},\theta)f(x_{t+1}\mid x_t^{(i)},\theta)dx_{t+1}.



\subsection{On-line Methods}

Putting the algorithms on-line means to update the parameters and states instantly as new observations coming into the data stream. For Bayesian dynamic models, however, the most natural option consists in treating the unknown parameter $\theta$, using the state space representation, as a component of the state which has no dynamic evolution, also referred to as a static parameter \cite{cappe2007overview}. 

The standard SMC is deficiency for on-line estimation. As a result of the successive resampling steps, after a certain time $n$, the approximation $\hat{p}(\theta\mid y^{1:t})$ will only contain a single unique value for $\theta$. In other words, SMC approximation of the marginalized parameter posterior distribution is represented by a single Dirac delta
function. It also causes error accumulation in successive Monte Carlo (MC) steps grows exponentially or polynomially in time.

The target is to estimate $p(\theta \mid y_{1:t})$ by
\begin{equation}
p(\theta \mid y_{1:t}) \propto p(y_{1:t} \mid \theta ) p(\theta )
\end{equation}
without introducing any bias or controlling the bias in states propagation. A pragmatic approach to reduce parameter sample degeneracy and error accumulation in successive MC approximations is to adding an artificial dynamic equation on $\theta$ \cite{higuchi2001self} \cite{kitagawa1998self}, which gives
\begin{align*}
\theta_{n+1} = \theta_n+\varepsilon_{n+1}.
\end{align*}
With a small artificial noise, SMC can now be applied to approximate $p(x^t,\theta\mid y^t)$. A related kernel density estimation method proposes a kernel density estimate of the target \cite{liu2001combined} \begin{align*}
\hat{p}(\theta\mid y^t) = \frac{1}{N}\sum M(\theta-\theta_n^{(i)}). 
\end{align*} 
Both of these methods require a significant amount of tuning.

A fixed-lag practical filtering is used to approximate
\begin{align*}
p(x_{0:n-L},\theta\mid y_{0:n-1})\approx p(x_{0:n-L},\theta \mid y^n)
\end{align*}
for $L$ large enough in reference \cite{polson2008practical}. $x_{0:n-L}$ has very little influence on observations coming after $n$. The choice of the lag $L$ is difficult and ther is a non-vanishing bias which is difficult to quantify.

A MCMC kernel with invariant density $p(x^t,\theta\mid y^t)$ is used in SMC algorithm. This method was firstly used in an on-line Bayesian parameter estimation, where the author in \cite{andrieu1999sequential} were using
\begin{align*}
K_n(x_{1:t}',\theta'\mid x_{1:t},\theta) = \delta_{x_{1:t}}(x_{1:t} ')p(\theta'\mid x_{1:t},y_{1:t}),
\end{align*}
where $p(y^t\mid\theta,x^t)=p(\theta\mid s_t(x^t,y^t))$ and $s_t(x^t,y^t)$ is a fixed-dimensional vector of sufficient statistics. MCMC can be used to maintain the diversity of the samples of $\theta$. Here the stationary distribution for the MCMC will be the full joint posterior distribution of states and parameters and apply MH or Gibbs sampling separately to $p(\theta \mid x^t,y^t)$ and $p(x^t \mid \theta,y^t)$. However, this method is not feasible for large dataset. 


\section{Combined State and Parameters Estimation of Sequential Monte Carlo Algorithm}

To work out the best estimations for $x$ and $u$, one has to solve the target function
\begin{align}\label{objecfun}
p(X\mid Y) = \int p(X\mid Y,\theta)p(\theta\mid Y)d\theta.
\end{align} 
The main work need to be done is finding an efficient way to sort out the integration in the above equation. Several methods can be used, such as cross validation, Expectation Maximization algorithm, Gibbs sampling and Metropolis-Hastings algorithm and so on. A Monte Carlo method is popular in research area solving this problem. Monte Carlo method is an algorithm that relies on repeated random sampling to obtain numerical results. To compute an integration of $\int f(x)dx$, one has to sampling as many independent $x_i \mbox{ } (i = 1,\cdots, N)$ as possible and numerically to find $\frac{1}{N}\sum_i f(x_i)$ to approximate the target function. 

In our target function, we have to sampling $\theta$ and use a numerical way to calculate its integration. Here are two ways of solving the sampling problem sequentially: 
\begin{align*}
\begin{cases}
\mbox{M1}: & p(\theta\mid Y_{1:t},Y_{t+1}) \propto p(Y_{1:t},Y_{t+1}\mid\theta)p(\theta)\\
\mbox{M2}: & p(\theta\mid Y_{1:t},Y_{t+1}) \propto p(Y_{t+1}\mid\theta,Y_{1:t})p(\theta\mid Y_{1:t})
\end{cases}.
\end{align*}


NOTES: add more..........


\subsection{General Linear Space}
In one dimensional state space model, we consider the hidden state process $\{x_t, t\geq 1\}$ is a stationary and ergodic Markov process and transited by $f(x'\mid x)$. In this paper, we assume that the current state $x_t$ only depends on the previous one step $x_{t-1}$, which is known as $\textit{AR(1)}$ model. As indicated by its name, the states are not observed directly but by another process $\{y_t, t\geq 1\}$, which is assumed depending on $\{x_t\}$ by the process $g(y\mid x)$ only and independent with each other. If the transition processes $f$ and $g$ are linear and normal distributed, we call this model $\textit{Linear Gaussian Model}$, that can be written as
\begin{align*}
y_t\mid x_t      &\sim N(\gamma x_t,\sigma^2) \\
x_t\mid x_{t-1} &\sim N(\phi x_{t-1},\tau^2),
\end{align*}
where $\sigma$ and $\tau$ are errors occurring in processes, $\gamma$ and $\phi$ are static process parameters. 

In a simple scenario, by assuming $\gamma=1$ gives us the joint distribution for $x_{0:t}$ and $y_{1:t}$ as following
\begin{equation*}
\begin{bmatrix} x\\y \end{bmatrix}
\sim N\left(0, \Sigma  \right),
\end{equation*}
where $\Sigma^{-1}$ looks like
\begin{equation*}
\begin{bmatrix}
\frac{1}{L^2}+\frac{\phi^2}{\tau^2} & \frac{-\phi}{\tau^2} & \cdots & 0 & 0 & 0& \cdots & 0\\\\
\frac{-\phi}{\tau^2}   & \frac{1+\phi^2}{\tau^2}+\frac{1}{\sigma^2}& \cdots & 0 & -\frac{1}{\sigma^2} &0 & \cdots & 0 \\
0 & \frac{-\phi}{\tau^2}   &  \cdots & 0 & 0& -\frac{1}{\sigma^2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0   &  \cdots & \frac{1}{\tau^2}+\frac{1}{\sigma^2} & 0 & 0 & \cdots &-\frac{1}{\sigma^2}\\
0 & -\frac{1}{\sigma^2}  & \cdots & 0 & \frac{1}{\sigma^2} & 0 & \cdots & 0 \\
0& 0 & \cdots & 0 & 0 &  \frac{1}{\sigma^2} & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0& \cdots &-\frac{1}{\sigma^2} & 0 & 0 & \cdots &  \frac{1}{\sigma^2}
\end{bmatrix},
\end{equation*}
is the general procedure matrix denoted as $\Sigma^{-1}=\begin{bmatrix} A & -B \\ -B & B \end{bmatrix}$. Its inverse is 
\begin{equation}
\Sigma=\begin{bmatrix} (A-B)^{-1} &  (A-B)^{-1} \\ (A-B)^{-1} & (I-A^{-1}B)^{-1}B^{-1} \end{bmatrix} \triangleq \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY}  \\ \Sigma_{YX} & \Sigma_{YY} 
\end{bmatrix}
\end{equation}
the covariance matrix, where $B$ is a $t\times t$ diagonal matrix with elements $\frac{1}{\sigma^2}$. The covariance matrices $\Sigma_{XX} =  (A-B)^{-1}$ and $\Sigma_{YY} =  (I-A^{-1}B)^{-1}B^{-1}$ are easily found. Here the parameter $\theta$ represents for the unknown parameters $\phi,\sigma,\tau$. 

To find a recursive way of calculating the log likelihood posteriors, we introduce the Sherman-Morrison-Woodbury formula here first. In the late 1940s and the 1950s, Sherman and Morrison\cite{sherman1950adjustment}, Woodbury \cite{woodbury1950inverting}, Bartlett \cite{bartlett1951inverse} and Bodewig \cite{bodewig1959matrix} discovered the following result. The original Sherman-Morrison-Woodbury (for short SMW) formula has been used to consider the inverse of matrices \cite{deng2011generalization}. In this paper, we will consider the more generalized case. 

Theorem 1.1 (Sherman-Morrison-Woodbury). Let $A \in B(H)$ and $G \in B(K)$ both be invertible, and $Y, Z \in B(K, H)$. Then $A + YGZ^*$ is invertible if and only if $G^{-1} + Z^∗A^{-1}Y$ is invertible. In which case,
\begin{equation}
(A+YGZ^*)^{-1}= A^{-1}-A^{-1}Y(G^{-1}+Z^∗A^{-1}Y)^{-1}Z^∗A^{-1}.
\end{equation}

%\begin{itemize}
%\item [1] Chun Yuan Deng, A generalization of the Sherman-Morrison-Woodbury formula.
%\item [3] J. Sherman, W.J. Morrison, Adjustment of an inverse matrix corresponding to a change in one element of a given matrix, Ann. Math. Statist. 21 (1950)
%124–127.
%\item [4] M.A. Woodbury, Inverting Modifed Matrices, Technical Report 42, Statistical Research Group, Princeton University, Princeton, NJ, 1950.
%\item [5] M.S. Bartlett, An inverse matrix adjustment arising in discriminant analysis, Ann. Math. Statist. 22 (1951) 107–111.
%\item [6] E. Bodewig, Matrix Calculus, North-Holland, Amsterdam, 1959.
%\end{itemize}

A simple form of SMW formula is Sherman-Morrison formula represented in the following statement \cite{bartlett1951inverse}:
Suppose $A\in R^{n\times n}$ is an invertible square matrix and $u,v\in R^n$ are column vectors. Then $A+uv\top$ is invertible $\iff 1+u^\top A^{-1}v\neq 0$. If $A+uv\top$ is invertible, then its inverse is given by
\begin{equation}
(A+uv^{T})^{-1}=A^{-1}-{A^{-1}uv^{T}A^{-1} \over 1+v^{T}A^{-1}u}.
\end{equation}



\subsubsection{The Forecast Distribution $p(y_{t+1}\mid y_{1:t},\theta)$}

The joint distribution for $y_{t+1}$ and $y_{1:t}$ is $p(y_{1:t+1}\mid \theta)\sim N(0,\Sigma_{YY})$, where $\Sigma_{YY} = (I-A^{-1}B)^{-1}B^{-1}$ is the covariance matrix given above. One may find the inverse of the covariance matrix 
\begin{align*}
\Sigma_{YY}^{-1} = B(I-A^{-1}B) =\frac{1}{\sigma^4}(\sigma^2 I-A^{-1}) \triangleq \frac{1}{\sigma^4} \left[\begin{matrix} 
Z_{t+1} & b_{t+1} \\
b_{t+1}^\top & K_{t+1}
\end{matrix} \right].
\end{align*}
Therefore, the original form of this covariance is 
\begin{align*} \Sigma_{YY} =\sigma^4 \left[ \begin{matrix}
(Z-bK^{-1}b^\top)^{-1} & -Z^{-1}b(K-b^\top Z^{-1}b)^{-1}\\
-K^{-1}b^\top (Z-bK^{-1}b^\top)^{-1} & (K-b^\top Z^{-1}b)^{-1}
\end{matrix}\right].
\end{align*}
For sake of simplicity, here we are using $Z$ to represent the $t\times t$ matrix $Z_{t+1}$, $b$ to represent the $t \times 1$ vector  $b_{t+1}$  and $K$ to represent the $1\times 1$ constant $K_{t+1}$. 


By denoting $C_{t+1} = \left[ \begin{array}{c} 0\\\vdots \\ 0 \\ 1\end{array} \right]$ and multiplying $\Sigma_{YY}^{-1}$, it gives us
\begin{align*}
\Sigma_{YY}^{-1} C_{t+1}= \frac{1}{\sigma^4}(\sigma^2 I-A^{-1}) C_{t+1}= \frac{1}{\sigma^4} \left[\begin{matrix} b_{t+1} \\ K_{t+1} \end{matrix} \right].
\end{align*} 
In order to find $b$ and $K$ easily, one has to use Sherman-Morrison formula in the following way, that
\begin{equation}
A_{t+1}^{-1}C_{t+1} = \left( I - \frac{M_{t+1}^{-1}u_{t+1}u_{t+1}^\top }{1+u_{t+1}^\top M_{t+1}^{-1} u_{t+1}} \right)M_{t+1}^{-1}C_{t+1},
\end{equation}
in which
\begin{align*}
M_{t+1}^{-1}C_{t+1}    &=\left[ \begin{array}{cc} A_{t}^{-1} & 0 \\ 0 & \sigma^2 \end{array} \right]C_{t+1}=\sigma^2 C_{t+1},\\
u_{t+1}^\top C_{t+1} & = \left[ \begin{array}{ccccc} 0 & \cdots & 0 &\frac{-\phi}{\tau} & \frac{1}{\tau} \end{array} \right] \left[ \begin{array}{c} 0 \\ \vdots \\ 0\\ 1 \end{array} \right]= \frac{1}{\tau}.
\end{align*}
Then the above equation becomes
\begin{equation}
A_{t+1}^{-1}C_{t+1} = \sigma^2 C_{t+1}-\frac{M_{t+1}^{-1} u_{t+1} \frac{\sigma^2}{\tau}}{1+u^\top M_{t+1}^{-1} u}.
\end{equation}
Moreover,
\begin{align*}
M_{t+1}^{-1} u_{t+1} &=\left[ \begin{array}{cc} A_{t}^{-1} & 0 \\ 0 & \sigma^2 \end{array} \right] \left[ \begin{array}{c}  0 \\ \vdots \\0 \\ -\frac{\phi}{\tau} \\\frac{1}{\tau} \end{array} \right] =
\left[ \begin{array}{cc} A_{t}^{-1} & 0 \\ 0 & \sigma^2 \end{array} \right] \left[ \begin{array}{c} -\frac{\phi}{\tau} C_t \\ \frac{1}{\tau}\end{array} \right] = \left[ \begin{array}{c} -\frac{\phi}{\tau} A_{t}^{-1}C_t \\ \frac{\sigma^2}{\tau}\end{array} \right] ,\\
u^\top M_{t+1}^{-1}  u &=\left[ \begin{array}{ccccc}  0 & \cdots & 0 & -\frac{\phi}{\tau} & \frac{1}{\tau} \end{array} \right] \left[ \begin{array}{c} -\frac{\phi}{\tau} A_{t}^{-1}C_t \\ \frac{\sigma^2}{\tau} \end{array} \right] =  \left[ \begin{array}{cc} -\frac{\phi}{\tau} C_t^\top & \frac{1}{\tau}\end{array} \right]  \left[ \begin{array}{c} -\frac{\phi}{\tau} A_{t}^{-1}C_t \\ \frac{\sigma^2}{\tau}\end{array} \right] =\frac{\phi^2}{\tau^2} C_t^\top A_{t}^{-1}C_t+\frac{\sigma^2}{\tau^2}.
\end{align*}
Thus
\begin{align}
\begin{split}
A_{t+1}^{-1}C_{t+1} &= \left[ \begin{array}{c} -b_{t+1} \\ \sigma^2-K_{t+1}\end{array} \right] = \sigma^2C_{t+1}-\frac{1}{1+\frac{\phi^2}{\tau^2} C_t^\top A_{t}^{-1}C_t+\frac{\sigma^2}{\tau^2}} \left[\begin{array}{c} -\frac{\phi\sigma^2}{\tau^2} A_{t}^{-1}C_t \\\frac{\sigma^4}{\tau^2} \end{array}\right] \\
&= \sigma^2C_{t+1}-\frac{1}{\tau^2+\phi^2C_t^\top A_{t}^{-1}C_t+\sigma^2} \left[\begin{array}{c} -\phi\sigma^2 A_{t}^{-1}C_t \\ \sigma^4 \end{array}\right]
\end{split}
\end{align}
and
\begin{equation*}
\sigma^2-K_{t+1} = \sigma^2 - \frac{\sigma^4}{\tau^2+\phi^2C_t^\top A_{t}^{-1}C_t+\sigma^2} = \sigma^2 - \frac{\sigma^4}{\tau^2+\sigma^2+\phi^2(\sigma^2-K_t)},
\end{equation*}
therefore
\begin{align}
K_{t+1}  &=\frac{\sigma^4}{\tau^2+\sigma^2+\phi^2(\sigma^2-K_t)},
\end{align}
and
\begin{align}b_{t+1} = 
\begin{bmatrix}
\frac{b_t\phi K_{t+1}}{\sigma^2} \\ \frac{K_{t+1}(\sigma^2+\tau^2)-\sigma^4 }{\phi\sigma^2}
\end{bmatrix},
\end{align}

\begin{align*}
\bar{\mu}_{t+1}      &= 0-\sigma^4 K^{-1}b^\top (Z-bK^{-1}b^\top)^{-1} \sigma^{-4} (Z-bK^{-1}b^\top) y_{1:t} \\
					 & =-K^{-1}b^\top y_{1:t} \\
					 & = \frac{\phi}{\sigma^2}K_t\bar{\mu}_t + \phi (1 - \frac{K_t}{\sigma^2})y_t, \\
\bar{\Sigma}_{t+1} &= \sigma^4(K-b^\top Z^{-1}b)^{-1}- \sigma^4K^{-1}b^\top (Z-bK^{-1}b^\top)^{-1} (Z-bK^{-1}b^\top)Z^{-1}b(K-b^\top Z^{-1}b)^{-1}\\
                     & = \sigma^4(I-K^{-1}b^\top Z^{-1}b)(K-b^\top Z^{-1}b)^{-1} \\
                     & = \sigma^4K_{t+1}^{-1},
\end{align*}
where $K_1=\frac{\sigma^4}{\frac{\phi^2}{\tau^2}+\frac{1}{L^2}}$.



\subsubsection{The Estimation Distribution $p(x_{t+1}\mid y_{1:t+1},\theta)$}

The joint distribution for $x_{t+1}$ and $y_{1:t+1}$ is $p(x_{t+1}, y_{1:t+1}  \mid  \theta)\sim N(0,\Gamma)$, where
\begin{equation*}
\Gamma=\begin{bmatrix} C_{t+1}^\top(A-B)^{-1}C_{t+1} & C_{t+1}^\top(A-B)^{-1}\\(A-B)^{-1}C_{t+1} & (I-A^{-1}B)^{-1}B^{-1} \end{bmatrix},
\end{equation*}
where $C_{t+1}^\top$ is a $1\times t+1$ vector $\left[ \begin{array}{cccc} 0 &\cdots & 0 & 1\end{array} \right]_{1 \times t+1}$ retrieving the last column of a matrix. Because of 
\begin{align*}
C_{t+1}^\top A_{t+1}^{-1} = \left[\begin{matrix} - b_{t+1}^\top & \sigma^2- K_{t+1} \end{matrix} \right],
\end{align*}
thus $x_{t+1}\mid y_{1:t+1}\sim N(\bar{\mu}_{t+1}^{(x)},\bar{\sigma}_{t+1}^{(x)2})$, where
\begin{align*}
\bar{\mu}_{t+1}^{(x)}       &= \phi \hat{x}_{t} +  C_{t+1}^\top (A-B)^{-1}B (I-A^{-1}B)y_{1:t+1}\\
                      &= \phi \hat{x}_{t} +  C_{t+1}^\top A^{-1}B y_{1:t+1} \\ &= \phi \hat{x}_{t} +  \frac{1}{\sigma^2}C_{t+1}^\top A^{-1} y_{1:t+1}\\
                      &=0+  \frac{1}{\sigma^2}\left[\begin{matrix} - b_{t+1}^\top & \sigma^2- K_{t+1} \end{matrix} \right]  \left[\begin{matrix} y_{1:t} \\ y_{y+1} \end{matrix} \right] \\
                      &= - \frac{1}{\sigma^2}b_{t+1}^\top y_{1:t}+(1-\frac{K_{t+1}}{\sigma^2})y_{t+1}\\
                      &=\frac{K_{t+1}\bar{\mu}_t}{\sigma^2}+(1-\frac{K_{t+1}}{\sigma^2})y_{t+1} \\
\bar{\sigma}_{t+1}^{(x)2}&=C_{t+1}^\top(A-B)^{-1}C_{t+1}-  C_{t+1}^\top(A-B)^{-1}  B(I-A^{-1}B) (A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top(A-B)^{-1}C_{t+1} -  C_{t+1}^\top A^{-1}B(A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top A^{-1}C_{t+1} \\ &= \sigma^2-K_{t+1}.
\end{align*}



\subsubsection{Approximations of The Parameters Posterior}

Because of the covariance  $\Sigma_{YY} =  (I-A^{-1}B)^{-1}B^{-1}$, therefore the inverse is 
\begin{align*}
\Sigma_{YY}^{-1} &= B(I-A^{-1}B)= BA^{-1}\Sigma_{XX}^{-1}.
\end{align*}
Given the Choleski decomposition $LL^\top = A$, we have
\begin{align*}
\Sigma_{YY}^{-1} &=BL^{-\top}L^{-1}\Sigma_{XX}^{-1}\\
&=(L^{-1}B)^\top(L^{-1}\Sigma_{XX}^{-1})\\
&=\mbox{solve}(L,B)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}).
\end{align*}
More usefully, by given another Choleski decomposition $RR^\top=A-B=\Sigma_{XX}^{-1}$,
\begin{align}\label{sigmayy01}
\begin{split}
Y^\top \Sigma_{YY}^{-1} Y &= \mbox{solve}(L,BY)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
&\triangleq W^\top \mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
\end{split}
\end{align}
\begin{align}\label{sigmayy02}
\begin{split}
\det\Sigma_{YY}^{-1} &= \det B \det L^{-\top}\det L^{-1}\det R\det R^\top\\
&= \det B(\det L^{-1})^2(\det R)^2.
\end{split}
\end{align}

From the objective function, the posterior distribution of $\theta$ is 
\begin{align*}
p(\theta \mid Y) &\propto p(Y\mid\theta)p(\theta) \propto e^{-\frac{1}{2} Y \Sigma_{YY}^{-1} Y } \sqrt{\det \Sigma_{YY}^{-1}} p(\theta).
\end{align*}
Then by taking natural logarithm on the posterior of $\theta$ and using the useful solutions in equations (\ref{sigmayy01}) and (\ref{sigmayy02}), we will have
\begin{align}\label{logL}
\ln L(\theta) &= -\frac{1}{2}Y^\top\Sigma_{YY}^{-1}Y+\frac{1}{2}\sum\ln\mbox{tr}(B)-\sum\ln\mbox{tr}(L)+\sum\ln\mbox{tr}(R) + \ln p(\theta).
\end{align}



%
%\subsection{Particle Learning Testing}
%A state space model is
%\begin{align*}
%y_t&=F_tx_t+\epsilon_t,\\
%x_t&=\phi_tx_{t-1}+w_t,\\
%x_0&\sim N(m_0,c_0),
%\end{align*}
%where $\epsilon_t\sim N(0,\sigma)$ and $w_t\sim N(0,W)$. $x_t$ are hidden status and $y_t$ are observations. Assuming that $V=W=1$, $F_t=1$, the initial value $x_0=0$. $\phi_t$ is the  parameter to be estimated. 
%
%$s_{t}^x$ is the sufficient statistics of the states and $s_{t}$ is the sufficient statistics of the parameter $\theta$ at time $t$. The Particle Learning algorithm runs as follows:
%
%\begin{itemize}
%\item Step 1. Resample $\{\tilde{z}_t^{(i)}\}_{i=1}^N=(\tilde{x}_t^{(i)},\tilde{s}_t^{(i)},\tilde{\theta}^{(i)})$ from $z_t^{(i)}=(x_t,s_t,\theta)^{(i)}$ with weight $w\propto p(y_{t+1}|s_{t}^x,\theta)$. It is found that \begin{equation*}
%p(y_{t+1}|s_{t}^x,\theta) \propto \exp \left(-\frac{1}{2}\frac{(y_{t+1}-\theta x_{t})^2}{\sigma^2} \right).
%\end{equation*}
%
%\item Step 2. Propagate $\tilde{x}_t^{(i)}$ to $x_{t+1}^{(i)}$ via $p(x_{t+1}|\tilde{z}_t^{(i)},y^{t+1})$. 
%\begin{align*}
%p(x_{t+1}|\tilde{z}_t^{(i)},y^{t+1}) &= p(x_{t+1}|s_t^x,\phi,y^{t+1}) \propto p(x_{t+1},y^{t+1}|s_t^x,\phi)\\
%&\propto p(x_{t+1}|s_t^x,\phi)p(y_{t+1}|x_{t+1},s_t^x,\phi) \\
%&=N(x_{t+1}|\phi x_t,1)N(y_{t+1}|x_{t+1},1)\\
%&\sim N(\frac{1}{2}(y_{t+1}+\phi x_t),\frac{1}{\sqrt{2}})
%\end{align*}
%
%\item Step 3. Update sufficient statistics $s_t$.
%\begin{align*}
%s_{t+1,1} &= x_{t+1} \\
%s_{t+1,2} &= x_tx_{t+1}+s_{t,2} = x_ts_{t,1}+s_{t,2} \\
%s_{t+1,3} &= x_{t}^2 + s_{t,3} = s_{t,1}^2 + s_{t,3} .
%\end{align*}
%
%\item Step 4. Sample $\theta$ from $p(\theta | s_t)$.
%\begin{align*}
%p(\theta | x^{t+1},y^{t+1}) & \propto p(x^{t+1},y^{t+1}|\theta)p(\theta)\propto p(x^{t+1}|\theta)p(\theta)\\
%&\sim N\left( \phi | \frac{s_{t+1,2}}{s_{t+1,3}},\frac{1}{s_{t+1,3}} \right).
%\end{align*}
%
%\item Step 5. Update $s_{t+1}^x = x_{t+1}$.
%\end{itemize}
%
%Particle learning is used to estimate one parameter $\phi$.
%\begin{figure}[h]
%\centering
%\includegraphics[width=14cm,height=9cm]{PFforphi}
%\end{figure}


\subsection{High Dimension Parameters Space of OU-Process}

The Brownian motion is used to construct the Ornstein Uhlenbeck (OU) process, which has become a popular tool for modeling interest rates and vehicle moving. The derivative of the Brownian motion $x_t$ does not exist at any point in time. Thus, if $x_t$ represents the position of a particle, we might be interested in
obtaining its velocity, which is the derivative of the motion. The OU process is an alternative model to the Brownian motion that overcomes the preceding problem.
It does this by considering the velocity $u_t$ of a Brownian motion at time $t$. Over a small time interval, two factors affect the change in velocity: the frictional
resistance of the surrounding medium whose effect is proportional to $u_t$ and the random impact of neighboring particles whose effect can be represented
by a standard Wiener process. Thus, because mass times velocity equals force, we have that
\begin{equation*}
mdu_t = -\omega u_tdt+dW_t,
\end{equation*}
where $\omega>0$ is called the friction coefficient and $m>0$ is the mass. If we define $\gamma = \omega /m$ and $\lambda = 1/m$, we obtain the OU process with the following differential
equation:
\begin{equation}
du_t= -\gamma u_tdt+\lambda dW_t.
\end{equation}

The OU process is used to describe the velocity of a particle in a fluid and is encountered in statistical mechanics. It is the model of choice for random movement
toward a concentration point. It is sometimes called a continuous-time Gauss Markov process, where a Gauss Markov process is a stochastic process
that satisfies the requirements for both a Gaussian process and a Markov process. Because a Wiener process is both a Gaussian process and a Markov process, in addition to being a stationary independent increment process, it can be considered a Gauss-Markov process with independent increments \cite{kijima1997markov}.

An OU-process model combing states and velocity is in the form of
\begin{align}
\begin{cases}
du_t = -\gamma u_t dt+ \lambda dW_t,\\
dx_t = u_t dt+\xi dW_t'.
\end{cases}
\end{align}
The solution can be found by integrating $dt$ out, that gives us 
\begin{align}
\begin{cases}
u_t &=u_{t-1}e^{-\gamma t} +\int_{0}^{t} \lambda e^{-\gamma (t-s)}dW_s,\\
x_t &=x_{t-1} +\frac{u_{t-1}}{\gamma}(1- e^{-\gamma t}) + \int_{0}^{t} \frac{\lambda}{\gamma}e^{\gamma  s} \left(1-e^{-\gamma t}\right)dW_s + \int_{0}^{t}\xi dW_s'.
\end{cases}
\end{align}
Therefore,  the joint distribution is 
\begin{align}
\begin{bmatrix} x_t \\ u_t \end{bmatrix} &\sim N\left(
\begin{bmatrix}\mu_t^{(x)} \\ \mu_t^{(u)}  \end{bmatrix} , 
\begin{bmatrix}
\sigma_t^{(x)2} & \rho_t\sigma_t^{(x)} \sigma_t^{(u)} \\
\rho_t\sigma_t^{(x)} \sigma_t^{(u)} & \sigma_t^{(u)2}
\end{bmatrix} \right),
\end{align}
where $\mu_t^{(x)}$ and $\mu_t^{(u)} $ are from the forward map process 
\begin{align}
\begin{bmatrix}\mu_t^{(x)} \\ \mu_t^{(u)}  \end{bmatrix}  = 
\begin{bmatrix}
1 & \frac{1-e^{-\gamma \Delta_t}}{\gamma} \\ 0 &  e^{-\gamma \Delta_t}
\end{bmatrix}  \begin{bmatrix} x_{t-1}^{(x)} \\ u_{t-1}  \end{bmatrix} \triangleq \Phi \begin{bmatrix} x_{t-1}^{(x)} \\ u_{t-1}  \end{bmatrix},
\end{align}
and 
\begin{align*}
\begin{cases}
\sigma_t^{(x)2} &=\frac{\lambda^2 \left(e^{2 \gamma\Delta_t}-1\right) \left(1 -e^{-\gamma\Delta_t}\right)^2}{2 \gamma ^3 } + \xi^2\Delta_t\\
\sigma_t^{(u)2} &= \frac{\lambda ^2 \left(1- e^{-2 \gamma\Delta_t}\right)}{2 \gamma } \\
\rho_t\sigma_t^{(x)}\sigma_t^{(u)} & =\frac{\lambda ^2 \left(e^{\gamma\Delta_t} -1\right) \left(1-e^{-2\gamma\Delta_t}\right)}{2 \gamma ^2}
\end{cases}
\end{align*}
In the above equations, $\Delta_t = T_t-T_{t-1}, \Delta_1=0$, $x_0\sim N(0,L_x^2), u_0\sim N(0,L_u^2)$, $\rho_t^2 = 1-\frac{\xi^2 \Delta_t}{\sigma_t^{(x)^2}}$. To be useful, $1-\rho_t^2 =\frac{\sigma_t^{(x)^2}}{\xi^2 \Delta_t}$. 

Moreover, the independent observation processes are
\begin{align*}\begin{cases} y_t=x_t+\epsilon_t,\\ v_t=u_t+\epsilon'_t, \end{cases} \end{align*}
where $\epsilon_t\sim N(0,\sigma),\epsilon'_t\sim N(0,\tau)$ are normally distributed independent errors. Thus, the joint distribution of observations is 
\begin{align}\label{obmodel}
\begin{bmatrix} y_t \\ v_t \end{bmatrix} &\sim N\left(
\begin{bmatrix}x_t \\ u_t \end{bmatrix} , 
\begin{bmatrix}
\sigma^2 & 0\\
0 & \tau^2
\end{bmatrix} \right).
\end{align}
Consequently, the parameter $\theta$ of an entire Ornstein-Uhlenbeck process is a set of five parameters from both hidden status and observation process, which is represented as $\theta = \{\gamma,\xi^2,\lambda,\sigma^2,\tau^2 \}$. 


Starting from the joint distribution of $x_{0:t},u_{0:t}$ and $y_{1:t},v_{1:t}$ by given $\theta$, it can be found that
\begin{equation}\label{jointmatrix}
\begin{bmatrix} \begin{matrix} \tilde{X}\\ \tilde{Y}  \end{matrix} \biggr\rvert \theta \end{bmatrix}
\sim N\left(0, \tilde{\Sigma} \right),
\end{equation}
where $\tilde{X}$ represents for the hidden statues $\{x,u\}$, $\tilde{Y}$ represents for observed $\{y,v\}$, $\theta$ is the set of five parameters.  The inverse of the covariance matrix $\tilde{\Sigma}^{-1}$ is the procedure matrix in the form of
\begin{align*} \tilde{\Sigma}^{-1}=
\begin{bmatrix}
Q_{xx} & Q_{xu} & -\frac{1}{\sigma^2}I & 0\\
Q_{ux} & Q_{uu} & 0 &-\frac{1}{\tau^2}I \\
-\frac{1}{\sigma^2}I & 0 & \frac{1}{\sigma^2}I  & 0\\
 0  &  -\frac{1}{\tau^2}I  & 0 & \frac{1}{\tau^2}I 
\end{bmatrix}.
\end{align*}
To make the covariance matrix a more beautiful form and convenient computing, $\tilde{X}$, $\tilde{Y}$ and $\tilde{\Sigma}$ can be rearranged in a time series order, that makes $X = \{x_1,u_1,x_2,u_2,\cdots, x_t, u_t \}$, $Y = \{y_1,v_1,y_2,v_2,\cdots, y_t, v_t \}$ and the new procedure matrix $\Sigma^{-1}$ looks like 
\begin{align*} \Sigma^{-1}=
\begin{bmatrix}
\sigma_{11}^{(x)2}+\frac{1}{\sigma^2} & \sigma_{11}^{(xu)2} & \cdots & \sigma_{1t}^{(x)2} & \sigma_{1t}^{(xu)2}  &  -\frac{1}{\sigma^2} & 0 & \cdots & 0 & 0\\
\sigma_{11}^{(ux)2}   & \sigma_{11}^{(u)2} +\frac{1}{\tau^2} & \cdots & \sigma_{1t}^{(ux)2} & \sigma_{1t}^{(x)2}  &  0 & -\frac{1}{\tau^2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots &\ddots & \vdots & \vdots \\
\sigma_{t1}^{(x)2}   & \sigma_{t1}^{(xu)2} & \cdots & \sigma_{tt}^{(x)2} +\frac{1}{\sigma^2}  & \sigma_{tt}^{(xu)2}  &  0 & 0 & \cdots & -\frac{1}{\sigma^2} & 0 \\
\sigma_{t1}^{(ux)2}   & \sigma_{t1}^{(u)2} & \cdots & \sigma_{tt}^{(ux)2} & \sigma_{tt}^{(u)2} +\frac{1}{\tau^2}  &  0 & 0 & \cdots & 0 &-\frac{1}{\tau^2} \\
- \frac{1}{\sigma^2} & 0 & \cdots & 0 & 0 &  \frac{1}{\sigma^2} & 0 & \cdots & 0 & 0\\
0  & -\frac{1}{\tau^2}& \cdots & 0 & 0 &  0 &  \frac{1}{\tau^2} & \cdots & 0 & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \vdots &\ddots & \vdots & \vdots \\
0 & 0& \cdots & -\frac{1}{\sigma^2}  &0&  0 & 0 & \cdots & \frac{1}{\sigma^2} & 0 \\
0 & 0 & \cdots & 0 & -\frac{1}{\tau^2}   &  0 & 0 & \cdots & 0 & \frac{1}{\tau^2}
\end{bmatrix} \triangleq \begin{bmatrix}
A_t& -B_t \\ -B_t^\top & B_t
\end{bmatrix},
\end{align*}
where $B_t$ is a $2t\times 2t$ diagonal matrix of observation errors at time $t$ in the form of $\begin{bmatrix}
\frac{1}{\sigma^2}& \cdot & \cdot &  \cdot  &  \cdot \\  \cdot & \frac{1}{\tau^2} & \cdot &  \cdot  &  \cdot  \\ 
\vdots & \vdots & \ddots & \vdots & \vdots \\
 \cdot  &  \cdot  & \cdot  & \frac{1}{\sigma^2}&  \cdot \\  \cdot  &  \cdot & \cdot  &  \cdot  & \frac{1}{\tau^2}
\end{bmatrix}$. 
In fact, the matrix $A_t$ is a $2t \times 2t$ bandwidth six sparse matrix at time $t$ in the process. Temporally, we are using $A$ and $B$ to represent the matrices  $A_t$ and $B_t$ here. Then we may find the covariance matrix by calculating the inverse of the procedure matrix as 
\begin{align*}
\Sigma &= \begin{bmatrix}
(A-B^\top B^{-1}B) ^{-1} & -(A-B^\top B^{-1}B)^{-1}B^\top B^{-1}\\
- B^{-1}B(A-B^\top B^{-1}B)^{-1} & (B-B^\top A^{-1}B) ^{-1}
\end{bmatrix} \\
&= \begin{bmatrix}
(A-B) ^{-1} & (A-B)^{-1}\\
(A-B)^{-1} & (I- A^{-1}B) ^{-1}B^{-1}
\end{bmatrix} \\
&\triangleq \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY} \\
\Sigma_{YX}  &\Sigma_{YY} 
\end{bmatrix}.
\end{align*}



\subsubsection{The Forecast Distribution $p(Y_{t+1}| Y_{1:t},\theta)$}

We are now using the capital letter $Y$ to represent the joint $\{y,v\}$ and $Y_{1:t} = \{y_1,v_1,y_2,v_2,\cdots, y_t, v_t \}$, $Y_{t+1} = \{y_{t+1}, v_{t+1}\}$. It is known that 
\begin{align*}
p(Y_{1:t},\theta) &\sim N\left( 0,\Sigma_{YY}^{(t)} \right)\\
p(Y_{t+1},Y_{1:t},\theta) &\sim N\left( 0,\Sigma_{YY}^{(t+1)} \right)\\
p(Y_{t+1}\mid Y_{1:t},\theta) &\sim N\left( \bar{\mu}_{t+1},\bar{\Sigma}_{t+1} \right)
\end{align*}
where the covariance matrix of the joint distribution is $\Sigma_{YY}^{(t+1)} = (I_{t+1}-A_{t+1}^{-1}B_{t+1})^{-1}B_{t+1}^{-1}$. Then, by taking its inverse, we will get
\begin{align*}
\Sigma_{YY}^{(t+1) (-1)} = B_{t+1}(I_{t+1}-A_{t+1}^{-1}B_{t+1}).
\end{align*}
To be clear, the matrix $B_t$ is short for the matrix $B_t(\sigma^2,\tau^2)$, which is $2t\times 2t$ diagonal matrix with elements $\frac{1}{\sigma^2},\frac{1}{\tau^2}$ repeating for $t$ times on its diagonal. For instance, the very simple $B_1(\sigma^2,\tau^2) = 
\begin{bmatrix}
\frac{1}{\sigma^2} & 0  \\
0 & \frac{1}{\tau^2}
\end{bmatrix}_{2\times 2}$ is a $2\times 2$ matrix. 

Because of $A$ is symmetric and invertible, $B$ is the diagonal matrix defined as above, then they have the following property 
\begin{align*}
& AB=A^\top B^\top = (BA)^\top, \\
& A^{-1}B = A^{-\top}B^\top = (BA^{-1})^\top. 
\end{align*}
Followed up the form of $\Sigma_{YY}^{(t+1) (-1)}$, we can find out that 
\begin{align*}
\Sigma_{YY}^{(t+1) (-1)} &= B_{t+1}(I_{t+1}-A_{t+1}^{-1}B_{t+1}) \\
&= B_{t+1}(B_{t+1}^{-1}-A_{t+1}^{-1})B_{t+1} \\
&\triangleq \begin{bmatrix} 
B_t & 0 \\ 0 & B_1 \end{bmatrix}
\begin{bmatrix} 
Z_{t+1} & b_{t+1} \\
b_{t+1}^\top & K_{t+1}
\end{bmatrix} \begin{bmatrix} 
B_t & 0 \\ 0 & B_1\end{bmatrix}
\end{align*}
where $Z_{t+1}$ is a $2t \times 2t$ matrix, $ b_{t+1} $ is a $2t \times 2$ matrix and $K_{t+1}$ is a $2 \times 2$ matrix. Thus by taking its inverse again, we will get 
\begin{align*} \Sigma_{YY}^{(t+1)}= \left[ \begin{matrix}
B_t^{-1} (Z_{t+1}-b_{t+1}K_{t+1}^{-1}b_{t+1}^\top)^{-1}B_t^{-1}  & - B_t^{-1}  Z_{t+1}^{-1}b_{t+1}(K_{t+1}-b_{t+1}^\top Z_{t+1}^{-1}b_{t+1})^{-1}B_1^{-1} \\
-B_1^{-1}  K_{t+1}^{-1}b_{t+1}^\top (Z_{t+1}-b_{t+1}K_{t+1}^{-1}b_{t+1}^\top)^{-1}B_t^{-1}  & B_1^{-1}  (K_{t+1}-b_{t+1}^\top Z_{t+1}^{-1}b_{t+1})^{-1}B_1^{-1} 
\end{matrix}\right].
\end{align*}

It is easy to find the relationship between $A_{t+1}$ and  $A_{t}$ in the Sherman-Morrison-Woodbury form is 
\begin{align*} A_{t+1} = 
\begin{bmatrix}
A_t & \cdot & \cdot  \\ \cdot &\frac{1}{\sigma^2} &\cdot  \\ \cdot  & \cdot  & \frac{1}{\tau^2} 
\end{bmatrix} + U_{t+1}U_{t+1}^\top \triangleq M_{t+1}  + U_{t+1}U_{t+1}^\top,
\end{align*}
where $M_{t+1} = \begin{bmatrix}
A_t & \cdot & \cdot  \\ \cdot &\frac{1}{\sigma^2} &\cdot  \\ \cdot  & \cdot  & \frac{1}{\tau^2}
\end{bmatrix}  = \begin{bmatrix}
A_t & 0 \\ 0 & B_1
\end{bmatrix}$ 
and its inverse is $M_{t+1}^{-1} =\begin{bmatrix}
A_t^{-1} & 0 \\ 0 & B_1^{-1}
\end{bmatrix}$. Additionallly, $U$ is a $2t+2 \times 2$ matrix in the following form 
\begin{align*}
U_{t+1} = \frac{1}{\sqrt{ 1-\rho_{t+1}^2} } \begin{bmatrix}
\mathbf{0}_{2t-2} & \mathbf{0}_{2t-2}  \\ \frac{1}{\sigma_{t+1}^{(x)}}& 0 \\
\frac{1-e^{-\gamma \Delta_{t+1}}}{\gamma \sigma_{t+1}^{(x)}}-\frac{\rho_{t+1} e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}} & \frac{\sqrt{1-\rho_{t+1}^2}e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}} \\
-\frac{1}{\sigma_{t+1}^{(x)}} & 0 \\
\frac{\rho_{t+1}}{\sigma_{t+1}^{(u)}} & -\frac{\sqrt{1-\rho_{t+1}^2}}{\sigma_{t+1}^{(u)}}
\end{bmatrix} \triangleq  \begin{bmatrix}
C_t S_{t+1} \\ D_{t+1}
\end{bmatrix},
\end{align*}
denoted by $S_{t+1} = \frac{1}{\sqrt{ 1-\rho_{t+1}^2} } \begin{bmatrix}
\frac{1}{\sigma_{t+1}^{(x)}}& 0 \\
\frac{1-e^{-\gamma \Delta_{t+1}}}{\gamma \sigma_{t+1}^{(x)}}-\frac{\rho_{t+1} e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}} & \frac{\sqrt{1-\rho_{t+1}^2}e^{-\gamma\Delta_{t+1}}}{\sigma_{t+1}^{(u)}}
\end{bmatrix}$, $D_{t+1} =  \frac{1}{\sqrt{ 1-\rho_{t+1}^2} }\begin{bmatrix}
-\frac{1}{\sigma_{t+1}^{(x)}} & 0 \\
\frac{\rho_{t+1}}{\sigma_{t+1}^{(u)}} & -\frac{\sqrt{1-\rho_{t+1}^2}}{\sigma_{t+1}^{(u)}}
\end{bmatrix}$ and $C_{t+1} = \begin{bmatrix} 0 & 0 \\ \vdots & \vdots \\ 0 & 0 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix}
\mathbf{0}_t \\ I_{2} \end{bmatrix}$. 


By post-multiplying $\Sigma_{YY}^{(t+1)(-1)}$ with $C_{t+1}$, it gives us
\begin{align*}
\Sigma_{YY}^{(t+1)(-1)} C_{t+1} &=  B_{t+1} (I_{t+1} -A_{t+1} ^{-1}B_{t+1} ) C_{t+1}  \\ 
&= \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix} \left( \begin{bmatrix} B_t^{-1} & 0 \\ 0 & B_1^{-1} \end{bmatrix}  -A_{t+1}^{-1} \right) \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}   C_{t+1}\\
&= \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}\begin{bmatrix} Z_{t+1} & b_{t+1} \\ b_{t+1}^\top  & K_{t+1} \end{bmatrix}  \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}   C_{t+1}\\
& = \begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix}\begin{bmatrix} b_{t+1}B_1\\ K_{t+1}B_1 \end{bmatrix}.
\end{align*}
and the property of $A_{t+1}^{-1}$ is 
\begin{equation*}
A_{t+1}^{-1}C_{t+1} = \begin{bmatrix}
-b_{t+1} \\ B_1^{-1} - K_{t+1}
\end{bmatrix}.
\end{equation*}
Moreover, by pre-multiplying $C_{t+1}^\top$ on the left side of the above equation, we will have 
\begin{align}\label{OUKtp1}
C_{t+1}^\top A_{t+1}^{-1}C_{t+1}  &= B_1^{-1} -K_{t+1},\\
K_{t+1} &= B_1^{-1} - C_{t+1}^\top A_{t+1}^{-1}C_{t+1}.
\end{align}


We may use Sherman-Morrison-Woodbury formula to find the inverse of $A_{t+1}$ in a recursive way, which is 
\begin{equation}
A_{t+1}^{-1} = (M_{t+1}+U_{t+1}U_{t+1}^\top)^{-1}= M_{t+1}^{-1}-M_{t+1}^{-1}U_{t+1}(I+U_{t+1}^\top M_{t+1}^{-1}U_{t+1})^{-1}U_{t+1}^\top M_{t+1}^{-1}.
\end{equation}
Consequently, it is easy to find that $M_{t+1}^{-1}C_{t+1} =\begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix} $ and 
\begin{align*}
A_{t+1}^{-1}C_{t+1} &= \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} & 0 \\ 0 & B_1^{-1} \end{bmatrix} \begin{bmatrix} C_tS_{t+1} \\ D \end{bmatrix}
(I+U_{t+1}^\top M_{t+1}^{-1}U_{t+1})^{-1}  \begin{bmatrix}
S_{t+1}^\top C_t^\top & D_{t+1}^\top \end{bmatrix} \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  \\ 
% ===
& = \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} C_tS_{t+1} \\B_1^{-1}D_{t+1} \end{bmatrix} 
(I+U_{t+1}^\top M_{t+1}^{-1}U_{t+1})^{-1}  D_{t+1}^\top B_1^{-1} \\ 
% ===
& = \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} C_tS_{t+1} \\B_1^{-1}D_{t+1} \end{bmatrix} 
(I+ S_{t+1}^\top C_t^\top A_t^{-1} C_t S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1} \\ 
% ===
& = \begin{bmatrix} 0 \\ B_1^{-1} \end{bmatrix}  - \begin{bmatrix}
A_t^{-1} C_tS_{t+1} \\B_1^{-1}D_{t+1} \end{bmatrix} 
(I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1}.
\end{align*}
Thus, by using the equation (\ref{OUKtp1}), we will get 
\begin{equation}
K_{t+1} =B_1^{-1}D_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1},
\end{equation}
and
\begin{align*}
b_{t+1} &= A_t^{-1}C_t S_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1} D_{t+1}^\top B_1^{-1} \\
&= \begin{bmatrix}
-b_t \\ B_1^{-1}-K_t 
\end{bmatrix}  S_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1} D_{t+1}^\top B_1^{-1}.
\end{align*}
To achieve the recursive updating formula, firstly we need to find the form of $b_{t+1}^\top B_t^2 Y_{1:t}$. In fact, it is 
\begin{align*}
b_{t+1}^\top B_t Y_{1:t} &= B_1^{-\top}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
\begin{bmatrix}
-b_t^\top  & B_1^{-1}-K_t 
\end{bmatrix} B_t \begin{bmatrix}
Y_{1:t-1} \\ Y_t
\end{bmatrix}\\
&= B_1^{-\top}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
\left(  -b_t^\top  B_{t-1}  Y_{1:t-1} + (B_1^{-1}-K_t )  B_1  Y_t      \right) \\ 
&= B_1^{-\top}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
\left(  K_t B_1\bar{\mu}_t+ (I-K_t B_1)  Y_t      \right), \\
\end{align*}
By using equation (\ref{recursiveKp1}) and simplifying the above equation, one can achieve a recursive updating form of the mean, which is 
\begin{align*}
%\bar{\mu}_{t+1} = -B_1K_{t+1}^{-1} B_1^{-1}D_{t+1}  (I+ S_{t+1}^\top  (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-\top} S_{t+1}^\top 
%\left(  K_t B_1 \bar{\mu}_t+ (I-K_tB_1)Y_t      \right).
\bar{\mu}_{t+1} &= -B_1K_{t+1}^{-1}b_{t+1}^\top B_t Y_{1:t} \\
&= -D_{t+1}^{-\top}S_{t+1}^\top (K_tB_1\bar{\mu}_t + (I-K_tB_1)Y_t) \\ 
&= -D_{t+1}^{-\top}S_{t+1}^\top ( Y_t +   K_tB_1(\bar{\mu}_t-Y_t)),
\end{align*}
where by simplifying $D^{-\top}S^\top$, one may find 
\begin{align*}
D_{t+1}^{-\top}S_{t+1}^\top = \begin{bmatrix}
-1 & -\frac{1-e^{-\gamma \Delta_{t+1}}}{\gamma} \\ 0 & - e^{-\gamma \Delta_{t+1}}
\end{bmatrix} = -\Phi_{t+1},
\end{align*}
which is the negative of forward process. Then the final form of recursive updating formula are 
\begin{align}
\begin{cases}
\bar{\mu}_{t+1}&=\Phi_{t+1} K_tB_1\bar{\mu}_t + \Phi_{t+1} (I-K_tB_1)Y_t\\
\bar{\Sigma}_{t+1}&=\left( B_1K_{t+1}B_1  \right)^{-1}
\end{cases}.
\end{align}
The matrix $K_{t+1}$ is updated via 
\begin{equation}\label{recursiveKp1}
K_{t+1} =B_1^{-1}D_{t+1} (I+ S_{t+1}^\top (B_1^{-1} - K_t)  S_{t+1} +D_{t+1}^\top B_1^{-1}D_{t+1}  )^{-1}  D_{t+1}^\top B_1^{-1},
\end{equation}
or updating its inverse in the following form makes the computation faster, that is 
\begin{align*}
\begin{cases}
K_{t+1}^{-1} &= B_1D_{t+1}^{-\top}D_{t+1}^{-1}B_1 + B_1\Phi_{t+1} (B_1^{-1} - K_t) \Phi_{t+1}^\top B_1+ B_1,\\
\bar{\Sigma}_{t+1}&= D_{t+1}^{-\top}D_{t+1}^{-1}+ \Phi_{t+1} (B_1^{-1} - K_t) \Phi_{t+1}^\top + B_1^{-1}
\end{cases}
\end{align*}
and $K_1 =B_1^{-1} - A_1^{-1} = \begin{bmatrix}
\frac{\sigma^4}{\sigma^2 +L_x^2} & 0 \\ 0 &\frac{\tau^4}{\tau^2 +L_u^2}
\end{bmatrix} $.

\subsubsection{The Estimation Distribution $p(X_{t+1}| Y_{1:t+1},\theta)$}

%
%Because of the joint distribution (\ref{jointmatrix}), one can find the best estimation with a given $\theta$ by
%\begin{align*}
%X \mid Y,\theta &\sim N \left( A^{-1}BY, A^{-1} \right) \\
%&\sim N(L^{-\top}L^{-1}BY,L^{-\top}L^{-1})\\
%&\sim N(L^{-\top}W,L^{-\top}L^{-1}),
%\end{align*}
%thus
%\begin{align*}
%\hat{X} = L^{-\top}(W+Z),
%\end{align*}
%where $Z \sim N(0, I(\sigma,\tau))$.
%
%
%For $x_{t+1}$, the joint distribution with $Y$ updated to stage $t+1$ is 
%\begin{align*}
%x_{t+1}, Y\mid \theta \sim N\left( 0, \begin{bmatrix}
%C_{t+1}^\top(A-B) ^{-1}C_{t+1} & C_{t+1}^\top (A-B)^{-1}\\
%(A-B)^{-1}C_{t+1} & (I- A^{-1}B) ^{-1}B^{-1}
%\end{bmatrix} \right),
%\end{align*}
%where $C_{t+1}^\top = \begin{bmatrix}
%0 & \cdots & 0 & 1 & 0 \\
%0 & \cdots & 0 & 0 & 1
%\end{bmatrix}$ is a $2 \times 2(t+1)$ matrix. Thus
%\begin{align*}
%x_{t+1}\mid Y,\theta \sim N(\bar{\mu}_{t+1}^{(x)},\bar{\Sigma}_{t+1}^{(x)}),
%\end{align*}
%where
%\begin{align*}
%\bar{\mu}_{t+1}^{(x)} & = C_{t+1}^\top A^{-1}BY =C_{t+1}^\top L^{-\top}W,\\
%\bar{\Sigma}_{t+1}^{(x)} & =C_{t+1}^\top A^{-1}C_{t+1} =U_{t+1}^\top U_{t+1},
%\end{align*}
%and $U_{t+1} = L^{-1} C_{t+1} = \mbox{solve}(L,C_{t+1})$.

The filtering distribution of the state given parameters is $p(X_t\mid Y_{1:t}, \theta )$. To find its form, one can use the joint distribution of $X_{t+1}$ and $Y_{1:t+1}$, which is $p(X_{t+1}, Y_{1:t+1}  \mid  \theta)\sim N(0,\Gamma)$, where
\begin{equation*}
\Gamma=\begin{bmatrix} C_{t+1}^\top(A-B)^{-1}C_{t+1} & C_{t+1}^\top(A-B)^{-1}\\(A-B)^{-1}C_{t+1} & (I-A^{-1}B)^{-1}B^{-1} \end{bmatrix}.
\end{equation*}
Because of 
\begin{align*}
C_{t+1}^\top A_{t+1}^{-1} = \left[\begin{matrix} - b_{t+1}^\top & B_1^{-1}- K_{t+1} \end{matrix} \right],
\end{align*}
then $X_{t+1}\mid Y_{1:t+1},\theta \sim N(\bar{\mu}_{t+1}^{(X)},\bar{\sigma}_{t+1}^{(X)2})$, where
\begin{align*}
\bar{\mu}_{t+1}^{(X)}       &= \Phi \hat{x}_{t} +  C_{t+1}^\top (A-B)^{-1}B (I-A^{-1}B)Y_{1:t+1}\\
                      &= \Phi \hat{x}_{t} +  C_{t+1}^\top A^{-1}B Y_{1:t+1} \\ 
                      &=0+ \begin{bmatrix} - b_{t+1}^\top & B_1^{-1}-K_{t+1} \end{bmatrix}\begin{bmatrix} B_t & 0 \\ 0 & B_1 \end{bmatrix} \begin{bmatrix} Y_{1:t} \\ Y_{y+1} \end{bmatrix} \\
                      &=   -b^\top B_t Y_{1:t} + (I - B_1K_{t+1})Y_{t+1} \\
                      & =  K_{t+1}B_1\bar{\mu}_{t+1 } + (I - B_1K_{t+1})Y_{t+1}  \\
\bar{\sigma}_{t+1}^{(X)2}&=C_{t+1}^\top(A-B)^{-1}C_{t+1}-  C_{t+1}^\top(A-B)^{-1}  B(I-A^{-1}B) (A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top(A-B)^{-1}C_{t+1} -  C_{t+1}^\top A^{-1}B(A-B)^{-1}C_{t+1}\\
                      &= C_{t+1}^\top A^{-1}C_{t+1} \\ &=B_1^{-1}-K_{t+1}.
\end{align*}

\subsubsection{Approximations of The Parameters Posterior}

From the joint distribution of $X$ and $Y$, we can find that  
\begin{align*}
\Sigma_{YY}^{-1} &= B(I-A^{-1}B)= BA^{-1}\Sigma_{XX}^{-1}.
\end{align*}
Given the Choleski decomposition $LL^\top = A$, we have
\begin{align*}
\Sigma_{YY}^{-1} &=BL^{-\top}L^{-1}\Sigma_{XX}^{-1}\\
&=(L^{-1}B)^\top(L^{-1}\Sigma_{XX}^{-1})\\
&=\mbox{solve}(L,B)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}).
\end{align*}
More usefully, by given another Choleski decomposition $RR^\top=A-B=\Sigma_{XX}^{-1}$,
\begin{align}\label{sigmayy01}
\begin{split}
Y^\top \Sigma_{YY}^{-1} Y &= \mbox{solve}(L,BY)^\top\mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
&\triangleq W^\top \mbox{solve}(L,\Sigma_{XX}^{-1}Y)\\
\end{split}
\end{align}
\begin{align}\label{sigmayy02}
\begin{split}
\det\Sigma_{YY}^{-1} &= \det B \det L^{-\top}\det L^{-1}\det R\det R^\top\\
&= \det B(\det L^{-1})^2(\det R)^2.
\end{split}
\end{align}

From the objective function, the second term in the integral is
\begin{align*}
p(\theta \mid Y) &\propto p(Y\mid\theta)p(\theta) \propto e^{-\frac{1}{2} Y \Sigma_{YY}^{-1} Y } \sqrt{\det \Sigma_{YY}^{-1}} P(\theta).
\end{align*}
Then by taking natural logarithm on the posterior of $\theta$ and using the useful solutions in equations (\ref{sigmayy01}) and (\ref{sigmayy02}), we will have
\begin{align}\label{logL}
\ln L(\theta) &= -\frac{1}{2}Y^\top\Sigma_{YY}^{-1}Y+\frac{1}{2}\sum\ln\mbox{tr}(B)-\sum\ln\mbox{tr}(L)+\sum\ln\mbox{tr}(R).
\end{align}





\section{Prior Distribution for Variance Parameters}

The well known Hierarchical Linear Model, where the parameters vary at more then one level, was firstly introduced by Lindley and Smith in 1972 and 1973 \cite{lindley1972bayes} \cite{smith1973general}. An extension of these models is   non-linear Hierarchical Model. Hierarchical Model can be used on data with many levels, although 2-level models are the most common ones. The state space model in equations (\ref{statemodel1}) and (\ref{statemodel2}) is one of Hierarchical Linear Model if $G_t$ and $F_t$ are linear and non-linear model if $G_t$ and $F_t$ are non-linear processes. Researchers have made a few discussions and works on these both linear and non-linear models. In this section, we only discuss on the prior for variance parameters in these models. 

Jonathan and Thomas in \cite{stroud2007sequential} have discussed a model, which is slightly different with a Gaussian state-space model in equations (\ref{statemodel1}) and (\ref{statemodel2}) from section one. The two errors $\omega_t$ and $\epsilon_t$ are assumed normally distributed as
\begin{align*}
\omega_t &\sim N(0,\alpha Q),\\
\epsilon_t &\sim N(0,\alpha R),
\end{align*}
where the two matrices $R$ and $Q$ are known and $\alpha$ is an unknown scale factor to be estimated. (Note that a perfect model is obtained by setting $Q= 0$.) Therefore, the density of Gaussian state-space model is
\begin{align*}
p(y_t\mid x_t,\alpha) &= N(F(x_t),\alpha R),\\
p(x_t\mid x_{t-1},\alpha) &= N(G(x_{t-1}),\alpha Q).
\end{align*}
The parameter $\alpha$ is assumed \textit{Inverse Gamma} distribution. 

Various non-informative and weakly-informative prior distributions have been suggested for
scale parameters in hierarchical models. Andrew Gelman gave a discussion on prior distributions for variance parameters in hierarchical models in 2006 \cite{gelman2006prior}. General considerations include using invariance \cite{jeffries1961theory}, maximum entropy \cite{jaynes1983papers} and agreement with classical estimators \cite{box2011bayesian}. 


\subsection{Priors Discussion}

$http://andrewgelman.com/2007/07/18/informative_and/$

Informative and noninformative priors
Posted by Andrew on	18 July 2007, 8:04 am
Neal writes,

As I start your Bayesian stuff, can I ask you the same question I asked Boris a few years ago, namely, as you note, noninf priors simply represent the situation where we know very little and want the data to speak (so in the end not too far from the classical view). Can you point me to any social science (closer to ps is better) where people actually update, so that the prior in a second study is the posterior of the first (whether or not the two studies done by same person or not).

Equivalently – point me to a study which uses non-inf priors. (as more than a toy – i know the piece by gill and his student).

Btw do you know the old piece by Harry Roberts, saying that as a scientist all we can report is the likelihood, and that everyone should put their own prior in and then produce their own posterior. so all articles would just be a computer program which takes as input my prior and produces my posterior given the likelihood surface estimated by the author?

My reply: now I like weakly informative priors. But that’s new since our books. Regarding informative priors in applied research, we can distinguish three categories:

(1) Prior distributions giving numerical information that is crucial to estimation of the model. This would be a traditional informative prior, which might come from a literature review or explicitly from an earlier data analysis.

(2) Prior distributions that are not supplying any controversial information but are strong enough to pull the data away from inappropriate inferences that are consistent with the likelihood. This might be called a weakly informative prior.

(3) Prior distributions that are uniform, or nearly so, and basically allow the information from the likelihood to be interpreted probabilistically. These are noninformative priors, or maybe, in some cases, weakly informative.

I have examples of (1), (2), and (3) in my own applied research. Category (3) is the most common for me, but an example of (2) is my 1990 paper with King on seats-votes curves, where we fit a mixture model and used an informative prior to constrain the locations, scales, and masses of the three components. An example of (3) is my 1996 paper with Bois and Jiang where we used an informative prior distribution for several parameters in a toxicology model. We were careful to parameterize the model so that these priors made sense, and the model also had an interesting two-level structure which we discuss in that paper and also in Section 9.1 of Bayesian Data Analysis.

Regarding your question about models where people actually update: we did this in our radon analysis (see here) where the posterior distribution from a national data analysis (based on data from over 80,000 houses) gives inference for each county in the U.S., which is in turn used as the prior distribution for the radon level in your house, which in turn can be updated if you have information from a measurement in your house.

One of the convenient things about doing applied statistics is that eventually I can come up with an example for everything from my own experience. (This also makes it fun to write books.)

Regarding your last comment: yes, there is an idea that a Bayesian wants everyone else to be non-Bayesian so that he or she can do cleaner analyses. I discuss that idea in this talk from 2003 which I’ve been too lazy to write up as a paper.

\subsection{Discussion two}
$http://andrewgelman.com/2007/05/11/weakly_informat/$

Weakly informative priors
Posted by Andrew on	11 May 2007, 1:01 pm

Bayesians traditionally consider prior distributions that (a) represent the actual state of subject-matter knowledge, or (b) are completely or essentially noninformative. We consider an alternative strategy: choosing priors that convey some generally useful information but clearly less than we actually have for the particular problem under study. We give some examples, including the Cauchy (0, 2.5) prior distribution for logistic regression coefficients, and then briefly discuss the major unsolved problem in Bayesian inference: the construction of models that are structured enough to learn from data but weak enough to learn from data.

I’m speaking Monday on this at Jun Liu’s workshop on Monte Carlo methods at Harvard (my talk is 9:45-10:30am Monday at 104 Harvard Hall).

Here’s the presentation. I think this is potentially a huge advance in how we think about Bayesian models.


