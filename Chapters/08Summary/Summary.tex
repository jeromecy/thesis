
Inference and characterization of planar trajectories have been confusing researchers for decades. Precise and efficient algorithms are highly demanded in all kinds of applications. In this thesis, an off-line method V-spline is proposed to reconstruct the whole trajectory and an on-line adaptive MCMC algorithm is used to update and track unknown state and parameter instantly. 

In Chapter \ref{ChapterTS}, the proposed V-spline is built up by new basis functions consisting of Hermite spline. For $n$ paired time series data $\left\lbrace t_i,y_i,v_i\right\rbrace_{i=1}^{n}$, the amount of basis functions is $2n$. In the new objective function \eqref{tractorsplineObjective}, V-spline incorporates both location and velocity information but penalizes excessive accelerations. It is not only minimizing the squared residuals of $\lvert y_i-f(t_i)\rvert^2$ but also reducing the squared residuals of $\lvert v_i-f'(t_i)\rvert^2$, for $i=1,\ldots,n$, with a new parameter $\gamma$. 

In the objective function of a conventional smoothing spline, the penalty parameter $\lambda$ is one single constant that controls the trade-off between interpolations ($\lambda\to 0$) and a straight line ($\lambda\to \infty$). Instead, the penalty parameter $\lambda(t)$ of a V-spline is a function, which is varying at different intervals. In fact, one can consider it a piecewise constant $\lambda(t_i)=\lambda_i$ on interval $[t_i,t_{i+1})$ for $i=1,\ldots,n-1$. Hence, in the objective function of V-spline, there are overall $n$ parameters, including $n-1$ $\lambda$s and $\gamma$, to be estimated. Additionally, to handle unexpected curvatures in the reconstruction, an adjusted penalty term $\frac{(\Delta t_i)^3}{(\Delta d_i)^2}$ is used to adapt to more complicated curvature status. The idea behind this term is either velocity and acceleration goes to zero, the penalty $\lambda$ should be large enough to enforce a straight line. 

It is proved that with improper priors, smoothing splines are corresponding to Bayes estimates, particularly, can be interpreted by Gaussian process regression in a reproducing kernel Hilbert space. This interesting property can be used in more flexible and general applications. Similarly, if the V-spline is equipped with an appropriate inner product \eqref{TractorSplineInnerProduct} and keep $\lambda$ constant, it is corresponding to the posterior mean of the Bayes estimates in a particular reproducing kernel Hilbert space $\mathcal{C}_{\mbox{\scriptsize p.w.}}^{(2)}[0,1]$, in which the second derivatives are piecewise continuous. This result is discussed in Chapter \ref{ChapterGPR}. Recall the property of V-splines that only if $\lambda(t)$ is constant and $\gamma=0$ would the second derivatives be continuous in the entire interval. Otherwise, the second derivatives may not be continuous at the knots but are linear in each interval. That is piecewise continuous. Therefore, this kind of V-splines is named trivial V-splines. By contrast, a non-trivial V-spline has a penalty function $\lambda(t)$. 

Furthermore, generic smoothing splines are always involving independent errors. Extended research is considering using correlated errors or correlated observations, that are more common in real life applications. In Chapter \ref{ChapterGPR}, it is proved by numerical simulation that the V-spline and its Bayes estimate return the same results even if the errors in observed $y$ and $v$ are correlated.  

To find the best parameters, an extended leave-one-out cross-validation technique is proposed to find all the smoothing parameters of interest in Chapter \ref{ChapterTS}. This method uses  observed data itself to tune the parameters at the optimal level. Accordingly, V-spline is a data-driven nonparametric regression solution to handle paired time series data consisting position and velocity information. However, for data with correlated errors, the generalized cross-validation algorithm is more effective. Being modified a generic GCV, in Chapter \ref{ChapterGPR}, an extended GCV is used for finding the optimal parameters for V-spline and its Bayes estimate containing correlated errors. Suppose the solution of a V-spline and its first derivative are in the form of $f=S(\lambda,\gamma)y+\gamma T(\lambda,\gamma)v$ and $f'=U(\lambda,\gamma)y+\gamma V(\lambda,\gamma)v$, the GCV is calculating the trace of matrices $S$, $T$, $U$ and $V$. It is much faster than calculating the sum of the single element in each matrix in LOOCV. 

Simulation studies are given to compare the performances of V-spline and other methods, such as Wavelet algorithms and penalized B-spline, in Chapter \ref{ChapterTS}. It is obvious that all algorithms are very competitive on reconstructing trajectories with respect to mean squared errors, but only the proposed V-spline returns the least true mean squared errors. That means the V-spline performs better and closer to the true singles. At the end of that chapter, a real data example are presented to demonstrate the effectiveness of V-spline. Being applied to a real GPS dataset, the parameter $\lambda$ is classified by $\lambda_u$ and $\lambda_d$ representing for two operating status of a mechanic boom. $\lambda_u$ is a set of $\left\lbrace\lambda_i\right\rbrace$ in the intervals where the boom is not operating and, by contrast, $\lambda_d$ is a set of $\left\lbrace\lambda_i\right\rbrace$ in the intervals where the boom is operating. The reconstruction from V-spline can be treated as the real trajectory of a moving vehicle with confidence. 

Without loss of generality, $\lambda(t)$ can be classified into more groups to adapt to complex maneuver system and V-spline is flexible to be applied on higher dimensional cases. 

However, subject to the property that smoothing splines require the solution of a global problem that involves the entire set of points to be interpolated, it might not be suitable for on-line estimation. Hence, Chapter \ref{ChapterFR} is the content of an overview of existing filtering and estimation algorithms. Some popular algorithms, such as Particle filter, are concentrating on inferring the unknown state but assuming the parameters are known. Moreover, the sample impoverishment has never been solved properly. Liu and West's filter is trying to kill particle degeneracy by incorporating with a shrinkage kernel. Meanwhile, it estimates the unknown parameters simultaneously. Storvik filter and Particle learning algorithms are marginalizing out the parameters through sufficient statistics and achieving a better estimation than Liu and West's filter. In some way, they are advanced algorithms but not practicable any time. In most situations, sufficient statistics are not available or hard to find. A more flexible and easy-implement method is in demand. 
 
As a result, an adaptive sequential MCMC algorithm is proposed in Chapter \ref{ChapterMCMC}. Similarly, with V-spline, the adaptive sequential MCMC is dealing with paired time series dataset including both position and velocity information. 

In the case of a linear state-space model and starting with a joint distribution over state $x$, observation $y$ and parameter $\theta$, an MCMC sampler is implemented with two phases. In the learning phase, a self-tuning sampler utilizes one-variable-at-a-time random walk Metropolis-Hastings (MH) algorithm to learn the parameter mean and covariance structure by aiming at a target acceptance rate. After exploring the parameter space, the information is used in the subsequent phase --- the estimation phase --- to inform the proposed mechanism and is also used in a delayed-acceptance algorithm. 

Suppose the mean and covariance matrix of $\theta$ are $m$ and $C=L^\top L$ respectively, where $L$ is its Cholesky decomposition. Then the proposal $\theta^*=\theta + \epsilon LZ$ and $Z\sim N(0,I)$, $\epsilon$ is the step size. The effect of $L$ is to reduce the correlation of proposals and move to the next step on purpose. The step size $\epsilon$ makes the proposal process more efficient. Rather than focusing on the criteria of efficiency (Eff) and effective sample size (ESS), the Eff in unit time (EffUT) and ESS in unit time (ESSUT) should be the new criteria to determine the optimal step size. By running the same amount of time, the optimal step size found by EffUT and ESSUT generates more effective and representative samples. 

Further, the delayed-acceptance algorithm uses a cheap surrogate $p(\theta\mid m, C)$ to the true posterior $p(\theta\mid y)$ in the first line of defense to keep not-good samples outside. Only good proposals would pass the first line and move forward to the additional expensive calculation. This strategy greatly improves sampling efficiency. 

Information on the resulting state of the system is indicated by a Gaussian mixture. For each sample of $\theta^{(i)}$, it matches an $x \sim N\left(\mu^{(i)},\sigma^{(i)}\right)$. Consequently, the final estimation of $x$ is given by a set of the Gaussian mixture. 

In the on-line mode, the algorithm is adaptive and uses a sliding window approach by cutting off historical data to accelerate sampling speed and to maintain appropriate acceptance. In a simple one parameter simulation in Chapter \ref{ChapterFR}, the proposed adaptive MCMC shows a stable feature comparing with other filtering methods. And in Chapter \ref{ChapterMCMC}, this algorithm shows an advantage in estimating irregularly sampled time series data. 

At the end of Chapter \ref{ChapterMCMC}, the proposed algorithm is applied to combined state and parameter estimation in the case of irregularly sampled GPS time series data. Suppose it is a four-dimensional linear Ornstein-Uhlenbeck (OU) process containing $z=\left\lbrace x,u,y,v\right\rbrace$ with respects to $\left\lbrace x,u\right\rbrace$ on $X$-axis and $\left\lbrace y,v\right\rbrace$ on $Y$-axis, the parameter space is in 5 dimensions, that is $\theta=\left\lbrace\gamma,\xi^2,\lambda^2,\sigma^2,\tau^2\right\rbrace$. The proposed algorithm efficiently infers the state $z_t$ through time $t$ within acceptable errors. Not limited, it also predicts the upcoming states simultaneously. 

As a conclusion, the proposed algorithms in this thesis are contributing to related areas, nevertheless, are not perfect. The V-spline may not be appropriate for on-line estimation and the sliding window adaptive MCMC algorithm is not using the entire data set that might lose some information. Future work and deeper research are required to improve their performances. 
