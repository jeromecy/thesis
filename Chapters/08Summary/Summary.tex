
Inference and characterization of planar trajectories have long been the focus of scientific and commercial research. Efficient algorithms for both precise and efficient trajectory reconstruction remain in high demand in a wide variety of applications. In this thesis, an off-line method named V-spline is proposed to reconstruct the whole trajectory and an on-line adaptive MCMC algorithm is used to update and track unknown state and parameter instantly. 

In Chapter \ref{ChapterTS}, the proposed V-spline is built up by new basis functions consisting of Hermite splines. For $n$ paired time series data $\left\lbrace t_i,y_i,v_i\right\rbrace_{i=1}^{n}$, the amount of basis functions is $2n$. In the new objective function \eqref{tractorsplineObjective}, V-spline incorporates both location and velocity information but penalizes excessive accelerations. It is not only minimizing the squared residuals of $\lvert y_i-f(t_i)\rvert^2$ but also reducing the squared residuals of $\lvert v_i-f'(t_i)\rvert^2$, for $i=1,\ldots,n$, with a new parameter $\gamma$. 

In the objective function of a conventional smoothing spline, the penalty parameter $\lambda$ is a constant number that controls the trade-off between interpolations ($\lambda\to 0$) and a straight line ($\lambda\to \infty$). Instead, the penalty parameter $\lambda(t)$ of a V-spline is a piecewise constant function, which is varying on each interval $[t_i,t_{i+1})$, $i=1,\ldots,n-1$. Hence, in the objective function of  V-splines, there are overall $n$ parameters, including $n-1$ $\lambda$s and $\gamma$, to be estimated. Additionally, to handle unexpected curvatures in the reconstruction, an adjusted penalty term $\frac{(\Delta t_i)^3}{(\Delta d_i)^2}$ adapts to more complicated curvature status. The idea behind this term is that either velocity and acceleration goes to zero, the penalty value $\lambda$ should be large enough to enforce a straight line. 

It is proved that, with improper priors, smoothing splines are corresponding to Bayes estimates. In particular, smoothing splines can be interpreted by Gaussian process regression in a certain reproducing kernel Hilbert space. This property extends smoothing splines to more flexible and general applications. Similarly, if a V-spline is equipped with an appropriate inner product \eqref{TractorSplineInnerProduct} and let $\lambda$ stay constant, it is corresponding to the posterior mean of the Bayes estimates in the reproducing kernel Hilbert space $\mathcal{C}_{\mbox{\scriptsize p.w.}}^{(2)}[0,1]$, in which the functions have piecewise continuous second derivatives. This result is discussed in Chapter \ref{ChapterGPR}. Recall the property of V-splines that if and only if $\lambda(t)$ is constant and $\gamma=0$ would the second derivatives be continuous on the entire interval. Otherwise, the second derivatives may not be continuous at the knots but are linear in each interval. 
%That is piecewise continuous. Therefore, this kind of V-splines is named trivial V-splines. By contrast, a non-trivial V-spline has a penalty function $\lambda(t)$. 

Furthermore, generic smoothing splines consider the errors are independent. Extended research considers correlated errors or correlated observations, that are more common in real life applications. In Chapter \ref{ChapterGPR}, it is proved by numerical simulation that V-spline and its Bayes estimate return the same results even if the errors in measurement are correlated. 

To find the best parameters, an extended leave-one-out cross-validation technique is proposed in Chapter \ref{ChapterTS} to find the smoothing parameters of interest. This method uses observations to tune the parameters to the optimal level. Accordingly, V-spline is a data-driven nonparametric regression solution to handle paired time series data consisting of position and velocity information. However, for data with correlated errors, the generalized cross-validation algorithm is more effective. Being modified a generic GCV, in Chapter \ref{ChapterGPR}, an extended GCV is used for finding the optimal parameters for V-spline and its Bayes estimate containing correlated errors. Suppose the solution of a V-spline and its first derivative are in the form of $f=S(\lambda,\gamma)y+\gamma T(\lambda,\gamma)v$ and $f'=U(\lambda,\gamma)y+\gamma V(\lambda,\gamma)v$, the GCV calculates the trace of matrices $S$, $T$, $U$ and $V$. It is much faster than calculating the sum of the single element in each matrix in LOOCV. 

Simulation studies are given to compare the performances of V-spline and other methods, such as Wavelet algorithms and penalized B-spline, in Chapter \ref{ChapterTS}. It is obvious that these algorithms are competitive on reconstructing trajectories. By contrast, only the proposed V-spline returns the least true mean squared errors. At the end of the chapter, a numeric simulation is presented to demonstrate the effectiveness of V-spline. Being applied to a real GPS dataset, the parameter $\lambda$ is classified by $\lambda_u$ and $\lambda_d$ representing for two operating status of a mechanic boom. $\lambda_u$ is a set of $\left\lbrace\lambda_i\right\rbrace$ in the intervals where the boom is not operating and, by contrast, $\lambda_d$ is a set of $\left\lbrace\lambda_i\right\rbrace$ in the intervals where the boom is operating. The reconstruction from V-spline can be treated as the real trajectory of a moving vehicle with confidence. 

Without loss of generality, $\lambda(t)$ can be classified into more groups to adapt to complex maneuver system and V-spline is flexible to be applied on higher dimensional cases. 

However, subject to the property that smoothing splines require the solution of a global problem that involves the entire set of points to be interpolated, it might not be suitable for on-line estimation. Hence, Chapter \ref{ChapterFR} give an overview of existing filtering and estimation algorithms. Some popular algorithms, such as Particle filter, are concentrating on inferring the unknown state but assuming the parameters known. Moreover, the sample impoverishment has never been solved properly. Liu and West's filter tries to kill particle degeneracy by incorporating with a shrinkage kernel and estimates the unknown parameters simultaneously. Storvik filter and Particle learning algorithms marginalize out the parameters through sufficient statistics to obtain a better outcome. In some way, they are advanced algorithms but not practicable at any time. In most circumstances, sufficient statistics are not available or hard to find. More flexible and easy-implement methods are in demand. 
 
An adaptive sequential MCMC algorithm is proposed in Chapter \ref{ChapterMCMC}. The adaptive sequential MCMC is dealing with paired time series dataset including both position and velocity information. 

In the case of a linear state-space model and starting with a joint distribution over state $x$, observation $y$ and parameter $\theta$, an MCMC sampler is implemented in two phases. In the learning phase, a self-tuning sampler utilizes one-variable-at-a-time random walk Metropolis-Hastings algorithm to learn the mean and covariance structure of the parameter space with aiming at a target acceptance rate for each parameter. After exploring the parameter space, the information is used in the subsequent phase --- the estimation phase --- to inform the proposed mechanism and is also used in a delayed-acceptance algorithm. 

Suppose the mean and covariance matrix of $\theta$ are $m$ and $C=L^\top L$ respectively, where $L$ is the Cholesky decomposition. The proposal $\theta^*=\theta + \epsilon LZ$, where $Z\sim N(0,I)$ and $\epsilon$ is the step size. The effect of $L$ is to reduce the correlation of proposals and move to the next step on purpose. The step size $\epsilon$ makes the proposal process more efficient. Rather than focusing on the criteria of efficiency (Eff) and effective sample size (ESS), the Eff in unit time (EffUT) and ESS in unit time (ESSUT) are the new criteria to determine the optimal step size. By running the same amount of time, the optimal step size found by EffUT and ESSUT helps sampler in generating more effective and representative samples. 

Further, the delayed-acceptance algorithm uses a cheap surrogate $p(\theta\mid m, C)$ to the true posterior $p(\theta\mid y)$ in the first line of defense to keep not-good samples outside. Only good proposals would pass the first line and move forward to the additional expensive calculation. This strategy greatly improves sampling efficiency. 

Information on the resulting state of the system is indicated by a Gaussian mixture. For each sample of $\theta^{(i)}$, it matches an $x^{(i)} \sim N\left(\mu^{(i)},\sigma^{(i)}\right)$. Consequently, the final estimation of $x$ is given by a set of Gaussian mixture. 

In the on-line mode, the algorithm is adaptive and uses a sliding window approach by cutting off historical data to accelerate sampling speed and to maintain appropriate acceptance rates. In a simple one parameter simulation in Chapter \ref{ChapterFR}, the proposed adaptive MCMC shows a stable feature comparing with other filters. In Chapter \ref{ChapterMCMC}, this algorithm shows an advantage in estimating irregularly sampled time series data. 

At the end of Chapter \ref{ChapterMCMC}, the proposed algorithm is applied to combined state and parameter estimation in the case of irregularly sampled time series GPS data. Suppose that the model is a four-dimensional linear Ornstein-Uhlenbeck (OU) process containing $z=\left\lbrace x,u,y,v\right\rbrace$ with respects to $\left\lbrace x,u\right\rbrace$ on $X$-axis and $\left\lbrace y,v\right\rbrace$ on $Y$-axis, the parameter space has 5 dimensions, which is  $\theta=\left\lbrace\gamma,\xi^2,\lambda^2,\sigma^2,\tau^2\right\rbrace$. The proposed algorithm efficiently infers the state $z_t$ along with time $t$ within acceptable errors. 

As a conclusion, the proposed algorithms in this thesis contribute to related areas, nevertheless, are not perfect. V-spline may not be appropriate for on-line estimation and the sliding window adaptive MCMC algorithm dose not use the entire data set that might lose some information. Future work and deeper research are required to improve their performances. 
