\relax 
\citation{agrawal2015introduction}
\citation{kaplan2005understanding,bajaj2002gps}
\citation{hightower2001location}
\citation{chadil2008real}
\citation{mcdonald2006intelligent}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterIntro}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{1}}
\citation{eubank2004simple}
\citation{durbin2012time}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The Problem}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of GPS data. Observed positions $y_t$ are shown. In trajectory reconstruction, the $y_t$ are combined with velocity information $v_t$ and operating characteristics $b_t$ to infer actual positions $x_s$, for times of interest $s$.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{realGPSdataset}{{1.1}{2}}
\newlabel{tracoverview}{{1.1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Smoothing Spline Based Reconstruction}{2}}
\citation{esl2009}
\citation{komoriya1989trajectory,ben2004geometric}
\citation{de1978practical,cox1982practical}
\citation{dierckx1995curve,eilers1996flexible}
\citation{gasparetto2007new}
\citation{wolberg1988cubic}
\citation{green1993nonparametric}
\citation{kim2004smoothing}
\citation{yao2005functional}
\citation{craven1978smoothing}
\citation{aydin2012smoothing}
\citation{esl2009}
\citation{schwarz2012geodesy}
\citation{green1993nonparametric}
\citation{gu1998model}
\citation{craven1978smoothing}
\citation{wahba1985comparison}
\citation{hurvich1998smoothing}
\citation{wand1997exact}
\citation{craven1978smoothing,hardle1988far,hardle1990applied,wahba1990spline,green1993nonparametric,cantoni2001resistant,aydin2013smoothing}
\newlabel{introSmoothingOb}{{1.2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Parameter Selection}{4}}
\citation{larson1931shrinkage}
\citation{arlot2010survey}
\citation{wahba1975completely}
\citation{esl2009}
\newlabel{chapOneCVform}{{1.3}{5}}
\citation{esl2009}
\citation{donoho1995wavelet}
\citation{liu2010data}
\citation{gasparetto2007new}
\newlabel{kfoldCV01}{{5}{6}}
\newlabel{kfoldCV02}{{8}{6}}
\newlabel{kfoldCV03}{{9}{6}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1.1}{\ignorespaces $k$-fold Cross-Validation\relax }}{6}}
\newlabel{kfoldCV}{{1.1}{6}}
\citation{biagiotti2013online}
\citation{anderson1979optimal}
\citation{chen2003bayesian,sarkka2013bayesian}
\citation{kalman1960new}
\citation{bishop2001introduction}
\citation{haykin2001kalman}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Bayesian Filtering}{7}}
\citation{chen2003bayesian,rhodes1971tutorial,kailath1981lectures,sorenson1985kalman}
\citation{tusell2011kalman}
\citation{haykin2001kalman,guzzi2016data}
\citation{gelb1974applied,bar1993estimation}
\citation{wan2000unscented}
\newlabel{introKFmodel}{{1.6}{8}}
\newlabel{KalmanEstimation}{{1.8}{8}}
\citation{wan2000unscented,gyorgy2014unscented}
\citation{chandrasekar2007comparison,laviola2003comparison,st2004comparison}
\citation{ran2010self}
\citation{oussalah2001adaptive}
\citation{liu2014filtering}
\citation{jauch2017recursive}
\citation{chen2003bayesian}
\citation{kloek1978bayesian}
\citation{chen2003bayesian}
\citation{kalos2008monte}
\citation{chen2003bayesian}
\citation{green1995reversible,berzuini1997dynamic}
\citation{rubin2004multiple,tanner1987calculation}
\citation{doucet2009tutorial}
\citation{kong1994sequential}
\citation{carpenter1999improved,godsill2001maximum,stavropoulos2001improved,smcmip2011}
\citation{chen2003bayesian,doucet2000sequential,chen2012monte,doucet2000rao}
\citation{ristic2004beyond}
\citation{godsill2000methodology}
\citation{carlin1992monte}
\citation{kokkala2016particle}
\citation{andrieu1999sequential,green1995reversible,andrieu2001model}
\citation{smith1993bayesian}
\citation{cappe2009inference,liu2008monte}
\citation{smith1993bayesian,tierney1994markov,gilks1995markov}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Markov Chain Monte Carlo Methods}{12}}
\citation{tierney1999some,liu2000multiple,green1995reversible}
\citation{mahendran2012adaptive}
\citation{andrieu2008tutorial,girolami2011riemann,atchade2009adaptive,roberts2009examples}
\citation{haario2001adaptive}
\citation{mahendran2012adaptive}
\newlabel{IntroAccp}{{1.15}{13}}
\citation{roberts2009examples}
\citation{duane1987hybrid}
\citation{neal2011mcmc}
\citation{betancourt2017conceptual}
\citation{bierkens2016piecewise}
\citation{turitsyn2011irreversible,bierkens2017limit}
\citation{bierkens2017limit}
\citation{christen2010general}
\citation{blaauw2011flexible}
\citation{christen2010general}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Original Contributions and Thesis Outline}{15}}
\citation{gu2013smoothing}
\citation{agarwal2003indexing}
\citation{gloderer2010spline}
\citation{magid2006spline}
\citation{dubins1957curves}
\citation{yang2010analytical}
\citation{wang2017curvature}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Adaptive Smoothing V-Spline for Trajectory Reconstruction}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterTS}{{2}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{19}}
\citation{magid2006spline}
\citation{yu2004curve}
\citation{komoriya1989trajectory,ben2004geometric}
\citation{gasparetto2007new}
\citation{erkorkmaz2001high}
\citation{yang2010analytical}
\citation{yao2005functional}
\citation{craven1978smoothing}
\citation{aydin2012smoothing}
\citation{green1993nonparametric}
\citation{schwarz2012geodesy}
\citation{green1993nonparametric}
\citation{sealfon2005smoothing}
\newlabel{smoothingob}{{2.1}{20}}
\citation{zhang2013cubic}
\citation{castro2006geometric}
\citation{silverman1985some,donoho1995wavelet}
\citation{gu1998model}
\citation{craven1978smoothing}
\citation{wahba1985comparison}
\citation{liu2010data}
\citation{donoho1994ideal}
\newlabel{objective}{{2.3}{21}}
\citation{silverman1985some}
\citation{donoho1995wavelet}
\citation{Hermite1863Remarque}
\citation{hintzen2010improved}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}V-Spline}{22}}
\newlabel{SectionTractorSpline}{{2.2}{22}}
\newlabel{tractorsplineObjective}{{2.4}{22}}
\newlabel{TractorSplineTheorem}{{1}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Constructing Basis Functions}{22}}
\newlabel{hermitebasis1}{{2.6}{23}}
\newlabel{cubicHermitesplineform}{{2.10}{23}}
\citation{esl2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Computing V-Spline}{24}}
\newlabel{tractormse}{{2.16}{24}}
\newlabel{thetahat}{{2.21}{24}}
\citation{esl2009}
\newlabel{flinearST}{{2.22}{25}}
\newlabel{flinearUV}{{2.23}{25}}
\newlabel{fhy}{{2.24}{25}}
\newlabel{TractorsplineCorollary}{{1}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Adjusted Penalty Term and Parameter Function}{26}}
\newlabel{VSplinePenaltyLambda}{{2.28}{26}}
\newlabel{APTintroequation}{{2.29}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Comparing reconstructions of cubic Hermite spline and straight line. On the left side, a genuine cubic Hermite spline is cooperating with noisy velocities. Even though the vehicle is not moving, the reconstruction is following the directions of $P_1$ and $P_2$ and gives a wiggle between the two points. On the right side, it is an expected reconstruction between two not-moving points after a long time gap. \relax }}{27}}
\newlabel{figureAPT}{{2.1}{27}}
\citation{green1993nonparametric}
\citation{green1993nonparametric}
\newlabel{adjustedpenalty}{{2.30}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Parameter Selection and Cross-Validation}{28}}
\newlabel{originalcv}{{2.31}{28}}
\newlabel{cvform}{{2.32}{28}}
\newlabel{crossvalidationmatrixA}{{2.33}{28}}
\citation{green1993nonparametric}
\newlabel{cvlema}{{1}{29}}
\newlabel{tractorsplinecvscore}{{2}{29}}
\newlabel{tractorcv}{{2.37}{29}}
\newlabel{cvlemma}{{2}{29}}
\citation{donoho1994ideal,donoho1995adapting,abramovich1998wavelet}
\citation{nason2010wavelet}
\citation{donoho1995adapting,abramovich1998wavelet}
\citation{krivobokova2008fast,ruppert2003semiparametric}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Simulation Study}{30}}
\newlabel{generateVelocity}{{2.42}{30}}
\newlabel{tractorsplinegeneratefunctions}{{2.43}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Regularly Sampled Time Series Data}{30}}
\citation{nelder1965simplex}
\newlabel{ofgamma0}{{2.44}{31}}
\newlabel{thetahat0}{{2.45}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Numerical Examples}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Numerical example: $\textit  {Blocks}$. Comparison of different reconstruction methods with simulated data.\relax }}{32}}
\newlabel{num1}{{2.2}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Numerical example: $\textit  {Bumps}$. Comparison of different reconstruction methods with simulated data.\relax }}{33}}
\newlabel{num2}{{2.3}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Numerical example: $\textit  {HeaviSine}$. Comparison of different reconstruction methods with simulated data.\relax }}{34}}
\newlabel{num3}{{2.4}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Numerical example: $\textit  {Doppler}$. Comparison of different reconstruction methods with simulated data\relax }}{35}}
\newlabel{num4}{{2.5}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Distribution of the penalty values $\lambda (t,\eta )$ in V-spline. Figures on the left side indicate the values varying in intervals. On the right side, these values are projected into reconstructions. The bigger the blacks dots present, the larger the penalty values are.\relax }}{36}}
\newlabel{numpenalty}{{2.6}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Estimated velocity functions by V-spline. The velocity is generated from the original simulation functions by equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 2.42\hbox {}\unskip \@@italiccorr )}}\relax }}{37}}
\newlabel{numvtractor}{{2.7}{37}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation}{37}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces MSE. Mean squared errors of different methods. The numbers in bold indicate the least error among these methods under the same level. The difference is not significant.\relax }}{38}}
\newlabel{mse3200}{{2.1}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Residual Analysis}{38}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces TMSE. True mean squared errors of different methods. The numbers in bold indicate the least error among these methods under the same level. The proposed V-spline returns the smallest TMSE among all the methods under the same level except for $\textit  {Doppler}$ with SNR=7. The differences are significant. \relax }}{39}}
\newlabel{tmse3200}{{2.2}{39}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Retrieved SNR. V-spline effectively retrieves the SNR, which is calculated by $\sigma _{\mathaccentV {hat}05E{f}} / \sigma _{(\mathaccentV {hat}05E{f}-y)}$. \relax }}{39}}
\newlabel{tablecompareSNR}{{2.3}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Irregularly Sampled Time Series Data}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Histogram of $\Delta T$ for irregularly sampled data\relax }}{40}}
\newlabel{gghistIrregularTime}{{2.8}{40}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Retrieved SNR of reconstructions from regularly and irregularly sampled data \relax }}{41}}
\newlabel{tablecompareSNRIreReg}{{2.4}{41}}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces MSE and TMSE of reconstructions from regularly and irregularly sampled data \relax }}{41}}
\newlabel{tablecompareSMEIreReg}{{2.5}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Comparison of regularly and irregularly sampled data\relax }}{42}}
\newlabel{irregularFigure}{{2.9}{42}}
\citation{zhang2013cubic}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Inference of Tractor Trajectories}{43}}
\newlabel{splineapplication}{{2.5}{43}}
\newlabel{tractorsplineObjective2D}{{2.48}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}1-Dimensional Trajectory Reconstruction}{44}}
\newlabel{penaltylamb}{{2.51}{44}}
\newlabel{gg512Points}{{2.10a}{45}}
\newlabel{sub@gg512Points}{{a}{45}}
\newlabel{gg512Path}{{2.10b}{45}}
\newlabel{sub@gg512Path}{{b}{45}}
\newlabel{gg512PointsX}{{2.10c}{45}}
\newlabel{sub@gg512PointsX}{{c}{45}}
\newlabel{gg512PointsY}{{2.10d}{45}}
\newlabel{sub@gg512PointsY}{{d}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Original data points. Figure 2.10a\hbox {} is the original positions recorded by GPS units. Circle points means the boom is not operating; cross points means it is operating. Figure 2.10b\hbox {} is the line-based trajectory by simply connecting all points sequentially with straight lines. Figure 2.10c\hbox {} is the original $x$ position. Figure 2.10d\hbox {} is the original $y$ positions.\relax }}{45}}
\newlabel{original512}{{2.10}{45}}
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Mean squared error. V-spline returns smallest errors among all these methods. P-spline was unable to reconstruct the $y$ trajectory as the original data set contains 0 $\Delta _y$.\relax }}{45}}
\newlabel{1dxymse}{{2.6}{45}}
\newlabel{ggRealdataXPSpline}{{2.11a}{46}}
\newlabel{sub@ggRealdataXPSpline}{{a}{46}}
\newlabel{ggRealdataXSure}{{2.11b}{46}}
\newlabel{sub@ggRealdataXSure}{{b}{46}}
\newlabel{ggRealdataXBayes}{{2.11c}{46}}
\newlabel{sub@ggRealdataXBayes}{{c}{46}}
\newlabel{ggRealdataXTractorGamma}{{2.11d}{46}}
\newlabel{sub@ggRealdataXTractorGamma}{{d}{46}}
\newlabel{ggRealdataXTractorAPT}{{2.11e}{46}}
\newlabel{sub@ggRealdataXTractorAPT}{{e}{46}}
\newlabel{ggRealdataXTractor}{{2.11f}{46}}
\newlabel{sub@ggRealdataXTractor}{{f}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Fitted data points on $x$ axis. Figure 2.11a\hbox {} Fitted by P-spline, which gives over-fitting on these points and misses some information. Figure 2.11b\hbox {} Fitted by wavelet ($\textit  {sure}$) algorithm. At some turning points, it gives over-fitting. Figure 2.11c\hbox {} Fitted by wavelet ($\textit  {BayesThresh}$) algorithm. It fits better than ($\textit  {sure}$) and the result is close to the proposed method. Figure 2.11d\hbox {} Fitted by V-spline without velocity information. The reconstruction is good to get the original trajectory. Figure 2.11e\hbox {} Fitted by V-spline without adjusted penalty term. It gives less fitting at boom-not-operating points because of a large time gap. Figure 2.11f\hbox {} Fitted by proposed method. It fits all data points in a good way.\relax }}{46}}
\newlabel{1dx}{{2.11}{46}}
\newlabel{ggRealdataYPSpline}{{2.12a}{47}}
\newlabel{sub@ggRealdataYPSpline}{{a}{47}}
\newlabel{ggRealdataYSure}{{2.12b}{47}}
\newlabel{sub@ggRealdataYSure}{{b}{47}}
\newlabel{ggRealdataYBayes}{{2.12c}{47}}
\newlabel{sub@ggRealdataYBayes}{{c}{47}}
\newlabel{ggRealdataYTractorGamma}{{2.12d}{47}}
\newlabel{sub@ggRealdataYTractorGamma}{{d}{47}}
\newlabel{ggRealdataYTractorAPT}{{2.12e}{47}}
\newlabel{sub@ggRealdataYTractorAPT}{{e}{47}}
\newlabel{ggRealdataYTractor}{{2.12f}{47}}
\newlabel{sub@ggRealdataYTractor}{{f}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Fitted data points on $y$ axis. Figure 2.12a\hbox {} Fitted P-spline is not applicable on $y$ axis as the matrix is not invertible. Figure 2.12b\hbox {} Fitted by wavelet ($\textit  {sure}$) algorithm. At some turning points, it gives over-fitting. Figure 2.12c\hbox {} Fitted by wavelet ($\textit  {BayesThresh}$) algorithm is much better than wavelet ($\textit  {sure}$). Figure 2.12d\hbox {} Fitted by V-spline without velocity information. The reconstruction is good to get the original trajectory. Figure 2.12e\hbox {} Fitted by V-spline without adjusted penalty term. It gives less fitting at boom-not-operating. Figure 2.12f\hbox {} Fitted by proposed method. It fits all data points in a good way.\relax }}{47}}
\newlabel{1dy}{{2.12}{47}}
\newlabel{penaltyxyggXYLine}{{2.13a}{48}}
\newlabel{sub@penaltyxyggXYLine}{{a}{48}}
\newlabel{penaltyxyggXYPath}{{2.13b}{48}}
\newlabel{sub@penaltyxyggXYPath}{{b}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The penalty value $\lambda (t)$ of the V-spline on $x$ and $y$ axes. Red dots are the measurements $\mathbf  {y}$. The bigger red dots in Figure 2.13b\hbox {} indicate larger penalty values at the points. It can be seen that most of large penalty values occur at turnings, where the tractor likely slows down and takes breaks. \relax }}{48}}
\newlabel{penaltyxygg}{{2.13}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}2-Dimensional Trajectory Reconstruction}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Combined reconstruction on easting ($x$) and northing ($y$) directions. Red dots are the measurements. The bigger size indicates larger penalty value at that point. \relax }}{49}}
\newlabel{1DCombinedXY}{{2.14}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces 2-dimensional reconstruction. Larger dots indicate bigger values of penalty function $\lambda (t)$.\relax }}{50}}
\newlabel{completecombind2dxy}{{2.15}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Penalty value of $\lambda (t)$ in 2-dimensional reconstruction.\relax }}{51}}
\newlabel{2dpenalty}{{2.16}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A complete 2-dimensional reconstruction on both easting and northing directions. Red dots are the measurements.\relax }}{51}}
\newlabel{complete2DXY}{{2.17}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusion and Discussion}{51}}
\citation{dieudonne2013foundations}
\citation{esl2009}
\citation{kim2004smoothing}
\citation{schoenberg1964spline}
\citation{hastie1990generalized}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}V-Spline as Bayes Estimate}{53}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterGPR}{{3}{53}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{53}}
\newlabel{GaussianProcessGeneralObjective}{{3.2}{53}}
\citation{wahba1978improper}
\citation{speckman2003fully}
\citation{heckman1991minimax}
\citation{branson2017nonparametric}
\citation{cox1993analysis}
\citation{craven1978smoothing}
\citation{wecker1983signal}
\citation{gu1991minimizing}
\citation{wahba1990optimal}
\citation{hastie1990generalized,wang1998smoothing}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Polynomial Smoothing Splines on $[0, 1]$ as Bayes Estimates}{54}}
\citation{judd1998numerical,chen2009feedback}
\citation{ellis2009}
\citation{whittaker1922new}
\citation{kimeldorf1971some,kimeldorf1970correspondence}
\citation{gu2013smoothing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Polynomial Smoothing Spline}{55}}
\newlabel{gausspriorequation}{{3.3}{55}}
\citation{aronszajn1950theory}
\citation{gu2013smoothing}
\citation{wahba1990spline,berlinet2011reproducing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Reproducing Kernel Hilbert Space on $[0,1]$}{56}}
\newlabel{GaussianProcessKernelR}{{3.6}{56}}
\newlabel{theoremRKHS}{{3}{56}}
\newlabel{theoremKernel}{{4}{56}}
\citation{gu2013smoothing}
\citation{gu2013smoothing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Polynomial Smoothing Splines as Bayes Estimates}{57}}
\newlabel{Ef0f0}{{3.7}{57}}
\newlabel{Ef1f1}{{3.8}{57}}
\citation{rasmussen2006gaussian}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Gaussian Process Regression}{58}}
\newlabel{GaussianProcessCovDef}{{3.17}{58}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}V-Spline as Bayes Estimate}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Reproducing Kernel Hilbert Space $\mathcal  {C}_{\unhbox \voidb@x \hbox {\relax \fontsize  {8}{9.5}\selectfont  p.w.}}^{(2)}[0,1]$}{59}}
\newlabel{maineq}{{3.22}{59}}
\newlabel{TractorSplineInnerProduct}{{3.23}{59}}
\newlabel{kerneleq}{{3.24}{60}}
\newlabel{TractorSplineKernelR0}{{3.25}{60}}
\newlabel{TractorSplineKernelR1}{{3.26}{60}}
\newlabel{GaussianProcessFunctionF}{{3.30}{60}}
\newlabel{GassianProcessRawequation}{{3.31}{61}}
\newlabel{matriteq}{{3.32}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Posterior of Bayes Estimates}{61}}
\newlabel{sectionBayesEstimate}{{3.3.2}{61}}
\citation{johnson1992applied}
\newlabel{GPrhoeq}{{3.35}{62}}
\newlabel{GPLemma}{{3}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Bayes Estimate for Non-trivial V-Spline}{63}}
\newlabel{nontrivialInner}{{3.44}{63}}
\newlabel{maineq2}{{3.53}{65}}
\newlabel{GaussianProcessFunctionF2}{{3.54}{65}}
\newlabel{GassianProcessRawequation2}{{3.55}{65}}
\newlabel{matriteq2}{{3.58}{65}}
\citation{wang1998smoothing}
\citation{opsomer2001nonparametric}
\citation{diggle1989spline}
\citation{kohn1992nonparametric}
\citation{wang1998smoothing}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}V-Spline with Correlated Random Errors}{66}}
\citation{syed2011review}
\citation{gu2013smoothing}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{68}}
\citation{cappe2009inference,smcmip2011,elliott1995estimation,cargnoni1997bayesian}
\citation{vieira2016online}
\citation{hangos2006analysis}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}An Overview of On-line State and Parameter Estimation}{69}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterFR}{{4}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{69}}
\citation{petris2009dynamic}
\citation{petris2009dynamic}
\citation{kalman1960new}
\citation{de1988likelihood}
\newlabel{statemodel1}{{4.1}{70}}
\newlabel{statemodel2}{{4.2}{70}}
\citation{pham2013development}
\citation{toloei2014state}
\citation{handschin1969monte,handschin1970monte}
\citation{gordon1993novel}
\citation{cappe2009inference}
\citation{smcmip2011,ristic2004beyond}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}State Estimation Filters}{71}}
\newlabel{sectionFiltering}{{4.2}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Sequential Monte Carlo Method}{71}}
\citation{arulampalam2002tutorial}
\newlabel{rawParticleFilter}{{4.6}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Importance sampling}{73}}
\newlabel{PFexpectation}{{4.10}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Sequential Importance Sampling and Resampling}{74}}
\citation{smcmip2011}
\citation{andrieu2010particle}
\citation{andrieu1999sequential,fearnhead2002markov,storvik2002particle}
\citation{pitt1999filtering}
\citation{pitt1999filtering}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.1}{\ignorespaces Sampling and Importance Sampling\relax }}{76}}
\newlabel{algorithmSIS}{{4.1}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Auxiliary Particle Filter}{76}}
\citation{gordon1993novel}
\citation{JOHANSEN20081498}
\citation{liu2008monte}
\citation{chopin2002sequential}
\citation{chopin2002sequential}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Sequential Particle Filter}{77}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.2}{\ignorespaces Sequential Particle Filter\relax }}{78}}
\newlabel{algorithmSequentialPF}{{4.2}{78}}
\citation{septier2009mcmc}
\citation{berzuini1997dynamic}
\citation{khan2005mcmc,golightly2006bayesian,pang2008models}
\citation{septier2009mcmc}
\citation{pang2008models}
\citation{septier2009multiple}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}MCMC-Based Particle Algorithm}{79}}
\newlabel{mcmcbasedposterior}{{4.29}{79}}
\citation{cappe2007overview}
\citation{kantas2009overview}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.3}{\ignorespaces MCMC-Based Particle Algorithm\relax }}{80}}
\newlabel{algorithmMCMCParticle}{{4.3}{80}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}On-line State and Parameter Estimation}{80}}
\newlabel{sectionStateandPara}{{4.3}{80}}
\citation{higuchi2001self,kitagawa1998self}
\citation{liu2001combined}
\citation{polson2008practical}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Artificial Dynamic Noise}{81}}
\newlabel{ArtificialNoise}{{4.3.1}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Practical Filtering}{81}}
\citation{polson2008practical,kantas2009overview}
\citation{liu2001combined}
\citation{west1993mixture}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Liu and West's Filter}{82}}
\newlabel{LiuandWestDensity}{{4.38}{82}}
\citation{liu2001combined}
\citation{storvik2002particle}
\citation{lopes2011particle}
\citation{storvik2002particle}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.4}{\ignorespaces Practical Filtering Algorithm\relax }}{83}}
\newlabel{algorithmPraticalFilter}{{4.4}{83}}
\citation{carvalho2010particle}
\citation{chen2000mixture}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.5}{\ignorespaces Liu and West's Filter\relax }}{84}}
\newlabel{algorithmLWFilter}{{4.5}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Storvik Filter}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Particle Learning}{84}}
\citation{vieira2016online}
\citation{chopin2010particle}
\citation{stroud2018bayesian}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.6}{\ignorespaces Storvik Filter\relax }}{85}}
\newlabel{algorithmStFilter}{{4.6}{85}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.7}{\ignorespaces Particle Learning Algorithm\relax }}{85}}
\newlabel{algorithmPL}{{4.7}{85}}
\citation{evensen1994sequential}
\citation{katzfuss2016understanding}
\citation{stroud2018bayesian}
\citation{mitchell2000adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Adaptive Ensemble Kalman Filter}{86}}
\newlabel{jointposterior}{{4.40}{86}}
\newlabel{jointposteriorterm1}{{4.41}{86}}
\newlabel{jointposteriorterm2}{{4.42}{86}}
\newlabel{ensembleKalmanForecast}{{4.44}{86}}
\citation{anderson2001ensemble}
\newlabel{esembleKalmanLikeli}{{4.45}{87}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.8}{\ignorespaces Adaptive Ensemble Kalman Filter\relax }}{87}}
\newlabel{algorithmEnKF}{{4.8}{87}}
\citation{dempster1977maximum}
\citation{kantas2009overview}
\citation{andrieu2005line}
\citation{andrieu2005line}
\citation{kantas2009overview}
\citation{andrieu2010particle}
\citation{liu2001combined}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}On-line Pseudo-Likelihood Estimation}{88}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Simulation Study}{88}}
\newlabel{sectionFilterreviewSimulation}{{4.4}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Plots of the inferred value of $\phi $ over time; median value shown for filtering methods and mean value shown for MCMC methods. Repeatedly running 50 times with cutting off the first \textbf  {300} data. It is apparent that all these algorithms converge to the true parameter (black horizontal line) along with time. St, PL and MCMC-vary have a narrower range. MCMC-100 has a higher variability and MCMC-vary has the least. The more data incorporated in the estimation phase the better approximation to be obtained. \relax }}{91}}
\newlabel{FilterRiewComparesion01}{{4.1}{91}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Box-plots comparison of all the algorithms. The proposed MCMC algorithm is more stable than other filters.\relax }}{92}}
\newlabel{FilterRiewComparesionTable}{{4.2}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Plot of state estimation over time; $\mathaccentV {hat}05E{x}_t$ is median value for filtering methods and mean value for MCMC methods. The filtering for $x_{300:897}$ is competitive. The algorithms return very similar estimates. The plots for MCMC-100 and MCMC-300 are hard to distinguish from MCMC-vary.\relax }}{93}}
\newlabel{FilterRiewComparesion02}{{4.3}{93}}
\citation{wakefield2013bayesian}
\citation{wan2000unscented}
\citation{andrieu2010particle}
\citation{poyiadjis2005maximum}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{94}}
\citation{vieira2016online}
\citation{kitagawa1998self}
\citation{liu2001combined}
\citation{storvik2002particle}
\citation{stroud2018bayesian}
\citation{carvalho2010particle}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Adaptive Sequential MCMC for On-line State and Parameter Estimation}{95}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterMCMC}{{5}{95}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{95}}
\citation{stroud2018bayesian}
\citation{polson2008practical}
\citation{haario1999adaptive}
\citation{hammersley1964percolation,geweke1989bayesian}
\citation{casella2004generalized,martino2010generalized}
\citation{geman1984stochastic}
\citation{metropolis1953equation,hastings1970monte}
\citation{payne2018two,quiroz2018speeding}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Bayesian Inference on Combined State and Parameter}{97}}
\newlabel{objecfun}{{5.2}{97}}
\citation{durbin2012time}
\newlabel{M1}{{5.3}{98}}
\newlabel{M2}{{5.4}{98}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}The Posterior Distribution}{98}}
\newlabel{sectionlogParameter}{{5.2.1}{98}}
\newlabel{generaljointmatrix}{{5.5}{98}}
\newlabel{inverseYY}{{5.8}{99}}
\newlabel{sigmayy01}{{5.10}{99}}
\newlabel{sigmayy02}{{5.11}{99}}
\newlabel{posteriortheta}{{5.12}{99}}
\newlabel{logposteriorL}{{5.13}{99}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}The Forecast Distribution}{99}}
\newlabel{sectionforecast}{{5.2.2}{99}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}The Estimation Distribution}{100}}
\newlabel{generalEstDistr}{{5.2.3}{100}}
\newlabel{estimationdistribution}{{5.20}{100}}
\newlabel{joinedXYgiventheta}{{5.21}{100}}
\citation{smith1993bayesian,tierney1994markov,gilks1995markov}
\citation{muller1991generic,tierney1994markov}
\citation{dongarra2000guest,medova2008bayesian}
\newlabel{generalmux}{{5.23}{101}}
\newlabel{generalSigx}{{5.24}{101}}
\newlabel{mixtureGaussian}{{5.26}{101}}
\newlabel{mixturemean}{{5.27}{101}}
\newlabel{mixturevariance}{{5.28}{101}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Random Walk Metropolis-Hastings Algorithm}{101}}
\citation{metropolis1953equation}
\citation{sherlock2016adaptive}
\citation{sherlock2010random}
\newlabel{alphabalance}{{5.31}{102}}
\newlabel{stepsizeep}{{5.32}{102}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}The Self-tuning Metropolis-Hastings Algorithm}{103}}
\newlabel{MCMConevariableRW}{{5.1a}{103}}
\newlabel{sub@MCMConevariableRW}{{a}{103}}
\newlabel{MCMCMultivariableRW}{{5.1b}{103}}
\newlabel{sub@MCMCMultivariableRW}{{b}{103}}
\newlabel{MCMCCorrelatedRW}{{5.1c}{103}}
\newlabel{sub@MCMCCorrelatedRW}{{c}{103}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Examples of 2-Dimensional random walk Metropolis-Hastings algorithm. Figure 5.1a\hbox {} is the trace of one-variable-at-a-time random walk. At each time, only one variable is changed and the other one stay constant. Figure 5.1b\hbox {} and 5.1c\hbox {} present the traces by multi-variable-at-a-time random walk. In Figure 5.1b\hbox {}, the proposal for each step is independent, but in Figure 5.1c\hbox {} the proposal are proposed correlated.\relax }}{103}}
\newlabel{randomwalk}{{5.1}{103}}
\citation{christen2005markov}
\newlabel{autostepab}{{5.37}{104}}
\newlabel{stRWMHselect}{{3}{104}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5.1}{\ignorespaces Self-tuning Random Walk Metropolis-Hastings Algorithm\relax }}{104}}
\newlabel{algoonevarible}{{5.1}{104}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Adaptive Delayed-Acceptance Metropolis-Hastings Algorithm}{104}}
\citation{sherlock2015efficiency}
\citation{stroud2018bayesian}
\citation{mathew2012bayesian}
\newlabel{dahalpha2}{{5.39}{105}}
\newlabel{dahalpha1}{{5.40}{105}}
\citation{sherlock2010random}
\citation{roberts2001optimal}
\citation{gelman1996efficient}
\citation{gilks1995markov}
\citation{roberts2001optimal}
\citation{roberts1997weak,bedard2007weak,beskos2009optimal,sherlock2009optimal,sherlock2013optimal}
\citation{sherlock2010random}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Efficiency of Metropolis-Hastings Algorithm}{106}}
\newlabel{effMHA}{{5.3.3}{106}}
\citation{andrieu2008tutorial}
\citation{sherlock2010random}
\citation{graves2011automatic}
\newlabel{MCMClargestep}{{5.2a}{107}}
\newlabel{sub@MCMClargestep}{{a}{107}}
\newlabel{MCMCsmallstep}{{5.2b}{107}}
\newlabel{sub@MCMCsmallstep}{{b}{107}}
\newlabel{MCMCproperstep}{{5.2c}{107}}
\newlabel{sub@MCMCproperstep}{{c}{107}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Metropolis-Hastings sampler for a single parameter with: 5.2a\hbox {} a large step size, 5.2b\hbox {} a small step size, 5.2c\hbox {} an appropriate step size. The upper plots show the sample chains and lower plots indicate the autocorrelation values for each case.\relax }}{107}}
\newlabel{largesmallstepsize}{{5.2}{107}}
\citation{roberts2001optimal}
\citation{roberts2001optimal}
\citation{kass1998markov,robert2004monte}
\citation{gong2016practical}
\citation{geyer1992practical}
\citation{sokal1997monte}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Simulation Studies}{109}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Simulation on Regularly Sampled Time Series Data}{110}}
\citation{lopes2011particle}
\newlabel{precisionMatrix}{{5.4.1}{111}}
\citation{sherman1950adjustment,woodbury1950inverting,bartlett1951inverse,bodewig1956matrix}
\citation{deng2011generalization}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Linear simulation with true parameter $\theta = \{ \phi =0.9,\tau ^2=0.5,\sigma ^2=1\}$. By transforming back to the original scale, the estimation of $\mathaccentV {hat}05E{\theta }$ is $\{\phi = 0.8810, \tau ^2 = 0.5247, \sigma ^2= 0.9416\}$. \relax }}{112}}
\newlabel{linearmarginplots}{{5.3}{112}}
\newlabel{sectionlinearRecursive}{{5.4.1}{112}}
\newlabel{beforeSMformula}{{5.55}{112}}
\citation{bartlett1951inverse}
\newlabel{theoremSMW}{{8}{113}}
\newlabel{SMWformula}{{5.56}{113}}
\newlabel{SMformula}{{5.57}{113}}
\newlabel{linearOUKreg}{{5.58}{113}}
\newlabel{linearOUbreg}{{5.59}{113}}
\citation{tandeo2011linear}
\citation{einstein1956investigations}
\citation{Schobel1999Stochastic}
\newlabel{MCMClinearsimuXall}{{5.4a}{114}}
\newlabel{sub@MCMClinearsimuXall}{{a}{114}}
\newlabel{MCMClinearsimuXt2}{{5.4b}{114}}
\newlabel{sub@MCMClinearsimuXt2}{{b}{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Linear simulation for $x_{1:t}$ and single $x_t$. In Figure 5.4a\hbox {}, the black dots are the true $x_{1:t}$ and the solid line represents the means of the estimation $\mu ^{(x)}_{1:t}$. In Figure 5.4b\hbox {}, the mean $\mu ^{(x)}_t$ of the chain $\mu ^{(x)}_{ti}$ is very close to the true $x$. In fact, the true $x$ falls in the interval $[\mu ^{(x)}_t-se^{(x)}_t,\mu ^{(x)}_t+se^{(x)}_t]$, where $se^{(x)}_t$ is the square rooted $\mathrm  {Var}(x_t)$.\relax }}{114}}
\newlabel{linearmarginXt}{{5.4}{114}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Simulation on Irregularly Sampled Time Series Data}{114}}
\citation{kijima1997markov}
\newlabel{linearOUequation}{{5.67}{115}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Simulated data. The solid dots indicate the true state $x$ and cross dots indicate observation $y$. Irregular time lag $\Delta _t$ are generated from \textit  {Inverse Gamma}(2,0.1) distribution.\relax }}{116}}
\newlabel{simuOUreview}{{5.5}{116}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Irregular time step OU process simulation. The estimation of $\mathaccentV {hat}05E{\theta }$ is $\{\gamma =0.4841, \lambda ^2=0.1032, \sigma ^2=0.9276\}$. In the plots, the horizontal dark lines are the true $\theta $. \relax }}{116}}
\newlabel{simuOUmarginplots}{{5.6}{116}}
\newlabel{linearOUKirreg}{{5.71}{117}}
\newlabel{linearOUbirreg}{{5.72}{117}}
\newlabel{linearOUmu}{{5.73}{117}}
\newlabel{linearOUsigma}{{5.74}{117}}
\newlabel{linearOUmean}{{5.75}{117}}
\newlabel{linearOUvar}{{5.76}{117}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}High Dimensional Ornstein-Uhlenbeck Process Application}{117}}
\newlabel{SectionHighDimensionalOU}{{5.5}{117}}
\newlabel{OUprocess}{{5.77}{117}}
\newlabel{MCMCOUallX}{{5.7a}{118}}
\newlabel{sub@MCMCOUallX}{{a}{118}}
\newlabel{MCMCOUallXt2}{{5.7b}{118}}
\newlabel{sub@MCMCOUallXt2}{{b}{118}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Irregular time step OU process simulation of $x_{1:t}$ and sole $x_t$. In Figure 5.7a\hbox {}, the dots is the true $x_{1:t}$ and the solid line represents the means of the estimation $\mu ^{(x)}_{1:t}$. In Figure 5.7b\hbox {}, the chain in solid line is the estimation $\mu ^{(x)}_{ti}$ and its mean $\mu ^{(x)}_t$; dotted line is the true value of $x_t$; dot-dash line on top is the observed value $y_t$; dashed lines represent the range of uncertainties $[\mu ^{(x)}_t-se^{(x)}_t,\mu ^{(x)}_t+se^{(x)}_t]$, where $se^{(x)}_t$ is the square rooted $\mathrm  {Var}(x_t)$.\relax }}{118}}
\newlabel{simuOUxt}{{5.7}{118}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Demonstration of line-based trajectory of a moving tractor. The time lags (right side figure) obtained from GPS units are irregular.\relax }}{119}}
\newlabel{realdatareview}{{5.8}{119}}
\newlabel{obseq}{{5.82}{119}}
\newlabel{obmodel}{{5.83}{119}}
\newlabel{jointmatrix}{{5.84}{119}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}The Posterior Distribution}{120}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}The Forecast Distribution}{121}}
\newlabel{OUupdatingK}{{5.99}{122}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}The Estimation Distribution}{122}}
\citation{lindley1972bayes,smith1973general}
\citation{gelman2006prior}
\citation{jeffries1961theory}
\citation{jaynes1983papers}
\citation{box2011bayesian}
\citation{gelman2008weakly}
\citation{stroud2007sequential}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Prior Distribution for Parameters}{123}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Efficiency of Delayed-Acceptance Metropolis-Hastings Algorithm}{124}}
\citation{christen2010general}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Probability density function and cumulative distribution function of \textit  {Inverse Gamma} with two parameters $\alpha $ and $\beta $. \relax }}{125}}
\newlabel{IGPDFCDF}{{5.9}{125}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces An example of Eff, EffUT, ESS and ESSUT found by running 10\tmspace  +\thinmuskip {.1667em}000 iterations with same data. The computation time is measured in seconds\nobreakspace  {}$s$. \relax }}{125}}
\newlabel{effeutessessutexampletable}{{5.1}{125}}
\citation{polson2008practical}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Influences of different step sizes on sampling efficiency (Eff), efficiency in unit time (EffUT), effective sample size (ESS) and effective sample size in unit time (ESSUT) found with the same data\relax }}{126}}
\newlabel{effeutessessutexamplefigure}{{5.10}{126}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Comparison of Eff, EffUT, ESS and ESSUT values with different step size. The $1000^\star $ means taking 1\tmspace  +\thinmuskip {.1667em}000 samples from a longer chain, like 1\tmspace  +\thinmuskip {.1667em}000 out of 5\tmspace  +\thinmuskip {.1667em}000 sample chain. The computation time is measured in seconds\nobreakspace  {}$s$.\relax }}{127}}
\newlabel{stepsizecompare}{{5.2}{127}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.6}A Sliding Window State and Parameter Estimation Approach}{127}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Comparison of efficiency (Eff), efficiency in unit time (EffUT), effective sample size (ESS) and effective sample size in unit time (ESSUT) against the different length of data. Increasing data length does not significantly improve the efficiency and ESSUT.\relax }}{128}}
\newlabel{compareLengthData}{{5.11}{128}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Comparing $\qopname  \relax o{ln}DA$ and $\qopname  \relax o{ln}L$ surfaces between not-updating-mean and updating-mean methods. It is obviously that the updating-mean method has higher dense log-surfaces, which contain more effective samples.\relax }}{130}}
\newlabel{comparenotanupDAL}{{5.12}{130}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Comparing acceptance rates $\alpha _1$, $\alpha _2$, EffUT and ESSUT between not-updating-mean and updating-mean methods. Black solid dots $\bullet $ indicate values obtained from not-updating-mean method and black solid triangular $\blacktriangle $ indicate values obtained from updating-mean method. The acceptance rates of the updating-mean method are more stable and effective samples are larger in unit computation time. \relax }}{131}}
\newlabel{comparenotanupfeatures}{{5.13}{131}}
\newlabel{algorithmlearningsurface}{{2}{132}}
\newlabel{algorithmestimaiton}{{3}{132}}
\newlabel{algorithmDA}{{4}{132}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5.2}{\ignorespaces Sliding Window Adaptive MCMC\relax }}{132}}
\newlabel{algorithmslidingwindow}{{5.2}{132}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.7}Application to 2-Dimensional GPS Data}{133}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Visualization of the parameters correlation matrix, which is found in the learning phase. Diagonal labels represent for $\gamma $, $\xi ^2$, $\lambda ^2$, $\sigma ^2$ and $\tau ^2$. \relax }}{133}}
\newlabel{realdatacorMatrix}{{5.14}{133}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Trace plots of $\theta $ after taking 1\tmspace  +\thinmuskip {.1667em}000 burn-in samples out from 5\tmspace  +\thinmuskip {.1667em}000 from the learning phase.\relax }}{134}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Estimations of $Z$ found by combined batch and sequential methods. The red line is the estimation by batch method and the green line is the sequential MCMC filtering estimation. Black dots are the measurements.\relax }}{135}}
\newlabel{MCMCfirstportionestimation}{{5.16}{135}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Uncertainties on easting and northing directions before the first cutting-off procedure. The means of uncertainties on each direction are about 0.5 meters. \relax }}{136}}
\newlabel{MCMCErrorFill}{{5.17}{136}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Discussion and Future Work}{136}}
\citation{laskey2003population}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Two learning phases are colored by red and two sequential estimation phases are colored by green. The algorithm is not able to estimate the data from 1121 till the end because of the lack of observations. Point 1 is the first point of the data stream. Points 2 and 3 are the switching points. Point 4 is the last point of the data stream. \relax }}{137}}
\newlabel{MCMCwholeestimation}{{5.18}{137}}
\citation{freund1995desicion,friedman2001greedy}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Work}{139}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterFuture}{{6}{139}}
\citation{babenko2009family,beygelzimer2015online}
\citation{chen2016xgboost}
\citation{ristic2004beyond,stroud2018bayesian,arulampalam2002tutorial,hartmann2016grid}
\citation{asanovic2006landscape}
\citation{Almasi1994Highly}
\citation{wu2012parallel}
\citation{kontoghiorghes2005handbook}
\citation{bradford1996markov,gelman1992inference}
\citation{wu2012parallel}
\citation{vanderwerken2013parallel}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Summary}{143}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterSummary}{{7}{143}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{147}}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\begingroup \let \l@chapter \l@section \let \l@section \l@subsection }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Proofs and Figures of V-Spline Theorems}{148}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendTS}{{A}{149}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Penalty Matrix in \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces 2.16\hbox {}\unskip \@@italiccorr )}}}{149}}
\newlabel{PenaltyTermDetails}{{A.1}{149}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Proof of Theorem 1\hbox {}}{150}}
\newlabel{AppendixTractorSplineProof}{{A.2}{150}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Proof of Corollary 1\hbox {}}{150}}
\newlabel{proofofCorollary}{{A.3}{150}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Proof of Lemma 2\hbox {}}{152}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Proof of Theorem 2\hbox {}}{152}}
\newlabel{th3proofeq1}{{A.5.1}{152}}
\newlabel{th3proofeq2}{{A.5.3}{153}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Reconstructions at SNR=3}{154}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Reconstructions of generated \textit  {Blocks}, \textit  {Bumps}, \textit  {HeaviSine} and \textit  {Doppler} functions by V-spline at SNR=3. The penalty values $\lambda (t)$ in V-spline are projected into reconstructions. The blacks dots are the measurements. The bigger blacks dots indicate the larger penalty values.\relax }}{154}}
\newlabel{TractorsplineSNR3}{{A.1}{154}}
\@writefile{toc}{\contentsline {section}{\numberline {A.7}Residual Analysis of Simulations}{155}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces ACF of residuals at SNR level of 7.\relax }}{155}}
\newlabel{tractorsplineSNR7acf}{{A.2}{155}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces ACF of residuals at SNR level of 3.\relax }}{156}}
\newlabel{tractorsplineSNR3acf}{{A.3}{156}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.4}{\ignorespaces Residuals of 2-dimensional real data reconstruction \relax }}{157}}
\newlabel{tractorsplineResidualsRealdata}{{A.4}{157}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Calculations and Figures of Adaptive Sequential MCMC}{158}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendMCMC}{{B}{158}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Linear Simulation Calculations}{158}}
\newlabel{linearcalculation}{{B.1}{158}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}OU-Process Calculation}{162}}
\newlabel{OUcalculation}{{B.2}{162}}
\newlabel{OUKtp1}{{B.2.2}{164}}
\newlabel{recursiveKp1}{{B.2.4}{164}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Covariance Matrix in Details}{167}}
\newlabel{covMatrixdetails}{{B.3}{167}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Real Data Implementation}{169}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Running the same amount of time and taking the same length of data, the step size $\epsilon =2.5$ returns the highest ESSUT value and generates more effective samples in a lower correlated relationship. \relax }}{170}}
\newlabel{1koutof8kfigures}{{B.1}{170}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Impacts of data length on optimal parameter. There is an obvious trend on the estimation against length of data in the estimation process. \relax }}{171}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Parameter estimation by running the whole surface learning and DA MH processes with different length of data\relax }}{172}}
\newlabel{lengthofdatacompare}{{B.4}{173}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Comparison Between Batch and Sliding Window Methods}{173}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Comparison of $\alpha _1$, $\alpha _2$, EffUT and ESSUT between batch MCMC (orange) and sliding window MCMC (green). \relax }}{173}}
\newlabel{batchwindowkeyfeature}{{B.3}{173}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Comparison of parameters estimation between batch MCMC (orange) and sliding window MCMC (green). \relax }}{174}}
\newlabel{batchwindowparameter}{{B.4}{174}}
\@writefile{toc}{\contentsline {section}{\numberline {B.6}Parameter Evolution Visualization}{175}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Parameter Evolution Visualization. The correlation among parameters does not change two much. The parameters are considered static. \relax }}{178}}
\newlabel{ParameterEvolutionVisualization}{{B.4}{178}}
\citation{ying2011semantic}
\citation{ivanov2012real}
\citation{douglas1973algorithms}
\citation{chen2009trajectory}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}A Spin-off Outcome: Data Simplification Method}{179}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendSimp}{{C}{179}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Introduction}{179}}
\@writefile{toc}{\contentsline {section}{\numberline {C.2}Simplification Algorithm}{180}}
\citation{lawson2011compression}
\citation{meratnia2004spatiotemporal}
\@writefile{toc}{\contentsline {section}{\numberline {C.3}Evaluation}{181}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Synchronized Euclidean Distance\relax }}{182}}
\newlabel{DataSimpSED}{{C.1}{182}}
\newlabel{DataSimpA}{{C.2a}{182}}
\newlabel{sub@DataSimpA}{{a}{182}}
\newlabel{DataSimpB}{{C.2b}{182}}
\newlabel{sub@DataSimpB}{{b}{182}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces C.2a\hbox {} indicates that the errors are measured at fixed sampling rate as sum of perpendicular distance chords. C.2b\hbox {} indicates that the errors are measured at fixed sampling rates as sum of time-synchronous distance chords.\relax }}{182}}
\newlabel{DataSimpAB}{{C.2}{182}}
\@writefile{toc}{\contentsline {section}{\numberline {C.4}Numerical Study}{182}}
\@writefile{lot}{\contentsline {table}{\numberline {C.1}{\ignorespaces Comparison between raw data and simplified data\relax }}{183}}
\newlabel{DataSimpCompTable}{{C.1}{183}}
\newlabel{ggRawTrac}{{C.3a}{184}}
\newlabel{sub@ggRawTrac}{{a}{184}}
\newlabel{ggDPTrac}{{C.3b}{184}}
\newlabel{sub@ggDPTrac}{{b}{184}}
\newlabel{ggSPTrac}{{C.3c}{184}}
\newlabel{sub@ggSPTrac}{{c}{184}}
\@writefile{lof}{\contentsline {figure}{\numberline {C.3}{\ignorespaces A segment start from time $t=2\tmspace  +\thinmuskip {.1667em}000$ to $3\tmspace  +\thinmuskip {.1667em}000$, recorded by GPS units. $\blacktriangle $ indicates that the boom is not operating. $\bullet $ indicates that the boom is operating. Figure C.3a\hbox {}, the trajectory connected by raw data with 27 points. Figure C.3b\hbox {}, the trajectory connected by simplified data with Douglas-Peucker algorithm with 24 points. Figure C.3c\hbox {}, the trajectory connected by simplified data with proposed simplification algorithm with 23 points.\relax }}{184}}
\newlabel{DataSimpRawTra}{{C.3}{184}}
\bibstyle{otago}
\bibdata{thesis}
\@writefile{lof}{\contentsline {figure}{\numberline {C.4}{\ignorespaces Trajectory fitted by Kalman filter. The mean squared errors of raw data, DP and proposed algorithm are 26.8922, 23.9788 and 23.9710 respectively.\relax }}{185}}
\newlabel{DataSimpKFTra}{{C.4}{185}}
\@writefile{toc}{\contentsline {section}{\numberline {C.5}Conclusion}{185}}
\@writefile{toc}{\endgroup }
\@writefile{toc}{\contentsline {chapter}{\hbox to\@tempdima {\hfil }References}{185}}
\bibcite{abramovich1998wavelet}{{1}{1998}{{Abramovich {\em  et~al.}}}{{Abramovich, Sapatinas, and Silverman}}}
\bibcite{agarwal2003indexing}{{2}{2003}{{Agarwal {\em  et~al.}}}{{Agarwal, Arge, and Erickson}}}
\bibcite{agrawal2015introduction}{{3}{2015}{{Agrawal and Zeng}}{{Agrawal and Zeng}}}
\bibcite{Almasi1994Highly}{{4}{1994}{{Almasi and Gottlieb}}{{Almasi and Gottlieb}}}
\bibcite{anderson1979optimal}{{5}{1979}{{Anderson and Moore}}{{Anderson and Moore}}}
\bibcite{anderson2001ensemble}{{6}{2001}{{Anderson}}{{Anderson}}}
\bibcite{andrieu1999sequential}{{7}{1999}{{Andrieu {\em  et~al.}}}{{Andrieu, De~Freitas, and Doucet}}}
\bibcite{andrieu2001model}{{8}{2001}{{Andrieu {\em  et~al.}}}{{Andrieu, Djuri{\'c}, and Doucet}}}
\bibcite{andrieu2010particle}{{9}{2010}{{Andrieu {\em  et~al.}}}{{Andrieu, Doucet, and Holenstein}}}
\bibcite{andrieu2005line}{{10}{2005}{{Andrieu {\em  et~al.}}}{{Andrieu, Doucet, and Tadic}}}
\bibcite{andrieu2008tutorial}{{11}{2008}{{Andrieu and Thoms}}{{Andrieu and Thoms}}}
\bibcite{arlot2010survey}{{12}{2010}{{Arlot and Celisse}}{{Arlot and Celisse}}}
\bibcite{aronszajn1950theory}{{13}{1950}{{Aronszajn}}{{Aronszajn}}}
\bibcite{arulampalam2002tutorial}{{14}{2002}{{Arulampalam {\em  et~al.}}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{asanovic2006landscape}{{15}{2006}{{Asanovic {\em  et~al.}}}{{Asanovic, Bodik, Catanzaro, Gebis, Husbands, Keutzer, Patterson, Plishker, Shalf, Williams, and Yelick}}}
\bibcite{atchade2009adaptive}{{16}{2009}{{Atchade {\em  et~al.}}}{{Atchade, Fort, Moulines, and Priouret}}}
\bibcite{aydin2013smoothing}{{17}{2013}{{Ayd{\i }n {\em  et~al.}}}{{Ayd{\i }n, Memmedli, and Omay}}}
\bibcite{aydin2012smoothing}{{18}{2012}{{Aydin and Tuzemen}}{{Aydin and Tuzemen}}}
\bibcite{babenko2009family}{{19}{2009}{{Babenko {\em  et~al.}}}{{Babenko, Yang, and Belongie}}}
\bibcite{bajaj2002gps}{{20}{2002}{{Bajaj {\em  et~al.}}}{{Bajaj, Ranaweera, and Agrawal}}}
\bibcite{bar1993estimation}{{21}{1996}{{Bar-Shalom and Li}}{{Bar-Shalom and Li}}}
\bibcite{bartlett1951inverse}{{22}{1951}{{Bartlett}}{{Bartlett}}}
\bibcite{bedard2007weak}{{23}{2007}{{B{\'e}dard}}{{B{\'e}dard}}}
\bibcite{ben2004geometric}{{24}{2004}{{Ben-Arieh {\em  et~al.}}}{{Ben-Arieh, Chang, Rys, and Zhang}}}
\bibcite{berlinet2011reproducing}{{25}{2011}{{Berlinet and Thomas-Agnan}}{{Berlinet and Thomas-Agnan}}}
\bibcite{berzuini1997dynamic}{{26}{1997}{{Berzuini {\em  et~al.}}}{{Berzuini, Best, Gilks, and Larizza}}}
\bibcite{beskos2009optimal}{{27}{2009}{{Beskos {\em  et~al.}}}{{Beskos, Roberts, and Stuart}}}
\bibcite{betancourt2017conceptual}{{28}{2017}{{Betancourt}}{{Betancourt}}}
\bibcite{beygelzimer2015online}{{29}{2015}{{Beygelzimer {\em  et~al.}}}{{Beygelzimer, Hazan, Kale, and Luo}}}
\bibcite{biagiotti2013online}{{30}{2013}{{Biagiotti and Melchiorri}}{{Biagiotti and Melchiorri}}}
\bibcite{bierkens2017limit}{{31}{2017}{{Bierkens and Duncan}}{{Bierkens and Duncan}}}
\bibcite{bierkens2016piecewise}{{32}{2016}{{Bierkens and Roberts}}{{Bierkens and Roberts}}}
\bibcite{bishop2001introduction}{{33}{2001}{{Bishop and Welch}}{{Bishop and Welch}}}
\bibcite{blaauw2011flexible}{{34}{2011}{{Blaauw and Christen}}{{Blaauw and Christen}}}
\bibcite{bodewig1956matrix}{{35}{1959}{{Bodewig}}{{Bodewig}}}
\bibcite{box2011bayesian}{{36}{2011}{{Box and Tiao}}{{Box and Tiao}}}
\bibcite{bradford1996markov}{{37}{1996}{{Bradford and Thomas}}{{Bradford and Thomas}}}
\bibcite{branson2017nonparametric}{{38}{2017}{{Branson {\em  et~al.}}}{{Branson, Rischard, Bornn, and Miratrix}}}
\bibcite{cantoni2001resistant}{{39}{2001}{{Cantoni and Ronchetti}}{{Cantoni and Ronchetti}}}
\bibcite{cappe2007overview}{{40}{2007}{{Capp{\'e} {\em  et~al.}}}{{Capp{\'e}, Godsill, and Moulines}}}
\bibcite{cappe2009inference}{{41}{2009}{{Capp{\'e} {\em  et~al.}}}{{Capp{\'e}, Moulines, and Ryd{\'e}n}}}
\bibcite{cargnoni1997bayesian}{{42}{1997}{{Cargnoni {\em  et~al.}}}{{Cargnoni, M{\"u}ller, and West}}}
\bibcite{carlin1992monte}{{43}{1992}{{Carlin {\em  et~al.}}}{{Carlin, Polson, and Stoffer}}}
\bibcite{carpenter1999improved}{{44}{1999}{{Carpenter {\em  et~al.}}}{{Carpenter, Clifford, and Fearnhead}}}
\bibcite{carvalho2010particle}{{45}{2010}{{Carvalho {\em  et~al.}}}{{Carvalho, Johannes, Lopes, and Polson}}}
\bibcite{casella2004generalized}{{46}{2004}{{Casella {\em  et~al.}}}{{Casella, Robert, and Wells}}}
\bibcite{castro2006geometric}{{47}{2006}{{Castro {\em  et~al.}}}{{Castro, Iglesias, Rodr{\'i}guez-Solano, and S{\'a}nchez}}}
\bibcite{chadil2008real}{{48}{2008}{{Chadil {\em  et~al.}}}{{Chadil, Russameesawang, and Keeratiwintakorn}}}
\bibcite{chandrasekar2007comparison}{{49}{2007}{{Chandrasekar {\em  et~al.}}}{{Chandrasekar, Ridley, and Bernstein}}}
\bibcite{chen2012monte}{{50}{2012}{{Chen {\em  et~al.}}}{{Chen, Shao, and Ibrahim}}}
\bibcite{chen2000mixture}{{51}{2000}{{Chen and Liu}}{{Chen and Liu}}}
\bibcite{chen2016xgboost}{{52}{2016}{{Chen and Guestrin}}{{Chen and Guestrin}}}
\bibcite{chen2009feedback}{{53}{2017}{{Chen}}{{Chen}}}
\bibcite{chen2009trajectory}{{54}{2009}{{Chen {\em  et~al.}}}{{Chen, Jiang, Zheng, Li, and Yu}}}
\bibcite{chen2003bayesian}{{55}{2003}{{Chen}}{{Chen}}}
\bibcite{chopin2002sequential}{{56}{2002}{{Chopin}}{{Chopin}}}
\bibcite{chopin2010particle}{{57}{2010}{{Chopin {\em  et~al.}}}{{Chopin, Iacobucci, Marin, Mengersen, Robert, Ryder, and Sch{\"a}fer}}}
\bibcite{christen2005markov}{{58}{2005}{{Christen and Fox}}{{Christen and Fox}}}
\bibcite{christen2010general}{{59}{2010}{{Christen and Fox}}{{Christen and Fox}}}
\bibcite{cox1993analysis}{{60}{1993}{{Cox}}{{Cox}}}
\bibcite{cox1982practical}{{61}{1982}{{Cox}}{{Cox}}}
\bibcite{craven1978smoothing}{{62}{1978}{{Craven and Wahba}}{{Craven and Wahba}}}
\bibcite{de1978practical}{{63}{1978}{{De~Boor {\em  et~al.}}}{{De~Boor, De~Boor, Math{\'e}maticien, De~Boor, and De~Boor}}}
\bibcite{de1988likelihood}{{64}{1988}{{De~Jong}}{{De~Jong}}}
\bibcite{dempster1977maximum}{{65}{1977}{{Dempster {\em  et~al.}}}{{Dempster, Laird, and Rubin}}}
\bibcite{deng2011generalization}{{66}{2011}{{Deng}}{{Deng}}}
\bibcite{dierckx1995curve}{{67}{1995}{{Dierckx}}{{Dierckx}}}
\bibcite{dieudonne2013foundations}{{68}{2013}{{Dieudonn{\'e}}}{{Dieudonn{\'e}}}}
\bibcite{diggle1989spline}{{69}{1989}{{Diggle and Hutchinson}}{{Diggle and Hutchinson}}}
\bibcite{dongarra2000guest}{{70}{2000}{{Dongarra and Sullivan}}{{Dongarra and Sullivan}}}
\bibcite{donoho1995adapting}{{71}{1995}{{Donoho and Johnstone}}{{Donoho and Johnstone}}}
\bibcite{donoho1995wavelet}{{72}{1995}{{Donoho {\em  et~al.}}}{{Donoho, Johnstone, Kerkyacharian, and Picard}}}
\bibcite{donoho1994ideal}{{73}{1994}{{Donoho and Johnstone}}{{Donoho and Johnstone}}}
\bibcite{smcmip2011}{{74}{2011}{{Doucet {\em  et~al.}}}{{Doucet, de~Freitas, and Gordon}}}
\bibcite{doucet2000rao}{{75}{2000}{{Doucet {\em  et~al.}}}{{Doucet, De~Freitas, Murphy, and Russell}}}
\bibcite{doucet2000sequential}{{76}{2000}{{Doucet {\em  et~al.}}}{{Doucet, Godsill, and Andrieu}}}
\bibcite{doucet2009tutorial}{{77}{2009}{{Doucet and Johansen}}{{Doucet and Johansen}}}
\bibcite{douglas1973algorithms}{{78}{1973}{{Douglas and Peucker}}{{Douglas and Peucker}}}
\bibcite{duane1987hybrid}{{79}{1987}{{Duane {\em  et~al.}}}{{Duane, Kennedy, Pendleton, and Roweth}}}
\bibcite{dubins1957curves}{{80}{1957}{{Dubins}}{{Dubins}}}
\bibcite{durbin2012time}{{81}{2012}{{Durbin and Koopman}}{{Durbin and Koopman}}}
\bibcite{eilers1996flexible}{{82}{1996}{{Eilers and Marx}}{{Eilers and Marx}}}
\bibcite{einstein1956investigations}{{83}{1956}{{Einstein}}{{Einstein}}}
\bibcite{elliott1995estimation}{{84}{1995}{{Elliott {\em  et~al.}}}{{Elliott, Aggoun, and Moore}}}
\bibcite{ellis2009}{{85}{2009}{{Ellis {\em  et~al.}}}{{Ellis, Sommerlade, and Reid}}}
\bibcite{erkorkmaz2001high}{{86}{2001}{{Erkorkmaz and Altintas}}{{Erkorkmaz and Altintas}}}
\bibcite{eubank2004simple}{{87}{2004}{{Eubank}}{{Eubank}}}
\bibcite{evensen1994sequential}{{88}{1994}{{Evensen}}{{Evensen}}}
\bibcite{fearnhead2002markov}{{89}{2002}{{Fearnhead}}{{Fearnhead}}}
\bibcite{freund1995desicion}{{90}{1995}{{Freund and Schapire}}{{Freund and Schapire}}}
\bibcite{friedman2001greedy}{{91}{2001}{{Friedman}}{{Friedman}}}
\bibcite{gasparetto2007new}{{92}{2007}{{Gasparetto and Zanotto}}{{Gasparetto and Zanotto}}}
\bibcite{gelb1974applied}{{93}{1974}{{Gelb}}{{Gelb}}}
\bibcite{gelman2006prior}{{94}{2006}{{Gelman}}{{Gelman}}}
\bibcite{roberts1997weak}{{95}{1997}{{Gelman {\em  et~al.}}}{{Gelman, Gilks, and Roberts}}}
\bibcite{gelman2008weakly}{{96}{2008}{{Gelman {\em  et~al.}}}{{Gelman, Jakulin, Pittau, and Su}}}
\bibcite{gelman1996efficient}{{97}{1996}{{Gelman {\em  et~al.}}}{{Gelman, Roberts, and Gilks}}}
\bibcite{gelman1992inference}{{98}{1992}{{Gelman and Rubin}}{{Gelman and Rubin}}}
\bibcite{geman1984stochastic}{{99}{1984}{{Geman and Geman}}{{Geman and Geman}}}
\bibcite{geweke1989bayesian}{{100}{1989}{{Geweke}}{{Geweke}}}
\bibcite{geyer1992practical}{{101}{1992}{{Geyer}}{{Geyer}}}
\bibcite{gilks1995markov}{{102}{1995}{{Gilks {\em  et~al.}}}{{Gilks, Richardson, and Spiegelhalter}}}
\bibcite{girolami2011riemann}{{103}{2011}{{Girolami and Calderhead}}{{Girolami and Calderhead}}}
\bibcite{gloderer2010spline}{{104}{2010}{{Gloderer and Hertle}}{{Gloderer and Hertle}}}
\bibcite{godsill2001maximum}{{105}{2001}{{Godsill {\em  et~al.}}}{{Godsill, Doucet, and West}}}
\bibcite{godsill2000methodology}{{106}{2000}{{Godsill {\em  et~al.}}}{{Godsill, Doucet, and West}}}
\bibcite{golightly2006bayesian}{{107}{2006}{{Golightly and Wilkinson}}{{Golightly and Wilkinson}}}
\bibcite{gong2016practical}{{108}{2016}{{Gong and Flegal}}{{Gong and Flegal}}}
\bibcite{gordon1993novel}{{109}{1993}{{Gordon {\em  et~al.}}}{{Gordon, Salmond, and Smith}}}
\bibcite{graves2011automatic}{{110}{2011}{{Graves}}{{Graves}}}
\bibcite{green1995reversible}{{111}{1995}{{Green}}{{Green}}}
\bibcite{green1993nonparametric}{{112}{1993}{{Green and Silverman}}{{Green and Silverman}}}
\bibcite{gu1998model}{{113}{1998}{{Gu}}{{Gu}}}
\bibcite{gu2013smoothing}{{114}{2013}{{Gu}}{{Gu}}}
\bibcite{gu1991minimizing}{{115}{1991}{{Gu and Wahba}}{{Gu and Wahba}}}
\bibcite{guzzi2016data}{{116}{2016}{{Guzzi}}{{Guzzi}}}
\bibcite{gyorgy2014unscented}{{117}{2014}{{Gy{\"o}rgy {\em  et~al.}}}{{Gy{\"o}rgy, Kelemen, and D{\'a}vid}}}
\bibcite{haario1999adaptive}{{118}{1999}{{Haario {\em  et~al.}}}{{Haario, Saksman, and Tamminen}}}
\bibcite{haario2001adaptive}{{119}{2001}{{Haario {\em  et~al.}}}{{Haario, Saksman, and Tamminen}}}
\bibcite{hammersley1964percolation}{{120}{1964}{{Hammersley and Handscomb}}{{Hammersley and Handscomb}}}
\bibcite{handschin1970monte}{{121}{1970}{{Handschin}}{{Handschin}}}
\bibcite{handschin1969monte}{{122}{1969}{{Handschin and Mayne}}{{Handschin and Mayne}}}
\bibcite{hangos2006analysis}{{123}{2006}{{Hangos {\em  et~al.}}}{{Hangos, Bokor, and Szederk{\'e}nyi}}}
\bibcite{hardle1990applied}{{124}{1990}{{H{\"a}rdle}}{{H{\"a}rdle}}}
\bibcite{hardle1988far}{{125}{1988}{{H{\"a}rdle {\em  et~al.}}}{{H{\"a}rdle, Hall, and Marron}}}
\bibcite{hartmann2016grid}{{126}{2016}{{Hartmann {\em  et~al.}}}{{Hartmann, Nowak, Pfandenhauer, Thielecke, and Heuberger}}}
\bibcite{esl2009}{{127}{2009}{{Hastie {\em  et~al.}}}{{Hastie, Tibshirani, and Friedman}}}
\bibcite{hastie1990generalized}{{128}{1990}{{Hastie and Tibshirani}}{{Hastie and Tibshirani}}}
\bibcite{hastings1970monte}{{129}{1970}{{Hastings}}{{Hastings}}}
\bibcite{haykin2001kalman}{{130}{2001}{{Haykin}}{{Haykin}}}
\bibcite{heckman1991minimax}{{131}{1991}{{Heckman and Woodroofe}}{{Heckman and Woodroofe}}}
\bibcite{Hermite1863Remarque}{{132}{1863}{{Hermite}}{{Hermite}}}
\bibcite{hightower2001location}{{133}{2001}{{Hightower and Borriello}}{{Hightower and Borriello}}}
\bibcite{higuchi2001self}{{134}{2001}{{Higuchi}}{{Higuchi}}}
\bibcite{hintzen2010improved}{{135}{2010}{{Hintzen {\em  et~al.}}}{{Hintzen, Piet, and Brunel}}}
\bibcite{hurvich1998smoothing}{{136}{1998}{{Hurvich {\em  et~al.}}}{{Hurvich, Simonoff, and Tsai}}}
\bibcite{ivanov2012real}{{137}{2012}{{Ivanov}}{{Ivanov}}}
\bibcite{jauch2017recursive}{{138}{2017}{{Jauch {\em  et~al.}}}{{Jauch, Bleimund, Rhode, and Gauterin}}}
\bibcite{jaynes1983papers}{{139}{1983}{{Jaynes}}{{Jaynes}}}
\bibcite{jeffries1961theory}{{140}{1961}{{Jeffries}}{{Jeffries}}}
\bibcite{JOHANSEN20081498}{{141}{2008}{{Johansen and Doucet}}{{Johansen and Doucet}}}
\bibcite{johnson1992applied}{{142}{1992}{{Johnson and Wichern}}{{Johnson and Wichern}}}
\bibcite{judd1998numerical}{{143}{1998}{{Judd}}{{Judd}}}
\bibcite{kailath1981lectures}{{144}{1981}{{Kailath}}{{Kailath}}}
\bibcite{kalman1960new}{{145}{1960}{{Kalman}}{{Kalman}}}
\bibcite{kalos2008monte}{{146}{2008}{{Kalos and Whitlock}}{{Kalos and Whitlock}}}
\bibcite{kantas2009overview}{{147}{2009}{{Kantas {\em  et~al.}}}{{Kantas, Doucet, Singh, and Maciejowski}}}
\bibcite{kaplan2005understanding}{{148}{2005}{{Kaplan and Hegarty}}{{Kaplan and Hegarty}}}
\bibcite{kass1998markov}{{149}{1998}{{Kass {\em  et~al.}}}{{Kass, Carlin, Gelman, and Neal}}}
\bibcite{katzfuss2016understanding}{{150}{2016}{{Katzfuss {\em  et~al.}}}{{Katzfuss, Stroud, and Wikle}}}
\bibcite{khan2005mcmc}{{151}{2005}{{Khan {\em  et~al.}}}{{Khan, Balch, and Dellaert}}}
\bibcite{kijima1997markov}{{152}{1997}{{Kijima}}{{Kijima}}}
\bibcite{kim2004smoothing}{{153}{2004}{{Kim and Gu}}{{Kim and Gu}}}
\bibcite{kimeldorf1971some}{{154}{1971}{{Kimeldorf and Wahba}}{{Kimeldorf and Wahba}}}
\bibcite{kimeldorf1970correspondence}{{155}{1970}{{Kimeldorf and Wahba}}{{Kimeldorf and Wahba}}}
\bibcite{kitagawa1998self}{{156}{1998}{{Kitagawa}}{{Kitagawa}}}
\bibcite{kloek1978bayesian}{{157}{1978}{{Kloek and Van~Dijk}}{{Kloek and Van~Dijk}}}
\bibcite{kohn1992nonparametric}{{158}{1992}{{Kohn {\em  et~al.}}}{{Kohn, Ansley, and Wong}}}
\bibcite{kokkala2016particle}{{159}{2016}{{Kokkala}}{{Kokkala}}}
\bibcite{komoriya1989trajectory}{{160}{1989}{{Komoriya and Tanie}}{{Komoriya and Tanie}}}
\bibcite{kong1994sequential}{{161}{1994}{{Kong {\em  et~al.}}}{{Kong, Liu, and Wong}}}
\bibcite{kontoghiorghes2005handbook}{{162}{2005}{{Kontoghiorghes}}{{Kontoghiorghes}}}
\bibcite{krivobokova2008fast}{{163}{2008}{{Krivobokova {\em  et~al.}}}{{Krivobokova, Crainiceanu, and Kauermann}}}
\bibcite{larson1931shrinkage}{{164}{1931}{{Larson}}{{Larson}}}
\bibcite{laskey2003population}{{165}{2003}{{Laskey and Myers}}{{Laskey and Myers}}}
\bibcite{laviola2003comparison}{{166}{2003}{{LaViola}}{{LaViola}}}
\bibcite{lawson2011compression}{{167}{2011}{{Lawson {\em  et~al.}}}{{Lawson, Ravi, and Hwang}}}
\bibcite{lindley1972bayes}{{168}{1972}{{Lindley and Smith}}{{Lindley and Smith}}}
\bibcite{liu2001combined}{{169}{2001}{{Liu and West}}{{Liu and West}}}
\bibcite{liu2008monte}{{170}{2008}{{Liu}}{{Liu}}}
\bibcite{liu2000multiple}{{171}{2000}{{Liu {\em  et~al.}}}{{Liu, Liang, and Wong}}}
\bibcite{liu2014filtering}{{172}{2014}{{Liu {\em  et~al.}}}{{Liu, Suo, Karimi, and Liu}}}
\bibcite{liu2010data}{{173}{2010}{{Liu and Guo}}{{Liu and Guo}}}
\bibcite{lopes2011particle}{{174}{2011}{{Lopes and Tsay}}{{Lopes and Tsay}}}
\bibcite{magid2006spline}{{175}{2006}{{Magid {\em  et~al.}}}{{Magid, Keren, Rivlin, and Yavneh}}}
\bibcite{mahendran2012adaptive}{{176}{2012}{{Mahendran {\em  et~al.}}}{{Mahendran, Wang, Hamze, and De~Freitas}}}
\bibcite{martino2010generalized}{{177}{2010}{{Martino and M{\'i}guez}}{{Martino and M{\'i}guez}}}
\bibcite{mathew2012bayesian}{{178}{2012}{{Mathew {\em  et~al.}}}{{Mathew, Bauer, Koistinen, Reetz, L{\'e}on, and Sillanp{\"a}{\"a}}}}
\bibcite{mcdonald2006intelligent}{{179}{2006}{{McDonald}}{{McDonald}}}
\bibcite{medova2008bayesian}{{180}{2008}{{Medova}}{{Medova}}}
\bibcite{meratnia2004spatiotemporal}{{181}{2004}{{Meratnia and Rolf}}{{Meratnia and Rolf}}}
\bibcite{metropolis1953equation}{{182}{1953}{{{M}etropolis {\em  et~al.}}}{{{M}etropolis, Rosenbluth, Rosenbluth, Teller, and Teller}}}
\bibcite{mitchell2000adaptive}{{183}{2000}{{Mitchell and Houtekamer}}{{Mitchell and Houtekamer}}}
\bibcite{muller1991generic}{{184}{1991}{{M{\"u}ller}}{{M{\"u}ller}}}
\bibcite{nason2010wavelet}{{185}{2010}{{Nason}}{{Nason}}}
\bibcite{neal2011mcmc}{{186}{2011}{{Neal}}{{Neal}}}
\bibcite{nelder1965simplex}{{187}{1965}{{Nelder and Mead}}{{Nelder and Mead}}}
\bibcite{opsomer2001nonparametric}{{188}{2001}{{Opsomer {\em  et~al.}}}{{Opsomer, Wang, and Yang}}}
\bibcite{oussalah2001adaptive}{{189}{2001}{{Oussalah and De~Schutter}}{{Oussalah and De~Schutter}}}
\bibcite{pang2008models}{{190}{2008}{{Pang {\em  et~al.}}}{{Pang, Li, and Godsill}}}
\bibcite{payne2018two}{{191}{2018}{{Payne and Mallick}}{{Payne and Mallick}}}
\bibcite{petris2009dynamic}{{192}{2009}{{Petris {\em  et~al.}}}{{Petris, Petrone, and Campagnoli}}}
\bibcite{pham2013development}{{193}{2013}{{Pham {\em  et~al.}}}{{Pham, Drieberg, and Nguyen}}}
\bibcite{pitt1999filtering}{{194}{1999}{{Pitt and Shephard}}{{Pitt and Shephard}}}
\bibcite{polson2008practical}{{195}{2008}{{Polson {\em  et~al.}}}{{Polson, Stroud, and M{\"u}ller}}}
\bibcite{poyiadjis2005maximum}{{196}{2005}{{Poyiadjis {\em  et~al.}}}{{Poyiadjis, Doucet, and Singh}}}
\bibcite{quiroz2018speeding}{{197}{2018}{{Quiroz {\em  et~al.}}}{{Quiroz, Tran, Villani, and Kohn}}}
\bibcite{ran2010self}{{198}{2010}{{Ran and Deng}}{{Ran and Deng}}}
\bibcite{rasmussen2006gaussian}{{199}{2006}{{Rasmussen and Williams}}{{Rasmussen and Williams}}}
\bibcite{rhodes1971tutorial}{{200}{1971}{{Rhodes}}{{Rhodes}}}
\bibcite{ristic2004beyond}{{201}{2004}{{Ristic {\em  et~al.}}}{{Ristic, Arulampalam, and Gordon}}}
\bibcite{robert2004monte}{{202}{2004}{{Robert}}{{Robert}}}
\bibcite{roberts2001optimal}{{203}{2001}{{Roberts and Rosenthal}}{{Roberts and Rosenthal}}}
\bibcite{roberts2009examples}{{204}{2009}{{Roberts and Rosenthal}}{{Roberts and Rosenthal}}}
\bibcite{rubin2004multiple}{{205}{2004}{{Rubin}}{{Rubin}}}
\bibcite{ruppert2003semiparametric}{{206}{2003}{{Ruppert {\em  et~al.}}}{{Ruppert, Wand, and Carroll}}}
\bibcite{sarkka2013bayesian}{{207}{2013}{{S{\"a}rkk{\"a}}}{{S{\"a}rkk{\"a}}}}
\bibcite{Schobel1999Stochastic}{{208}{1999}{{Sch{\"o}bel and Zhu}}{{Sch{\"o}bel and Zhu}}}
\bibcite{schoenberg1964spline}{{209}{1964}{{Schoenberg}}{{Schoenberg}}}
\bibcite{schwarz2012geodesy}{{210}{2012}{{Schwarz}}{{Schwarz}}}
\bibcite{sealfon2005smoothing}{{211}{2005}{{Sealfon {\em  et~al.}}}{{Sealfon, Verde, and Jimenez}}}
\bibcite{septier2009multiple}{{212}{2009}{{Septier {\em  et~al.}}}{{Septier, Carmi, Pang, and Godsill}}}
\bibcite{septier2009mcmc}{{213}{2009}{{Septier {\em  et~al.}}}{{Septier, Pang, Carmi, and Godsill}}}
\bibcite{sherlock2013optimal}{{214}{2013}{{Sherlock}}{{Sherlock}}}
\bibcite{sherlock2010random}{{215}{2010}{{Sherlock {\em  et~al.}}}{{Sherlock, Fearnhead, and Roberts}}}
\bibcite{sherlock2016adaptive}{{216}{2017}{{Sherlock {\em  et~al.}}}{{Sherlock, Golightly, and Henderson}}}
\bibcite{sherlock2009optimal}{{217}{2009}{{Sherlock and Roberts}}{{Sherlock and Roberts}}}
\bibcite{sherlock2015efficiency}{{218}{2015}{{Sherlock {\em  et~al.}}}{{Sherlock, Thiery, and Golightly}}}
\bibcite{sherman1950adjustment}{{219}{1950}{{Sherman and Morrison}}{{Sherman and Morrison}}}
\bibcite{silverman1985some}{{220}{1985}{{Silverman}}{{Silverman}}}
\bibcite{smith1973general}{{221}{1973}{{Smith}}{{Smith}}}
\bibcite{smith1993bayesian}{{222}{1993}{{Smith and Roberts}}{{Smith and Roberts}}}
\bibcite{sokal1997monte}{{223}{1997}{{Sokal}}{{Sokal}}}
\bibcite{sorenson1985kalman}{{224}{1985}{{Sorenson}}{{Sorenson}}}
\bibcite{speckman2003fully}{{225}{2003}{{Speckman and Sun}}{{Speckman and Sun}}}
\bibcite{st2004comparison}{{226}{2004}{{St-Pierre and Gingras}}{{St-Pierre and Gingras}}}
\bibcite{stavropoulos2001improved}{{227}{2001}{{Stavropoulos and Titterington}}{{Stavropoulos and Titterington}}}
\bibcite{storvik2002particle}{{228}{2002}{{Storvik}}{{Storvik}}}
\bibcite{stroud2007sequential}{{229}{2007}{{Stroud and Bengtsson}}{{Stroud and Bengtsson}}}
\bibcite{stroud2018bayesian}{{230}{2018}{{Stroud {\em  et~al.}}}{{Stroud, Katzfuss, and Wikle}}}
\bibcite{syed2011review}{{231}{2011}{{Syed}}{{Syed}}}
\bibcite{tandeo2011linear}{{232}{2011}{{Tandeo {\em  et~al.}}}{{Tandeo, Ailliot, and Autret}}}
\bibcite{tanner1987calculation}{{233}{1987}{{Tanner and Wong}}{{Tanner and Wong}}}
\bibcite{tierney1994markov}{{234}{1994}{{Tierney}}{{Tierney}}}
\bibcite{tierney1999some}{{235}{1999}{{Tierney and Mira}}{{Tierney and Mira}}}
\bibcite{toloei2014state}{{236}{2014}{{Toloei and Niazi}}{{Toloei and Niazi}}}
\bibcite{turitsyn2011irreversible}{{237}{2011}{{Turitsyn {\em  et~al.}}}{{Turitsyn, Chertkov, and Vucelja}}}
\bibcite{tusell2011kalman}{{238}{2011}{{Tusell}}{{Tusell}}}
\bibcite{vanderwerken2013parallel}{{239}{2013}{{VanDerwerken and Schmidler}}{{VanDerwerken and Schmidler}}}
\bibcite{vieira2016online}{{240}{2016}{{Vieira and Wilkinson}}{{Vieira and Wilkinson}}}
\bibcite{wahba1978improper}{{241}{1978}{{Wahba}}{{Wahba}}}
\bibcite{wahba1985comparison}{{242}{1985}{{Wahba}}{{Wahba}}}
\bibcite{wahba1990spline}{{243}{1990}{{Wahba}}{{Wahba}}}
\bibcite{wahba1990optimal}{{244}{1990}{{Wahba and Wang}}{{Wahba and Wang}}}
\bibcite{wahba1975completely}{{245}{1975}{{Wahba and Wold}}{{Wahba and Wold}}}
\bibcite{wakefield2013bayesian}{{246}{2013}{{Wakefield}}{{Wakefield}}}
\bibcite{wan2000unscented}{{247}{2000}{{Wan and Van Der~Merwe}}{{Wan and Van Der~Merwe}}}
\bibcite{wand1997exact}{{248}{1997}{{Wand and Gutierrez}}{{Wand and Gutierrez}}}
\bibcite{wang2017curvature}{{249}{2017}{{Wang {\em  et~al.}}}{{Wang, Jiang, Li, and Sun}}}
\bibcite{wang1998smoothing}{{250}{1998}{{Wang}}{{Wang}}}
\bibcite{wecker1983signal}{{251}{1983}{{Wecker and Ansley}}{{Wecker and Ansley}}}
\bibcite{west1993mixture}{{252}{1993}{{West}}{{West}}}
\bibcite{whittaker1922new}{{253}{1922}{{Whittaker}}{{Whittaker}}}
\bibcite{wolberg1988cubic}{{254}{1988}{{Wolberg}}{{Wolberg}}}
\bibcite{woodbury1950inverting}{{255}{1950}{{Woodbury}}{{Woodbury}}}
\bibcite{wu2012parallel}{{256}{2012}{{Wu {\em  et~al.}}}{{Wu, Sun, Beissinger, Rosa, Weigel, de~Leon~Gatti, and Gianola}}}
\bibcite{yang2010analytical}{{257}{2010}{{Yang and Sukkarieh}}{{Yang and Sukkarieh}}}
\bibcite{yao2005functional}{{258}{2005}{{Yao {\em  et~al.}}}{{Yao, M{\"u}ller, and Wang}}}
\bibcite{ying2011semantic}{{259}{2011}{{Ying {\em  et~al.}}}{{Ying, Lee, Weng, and Tseng}}}
\bibcite{yu2004curve}{{260}{2004}{{Yu {\em  et~al.}}}{{Yu, Kim, Bailey, and Gamboa}}}
\bibcite{zhang2013cubic}{{261}{2013}{{Zhang {\em  et~al.}}}{{Zhang, Guo, and Gao}}}
