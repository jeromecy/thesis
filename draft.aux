\relax 
\citation{kaplan2005understanding}
\citation{bajaj2002gps}
\citation{hightower2001location}
\citation{chadil2008real}
\citation{mcdonald2006intelligent}
\citation{eubank2004simple}
\citation{durbin2012time}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterIntro}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Background}{1}}
\citation{esl2009}
\citation{komoriya1989trajectory}
\citation{ben2004geometric}
\citation{de1978practical}
\citation{cox1982practical}
\citation{dierckx1995curve}
\citation{eilers1996flexible}
\citation{gasparetto2007new}
\citation{wolberg1988cubic}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Smoothing Spline Based Reconstruction}{2}}
\citation{green1993nonparametric}
\citation{kim2004smoothing}
\citation{yao2005functional}
\citation{craven1978smoothing}
\citation{aydin2012smoothing}
\citation{esl2009}
\citation{schwarz2012geodesy}
\citation{green1993nonparametric}
\newlabel{introSmoothingOb}{{1.1}{3}}
\citation{gu1998model}
\citation{craven1978smoothing}
\citation{wahba1985comparison}
\citation{hurvich1998smoothing}
\citation{wand1997exact}
\citation{craven1978smoothing}
\citation{hardle1988far}
\citation{hardle1990applied}
\citation{wahba1990spline}
\citation{green1993nonparametric}
\citation{cantoni2001resistant}
\citation{aydin2013smoothing}
\citation{larson1931shrinkage}
\citation{arlot2010survey}
\citation{wahba1975completely}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Cross-Validation Parameter Selection}{4}}
\citation{esl2009}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{kfoldCV01}{{5}{5}}
\newlabel{kfoldCV02}{{8}{5}}
\newlabel{kfoldCV03}{{9}{5}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1.1}{\ignorespaces $k$-fold cross-validation.\relax }}{5}}
\newlabel{kfoldCV}{{1.1}{5}}
\citation{esl2009}
\citation{donoho1995wavelet}
\citation{liu2010data}
\citation{biagiotti2013online}
\citation{anderson1979optimal}
\citation{chen2003bayesian}
\citation{sarkka2013bayesian}
\citation{kalman1960new}
\citation{kalman1960new}
\citation{bishop2001introduction}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Bayesian Filtering}{6}}
\citation{haykin2001kalman}
\citation{chen2003bayesian}
\citation{rhodes1971tutorial}
\citation{kailath1981lectures}
\citation{sorenson1985kalman}
\citation{tusell2011kalman}
\newlabel{introKFmodel}{{1.2}{7}}
\newlabel{KalmanEstimation}{{1.3}{7}}
\citation{gelb1974applied}
\citation{bar1993estimation}
\citation{wan2000unscented}
\citation{julier1997new}
\citation{wan2000unscented}
\citation{gyorgy2014unscented}
\citation{chandrasekar2007comparison}
\citation{laviola2003comparison}
\citation{st2004comparison}
\citation{ran2010self}
\citation{oussalah2001adaptive}
\citation{liu2014filtering}
\citation{jauch2017recursive}
\citation{chen2003bayesian}
\citation{kloek1978bayesian}
\citation{chen2003bayesian}
\citation{kalos2008monte}
\citation{chen2003bayesian}
\citation{green1995reversible}
\citation{berzuini1997dynamic}
\citation{rubin2004multiple}
\citation{tanner1987calculation}
\citation{doucet2009tutorial}
\citation{kong1994sequential}
\citation{carpenter1999improved}
\citation{godsill2001maximum}
\citation{stavropoulos2001improved}
\citation{smcmip2011}
\citation{chen2003bayesian}
\citation{doucet2000sequential}
\citation{doucet2000rao}
\citation{chen2012monte}
\citation{ristic2004beyond}
\citation{godsill2000methodology}
\citation{carlin1992monte}
\citation{andrieu1999sequential}
\citation{green1995reversible}
\citation{andrieu2001model}
\citation{smith1993bayesian}
\citation{cappe2009inference}
\citation{liu2008monte}
\citation{smith1993bayesian}
\citation{tierney1994markov}
\citation{gilks1995markov}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Markov Chain Monte Carlo Methods}{11}}
\citation{tierney1999some}
\citation{liu2000multiple}
\citation{green1995reversible}
\citation{geman1984stochastic}
\citation{smith1993bayesian}
\citation{mackay2003information}
\citation{sorensen2007likelihood}
\citation{garcia1996multivariate}
\citation{mathew2012bayesian}
\citation{gelfand1990illustration}
\newlabel{IntroAccp}{{1.4}{12}}
\citation{mahendran2012adaptive}
\citation{andrieu2008tutorial}
\citation{atchade2009adaptive}
\citation{roberts2009examples}
\citation{haario2001adaptive}
\citation{mahendran2012adaptive}
\citation{roberts2009examples}
\citation{duane1987hybrid}
\citation{neal2011mcmc}
\citation{betancourt2017conceptual}
\citation{bierkens2016piecewise}
\citation{turitsyn2011irreversible}
\citation{bierkens2017limit}
\citation{bierkens2017limit}
\citation{bierkens2017limit}
\citation{bierkens2016zig}
\citation{christen2010general}
\citation{blaauw2011flexible}
\citation{christen2010general}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Outline of Thesis}{16}}
\citation{ben2004geometric}
\citation{Atkinson2004}
\citation{taylor2000integration}
\citation{agarwal2003indexing}
\citation{gloderer2010spline}
\citation{magid2006spline}
\citation{dubins1957curves}
\citation{yang2010analytical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Adaptive Smoothing Tractor Spline for Trajectory Reconstruction}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterTS}{{2}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{17}}
\citation{magid2006spline}
\citation{yu2004curve}
\citation{komoriya1989trajectory}
\citation{ben2004geometric}
\citation{gasparetto2007new}
\citation{erkorkmaz2001high}
\citation{yang2010analytical}
\citation{yao2005functional}
\citation{craven1978smoothing}
\citation{aydin2012smoothing}
\citation{green1993nonparametric}
\citation{schwarz2012geodesy}
\citation{green1993nonparametric}
\citation{sealfon2005smoothing}
\newlabel{smoothingob}{{2.1}{18}}
\citation{zhang2013cubic}
\citation{castro2006geometric}
\citation{castro2006geometric}
\citation{silverman1985some}
\citation{donoho1995wavelet}
\citation{gu1998model}
\citation{craven1978smoothing}
\citation{wahba1985comparison}
\citation{liu2010data}
\newlabel{objective}{{2.3}{19}}
\citation{donoho1994ideal}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Tractor spline}{20}}
\newlabel{SectionTractorSpline}{{2.2}{20}}
\newlabel{tractorsplineObjective}{{2.5}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Basis Functions}{21}}
\newlabel{hermitebasis1}{{2.6}{21}}
\newlabel{basisindependent}{{2}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The two basis functions $N_{2k+1}$ and $N_{2k+2}$ on an arbitrary interval $[t_k, t_{k+2})$. It is apparently that these basis functions are continuous on this interval and have continuous first derivatives.\relax }}{23}}
\newlabel{basisfigure}{{2.1}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Solution to The Objective Function}{23}}
\newlabel{tractormse}{{2.16}{23}}
\citation{esl2009}
\citation{esl2009}
\newlabel{thetahat}{{2.20}{24}}
\newlabel{fhy}{{2.24}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Adjusted Penalty Term and Parameter Function}{25}}
\citation{green1993nonparametric}
\citation{green1993nonparametric}
\newlabel{adjustedpenalty}{{2.26}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Parameter Selection and Cross Validation}{26}}
\newlabel{originalcv}{{2.27}{26}}
\newlabel{crossvalidationmatrixA}{{2.29}{26}}
\newlabel{cvlema}{{1}{27}}
\newlabel{cvscore}{{3}{27}}
\newlabel{tractorcv}{{2.35}{27}}
\citation{donoho1994ideal}
\citation{donoho1995adapting}
\citation{abramovich1998wavelet}
\citation{nason2010wavelet}
\citation{donoho1995adapting}
\citation{abramovich1998wavelet}
\citation{krivobokova2008fast}
\citation{ruppert2003semiparametric}
\newlabel{cvlemma}{{2}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Simulation Study}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Numerical Examples}{28}}
\newlabel{generateVelocity}{{2.40}{28}}
\citation{nelder1965simplex}
\newlabel{ofgamma0}{{2.41}{29}}
\newlabel{thetahat0}{{2.42}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Numerical example: $\textit  {Blocks}$. Comparison of different reconstruction methods with simulated data.\relax }}{30}}
\newlabel{num1}{{2.2}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Numerical example: $\textit  {Bumps}$. Comparison of different reconstruction methods with simulated data.\relax }}{31}}
\newlabel{num2}{{2.3}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Numerical example: $\textit  {HeaviSine}$. Comparison of different reconstruction methods with simulated data.\relax }}{32}}
\newlabel{num3}{{2.4}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Numerical example: $\textit  {Doppler}$. Comparison of different reconstruction methods with simulated data.\relax }}{33}}
\newlabel{num4}{{2.5}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Distribution of penalty values in reconstructed Tractor spline. Figures on left side indicate the values of $\lambda (t)$ varying in intervals. On right side, $\lambda (t)$ is projected into reconstructions. The bigger the blacks dots present, the larger the penalty values are.\relax }}{35}}
\newlabel{numpenalty}{{2.6}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Evaluation}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Estimated velocity functions by Tractor spline. The velocity is generated from the original simulation functions by equation (2.40\hbox {})\relax }}{36}}
\newlabel{numvtractor}{{2.7}{36}}
\citation{zhang2013cubic}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces MSE. Mean square errors of different methods. The numbers in bold indicate the smallest error among these methods under the same level. The difference is not significant.\relax }}{37}}
\newlabel{mse3200}{{2.1}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Application on Real Dataset}{37}}
\newlabel{splineapplication}{{2.5}{37}}
\newlabel{tractorsplineObjective2D}{{2.46}{37}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces TMSE. True mean square errors of different methods. The numbers in bold indicate the smallest error among these methods under the same level. The proposed Tractor spline returns the smallest TMSE among all the methods under the same level except for $\textit  {Doppler}$ with SNR=7. The differences are significant. \relax }}{38}}
\newlabel{tmse3200}{{2.2}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Original data points. (a) Original positions recorded by GPS units. Circle points means the boom is not working; cross points means it is working. (b) Original trajectory with line-based method: simply connect all the points sequentially with straight lines. (c) Original positions on $x$-axis. (d) Original positions on $y$-axis.\relax }}{39}}
\newlabel{original512}{{2.8}{39}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Mean square error. Tractor spline returns smallest errors among all these methods. P-spline was unable to reconstruct the $y$ trajectory as the original dataset contains value-$0$ time differences.\relax }}{40}}
\newlabel{1dxymse}{{2.3}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}1-Dimension Trajectory}{40}}
\newlabel{penaltylamb}{{2.49}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Fitted data points on $x$ axis. (a) Fitted by P-spline, which gives over-fitting on these points and misses some information. (b) Fitted by wavelet ($\textit  {sure}$) algorithm. At some turning points, it gives over-fitting. (c) Fitted by wavelet ($\textit  {BayesThresh}$) algorithm. It fits better than ($\textit  {sure}$) and the result is close to the proposed method. (d) Fitted by Tractor spline without velocity information. The reconstruction is good to get the original trajectory. (e) Fitted by Tractor spline without adjusted penalty term. It gives less fitting at boom-not-working points because of a large time gap. (f) Fitted by proposed method. It fits all data points in a good way.\relax }}{41}}
\newlabel{1dx}{{2.9}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Fitted data points on $y$ axis. (a) Fitted P-spline is not applicable on $y$ axis as the matrix is not invertible. (b) Fitted by wavelet ($\textit  {sure}$) algorithm. At some turning points, it gives over-fitting. (c) Fitted by wavelet ($\textit  {BayesThresh}$) algorithm is much better than wavelet ($\textit  {sure}$). (d) Fitted by Tractor spline without velocity information. The reconstruction is good to get the original trajectory. (e) Fitted by Tractor spline without adjusted penalty term. It gives less fitting at boom-not-working. (f) Fitted by proposed method. It fits all data points in a good way.\relax }}{42}}
\newlabel{1dy}{{2.10}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}2-Dimension Trajectory}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Penalty function of Tractor spline on $x$ and $y$ axises. The big black dots in plots (c) indicate large penalty values. It can be seen that most of large penalty values occur at turnings, where the tractor likely slows down and takes breaks. \relax }}{44}}
\newlabel{penaltyxygg}{{2.11}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces 2-Dimensional reconstruction with penalty function. Larger dots indicate bigger penalty values. The mean square errors (MSE) on $x$ and $y$ are 0.2407 and 0.4784 respectively. The mean distance error $\sqrt  {(\mathaccentV {hat}05E{f}_x-x)^2+ (\mathaccentV {hat}05E{f}_y-y)^2}$ is 0.6458.\relax }}{45}}
\newlabel{2dxy}{{2.12}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Penalty function of 2-Dimensional reconstruction.\relax }}{45}}
\newlabel{2dpenalty}{{2.13}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusion and Discussion}{46}}
\citation{dieudonne2013foundations}
\citation{esl2009}
\citation{kim2004smoothing}
\citation{schoenberg1964spline}
\citation{hastie1990generalized}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Tractor Spline as Bayesian Estimation}{47}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterGPR}{{3}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{47}}
\newlabel{GaussianProcessGeneralObjective}{{3.1}{47}}
\citation{wahba1978improper}
\citation{speckman2003fully}
\citation{heckman1991minimax}
\citation{branson2017nonparametric}
\citation{cox1993analysis}
\citation{craven1978smoothing}
\citation{wecker1983signal}
\citation{gu1991minimizing}
\citation{wahba1990optimal}
\citation{hastie1990generalized}
\citation{wang1998smoothing}
\citation{judd1998numerical}
\citation{chen2009feedback}
\citation{ellis2009}
\citation{whittaker1922new}
\citation{kimeldorf1971some}
\citation{kimeldorf1970correspondence}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Polynomial Smoothing Splines on $[0, 1]$ as Bayes Estimates}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Polynomial Smoothing Spline}{49}}
\citation{aronszajn1950theory}
\citation{gu2013smoothing}
\citation{wahba1990spline}
\citation{berlinet2011reproducing}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Reproducing Kernel Hilbert Space $\mathcal  {H}^{(m)}[0,1]$}{50}}
\newlabel{GaussianProcessKernelR}{{3.5}{50}}
\newlabel{theoremRKHS}{{4}{50}}
\newlabel{theoremKernel}{{5}{50}}
\citation{gu2013smoothing}
\citation{gu2013smoothing}
\citation{rasmussen2006gaussian}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Polynomial Smoothing Spline as Bayes Estimates}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Gaussian Process Regression}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Tractor Spline as Bayes Estimate}{53}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Reproducing Kernel Hilbert Space $\mathcal  {C}_{\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip p.w.}^{2}[0,1]$}{53}}
\newlabel{maineq}{{3.12}{53}}
\newlabel{TractorSplineInnerProduct}{{3.13}{53}}
\newlabel{kerneleq}{{3.14}{53}}
\newlabel{GaussianProcessFunctionF}{{3.15}{54}}
\newlabel{GassianProcessRawequation}{{3.16}{55}}
\newlabel{matriteq}{{3.17}{55}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Posterior of Bayes Estimates}{55}}
\newlabel{GPrhoeq}{{3.19}{56}}
\newlabel{GPLemma}{{3}{56}}
\citation{wang1998smoothing}
\citation{diggle1989spline}
\citation{kohn1992nonparametric}
\citation{wang1998smoothing}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Tractor Spline with Correlated Random Errors}{57}}
\citation{syed2011review}
\citation{gu2013smoothing}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}A Numeric Simulation}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparing (trivial) Tractor spline and its Bayes estimate. Two methods are corresponding to each other. In this figure, black dots indicate observations and solid line stands for the reconstruction. The Tractor spline and its posterior $\mathrm  {E}(f(t) \mid \mathbf  {y}, \mathbf  {v})$ of Bayes estimate give the same results. \relax }}{60}}
\citation{cappe2009inference}
\citation{smcmip2011}
\citation{elliott1995estimation}
\citation{cargnoni1997bayesian}
\citation{vieira2016online}
\citation{hangos2006analysis}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}A Brief Overview of On-line State and Parameters Estimation}{61}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterFR}{{4}{61}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{61}}
\citation{petris2009dynamic}
\citation{petris2009dynamic}
\citation{kalman1960new}
\citation{de1988likelihood}
\newlabel{statemodel1}{{4.1}{62}}
\newlabel{statemodel2}{{4.2}{62}}
\citation{handschin1969monte}
\citation{handschin1970monte}
\citation{gordon1993novel}
\citation{cappe2009inference}
\citation{smcmip2011}
\citation{ristic2004beyond}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Filtering Problem and Estimation}{63}}
\newlabel{sectionFiltering}{{4.2}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Sequential Monte Carlo Method}{63}}
\citation{arulampalam2002tutorial}
\newlabel{rawParticleFilter}{{4.3}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Importance sampling}{65}}
\newlabel{PFexpectation}{{4.4}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Sequential Importance Sampling and Resampling}{66}}
\citation{smcmip2011}
\citation{andrieu2010particle}
\citation{andrieu1999sequential}
\citation{fearnhead2002markov}
\citation{storvik2002particle}
\citation{pitt1999filtering}
\citation{pitt1999filtering}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.1}{\ignorespaces Sampling and Importance Sampling.\relax }}{68}}
\newlabel{algorithmSIS}{{4.1}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Auxiliary Particle Filter}{68}}
\citation{gordon1993novel}
\citation{JOHANSEN20081498}
\citation{liu2008monte}
\citation{chopin2002sequential}
\citation{chopin2002sequential}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Sequential Particle Filter}{69}}
\citation{tierney1994markov}
\citation{septier2009mcmc}
\citation{berzuini1997dynamic}
\citation{khan2005mcmc}
\citation{golightly2006bayesian}
\citation{pang2008models}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.2}{\ignorespaces Sequential Particle Filter.\relax }}{70}}
\newlabel{algorithmSequentialPF}{{4.2}{70}}
\citation{septier2009mcmc}
\citation{pang2008models}
\citation{septier2009multiple}
\citation{cappe2007overview}
\citation{kantas2009overview}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}MCMC-Based Particle Algorithm}{71}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.3}{\ignorespaces MCMC-Based Particle Algorithm.\relax }}{72}}
\newlabel{algorithmMCMCParticle}{{4.3}{72}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}On-line State and Parameters Estimation}{72}}
\newlabel{sectionStateandPara}{{4.3}{72}}
\citation{higuchi2001self}
\citation{kitagawa1998self}
\citation{liu2001combined}
\citation{polson2008practical}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Artificial Dynamic Noise}{73}}
\newlabel{ArtificialNoise}{{4.3.1}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Practical Filtering}{73}}
\citation{polson2008practical}
\citation{kantas2009overview}
\citation{liu2001combined}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.4}{\ignorespaces Practical Filtering Algorithm.\relax }}{74}}
\newlabel{algorithmPraticalFilter}{{4.4}{74}}
\citation{west1993mixture}
\citation{liu2001combined}
\citation{storvik2002particle}
\citation{lopes2011particle}
\citation{storvik2002particle}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Liu and West's Filter}{75}}
\newlabel{LiuandWestDensity}{{4.10}{75}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Storvik Filter}{75}}
\citation{carvalho2010particle}
\citation{chen2000mixture}
\citation{vieira2016online}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.5}{\ignorespaces Liu and West's Filter.\relax }}{76}}
\newlabel{algorithmLWFilter}{{4.5}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Particle Learning}{76}}
\citation{chopin2010particle}
\citation{stroud2016bayesian}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.6}{\ignorespaces Storvik Filter.\relax }}{77}}
\newlabel{algorithmStFilter}{{4.6}{77}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.7}{\ignorespaces Particle Learning Algorithm.\relax }}{77}}
\newlabel{algorithmPL}{{4.7}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Adaptive Ensemble Kalman Filter}{77}}
\citation{kalman1960new}
\citation{evensen1994sequential}
\citation{katzfuss2016understanding}
\citation{stroud2016bayesian}
\citation{mitchell2000adaptive}
\newlabel{jointposterior}{{4.3.6}{78}}
\newlabel{jointposteriorterm1}{{4.12}{78}}
\newlabel{jointposteriorterm2}{{4.13}{78}}
\newlabel{ensembleKalmanForecast}{{4.14}{78}}
\newlabel{esembleKalmanLikeli}{{4.15}{78}}
\citation{anderson2001ensemble}
\citation{dempster1977maximum}
\@writefile{loa}{\contentsline {algocf}{\numberline {4.8}{\ignorespaces Adaptive Ensemble Kalman Filter.\relax }}{79}}
\newlabel{algorithmEnKF}{{4.8}{79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}On-line Pseudo-Likelihood Estimation}{79}}
\citation{kantas2009overview}
\citation{andrieu2005line}
\citation{andrieu2005line}
\citation{kantas2009overview}
\citation{andrieu2010particle}
\citation{liu2001combined}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Simulation Study}{80}}
\newlabel{sectionFilterreviewSimulation}{{4.4}{80}}
\citation{wakefield2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{82}}
\citation{wan2000unscented}
\citation{andrieu2010particle}
\citation{poyiadjis2005maximum}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Trace plots. Repeatedly running 50 times. Cutting off the first \textbf  {300} data. It is apparently that all these algorithms converge to the true parameter (black horizontal line) along time. St, PL and MCMC-vary have a narrower range. MCMC-100 has a higher variability and MCMC-vary has the least. The more data incorporated in the estimation phase the better approximation to be obtained. \relax }}{84}}
\newlabel{FilterRiewComparesion01}{{4.1}{84}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Box-plots comparison of all the algorithms. The proposed MCMC algorithm is more stable than other filters.\relax }}{85}}
\newlabel{FilterRiewComparesionTable}{{4.2}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The filtering for $x_{300:897}$ is very competitive. These algorithms return very close estimations. \relax }}{86}}
\newlabel{FilterRiewComparesion02}{{4.3}{86}}
\citation{vieira2016online}
\citation{kitagawa1998self}
\citation{liu2001combined}
\citation{storvik2002particle}
\citation{stroud2016bayesian}
\citation{carvalho2010particle}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Adaptive Sequential MCMC for On-line State and Parameters Estimation}{87}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterMCMC}{{5}{87}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{87}}
\citation{stroud2016bayesian}
\citation{polson2008practical}
\citation{haario1999adaptive}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Bayesian Inference on Combined State and Parameters}{88}}
\citation{hammersley1964percolation}
\citation{geweke1989bayesian}
\citation{casella2004generalized}
\citation{martino2010generalized}
\citation{geman1984stochastic}
\citation{metropolis1953equation}
\citation{hastings1970monte}
\newlabel{obserY}{{5.1}{89}}
\newlabel{hiddX}{{5.2}{89}}
\newlabel{objecfun}{{5.3}{89}}
\newlabel{M1}{{5.4}{89}}
\newlabel{M2}{{5.5}{89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Log-likelihood Function of Parameter Posterior}{90}}
\newlabel{sectionlogParameter}{{5.2.1}{90}}
\newlabel{generaljointmatrix}{{5.6}{90}}
\newlabel{sigmayy01}{{5.7}{91}}
\newlabel{sigmayy02}{{5.8}{91}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}The Forecast Distribution}{91}}
\newlabel{sectionforecast}{{5.2.2}{91}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}The Estimation Distribution}{92}}
\newlabel{generalEstDistr}{{5.2.3}{92}}
\newlabel{generalmux}{{5.13}{92}}
\newlabel{generalSigx}{{5.14}{92}}
\citation{smith1993bayesian}
\citation{tierney1994markov}
\citation{gilks1995markov}
\citation{muller1991generic}
\citation{tierney1994markov}
\citation{dongarra2000guest}
\citation{medova2008bayesian}
\newlabel{mixtureGaussian}{{5.15}{93}}
\newlabel{mixturemean}{{5.16}{93}}
\newlabel{mixturevariance}{{5.17}{93}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Random Walk Metropolis-Hastings algorithm}{93}}
\citation{metropolis1953equation}
\citation{sherlock2016adaptive}
\citation{sherlock2010random}
\newlabel{alphabalance}{{5.18}{94}}
\newlabel{stepsizeep}{{5.19}{94}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Self-tuning Metropolis-Hastings Algorithm}{94}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Examples of 2-Dimension Random Walk Metropolis-Hastings algorithm. Figure (a) is using one-variable-at-a-time proposal Random Walk. At each time, only one variable is changed and the other one stay constant. Figure (b) and (c) are using multi-variable-at-a-time Random Walk. The difference is in figure (b), every forward step are proposed independently, but in (c) are proposed according to the covariance matrix. \relax }}{95}}
\newlabel{randomwalk}{{5.1}{95}}
\citation{christen2005markov}
\newlabel{autostepab}{{5.22}{96}}
\newlabel{algoonevarible}{{10}{96}}
\newlabel{stRWMHselect}{{3}{96}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5.1}{\ignorespaces Self-tuning Random Walk Metropolis-Hastings Algorithm.\relax }}{96}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Adaptive Delayed Acceptance Metropolis-Hastings Algorithm}{96}}
\citation{sherlock2015efficiency}
\citation{stroud2016bayesian}
\citation{mathew2012bayesian}
\newlabel{dahalpha2}{{5.23}{97}}
\newlabel{dahalpha1}{{5.24}{97}}
\citation{sherlock2010random}
\citation{roberts2001optimal}
\citation{gelman1996efficient}
\citation{gilks1995markov}
\citation{roberts2001optimal}
\citation{roberts1997weak}
\citation{bedard2007weak}
\citation{beskos2009optimal}
\citation{sherlock2009optimal}
\citation{sherlock2013optimal}
\citation{sherlock2010random}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Efficiency of Metropolis-Hastings Algorithm}{98}}
\newlabel{effMHA}{{5.3.3}{98}}
\citation{andrieu2008tutorial}
\citation{sherlock2010random}
\citation{graves2011automatic}
\citation{roberts2001optimal}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Metropolis algorithm sampling for a single parameter with (a) a large step size, (b) a small step size, (c) an appropriate step size. The upper plots show the sample chain and lower plots indicate the autocorrelation for each case.\relax }}{99}}
\newlabel{largesmallstepsize}{{5.2}{99}}
\citation{roberts2001optimal}
\citation{kass1998markov}
\citation{robert2004monte}
\citation{gong2016practical}
\citation{geyer1992practical}
\citation{sokal1997monte}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Simulation Studies}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Simulation on Regular Time Series Data}{102}}
\citation{lopes2011particle}
\newlabel{linearlogL}{{5.29}{103}}
\newlabel{sectionlinearRecursive}{{5.4.1}{103}}
\citation{sherman1950adjustment}
\citation{woodbury1950inverting}
\citation{bartlett1951inverse}
\citation{bodewig1959matrix}
\citation{deng2011generalization}
\citation{bartlett1951inverse}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Linear simulation with true parameter $\theta = \{\phi =0.9,\tau ^2=0.5,\sigma ^2=1\}$. By transforming to original scale, the estimation is $\mathaccentV {hat}05E{\theta }=\{ \phi = 0.8810, \tau ^2 = 0.5247,\sigma ^2= 0.9416\}$. \relax }}{104}}
\newlabel{linearmarginplots}{{5.3}{104}}
\newlabel{beforeSMformula}{{5.30}{104}}
\newlabel{SMWformula}{{5.31}{104}}
\newlabel{SMformula}{{5.32}{104}}
\newlabel{linearmu}{{5.37}{105}}
\newlabel{linearsigma}{{5.38}{105}}
\citation{tandeo2011linear}
\citation{einstein1956investigations}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Linear simulation of $x_{1:t}$ and single $x_t$.In sub-figure (a), the dots is the true $x_{1:t}$ and the solid line is the estimation $\mathaccentV {hat}05E{x}_{1:t}$. In sub-figure (b), the estimation $\mathaccentV {hat}05E{x}_t$ is very close to the true $x$. In fact, the true $x$ falls in the interval $[\mathaccentV {hat}05E{x}-\varepsilon ,\mathaccentV {hat}05E{x}+\varepsilon ]$.\relax }}{106}}
\newlabel{linearmarginXt}{{5.4}{106}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Simulation on Irregular Time Series Data}{106}}
\citation{vaughan2015goodness}
\citation{kijima1997markov}
\newlabel{linearOUequation}{{5.41}{107}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Simulated data. The solid dots indicate the true state $x$ and cross dots indicate observation $y$. Irregular time lag $\Delta _t$ are generated from \textit  {Inverse Gamma}(2,0.1) distribution.\relax }}{108}}
\newlabel{simuOUreview}{{5.5}{108}}
\newlabel{simuOUlogL}{{5.45}{109}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Irregular time step OU process simulation. The estimation of $\mathaccentV {hat}05E{\theta }$ is $\{\gamma =0.4841, \lambda ^2=0.1032, \sigma ^2=0.9276\}$. In the plots, the horizontal dark lines are the true $\theta $. \relax }}{109}}
\newlabel{simuOUmarginplots}{{5.6}{109}}
\newlabel{linearOUK}{{5.46}{110}}
\newlabel{linearOUmu}{{5.48}{110}}
\newlabel{linearOUsigma}{{5.49}{110}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}High Dimensional OU-Process Application}{110}}
\newlabel{SectionHighDimensionalOU}{{5.5}{110}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Irregular time step OU process simulation of $x_{1:t}$ and sole $x_t$. In sub-figure (a), the dots is the true $x_{1:t}$ and the solid line is the estimation $\mathaccentV {hat}05E{x}_{1:t}$. In sub-figure (b), the chain in solid line is the estimation $\mathaccentV {hat}05E{x}_t$; dotted line is the true value of $x$; dot-dash line on top is the observed value of $y$; dashed lines are the estimated error. \relax }}{111}}
\newlabel{simuOUxt}{{5.7}{111}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The trajectory of a moving tractor. The time lags (right side figure) obtained from GPS units are irregular.\relax }}{111}}
\newlabel{realdatareview}{{5.8}{111}}
\newlabel{OUprocess}{{5.52}{111}}
\newlabel{obseq}{{5.56}{112}}
\newlabel{obmodel}{{5.57}{112}}
\newlabel{jointmatrix}{{5.58}{112}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Approximations of The Parameters Posterior}{114}}
\newlabel{logL}{{5.59}{114}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}The Forecast Distribution}{114}}
\newlabel{OUupdatingK}{{5.61}{115}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}The Estimation Distribution}{116}}
\citation{lindley1972bayes}
\citation{smith1973general}
\citation{gelman2006prior}
\citation{jeffries1961theory}
\citation{jaynes1983papers}
\citation{box2011bayesian}
\citation{gelman2008weakly}
\citation{stroud2007sequential}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Prior Distribution for Parameters}{117}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Probability density function and cumulative distribution function of \textit  {Inverse Gamma} with two parameters $\alpha $ and $\beta $. \relax }}{118}}
\newlabel{IGPDFCDF}{{5.9}{118}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Efficiency of DA-MH}{119}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces An example of Eff, EUT, ESS and ESSUT found by running 10\tmspace  +\thinmuskip {.1667em}000 iterations with same data. The computation time is measured in seconds $s$. \relax }}{119}}
\newlabel{effeutessessutexampletable}{{5.1}{119}}
\citation{polson2008practical}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Influences of different step sizes on sampling efficiency (Eff), efficiency in unit time (EUT), effective sample size (ESS) and effective sample size in unit time (ESSUT) found by using the same data\relax }}{120}}
\newlabel{effeutessessutexamplefigure}{{5.10}{120}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.6}Sliding Window State and Parameter Estimation}{120}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Comparing Eff, EUT, ESS and ESSUT values using different step size. The $1000^\star $ means taking 1\tmspace  +\thinmuskip {.1667em}000 samples from a longer chain, like 1\tmspace  +\thinmuskip {.1667em}000 out of 5\tmspace  +\thinmuskip {.1667em}000 sample chain. The computation time is measured in seconds $s$.\relax }}{121}}
\newlabel{stepsizecompare}{{5.2}{121}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Comparison efficiency (Eff), efficiency in unit time (EffUT), effective sample saize (ESS) and effective sample size in unit time (ESSUT) against different length of data. Increasing data length doesn't significantly improve the efficiency and ESS in unit time.\relax }}{122}}
\newlabel{compareLengthData}{{5.11}{122}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.7}Implementation}{123}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Comparison $\qopname  \relax o{ln}DA$ and $\qopname  \relax o{ln}L$ surfaces between not-updating-mean and updating-mean methods. It is obviously that the updating-mean method has dense log-surfaces indicating more effective samples.\relax }}{124}}
\newlabel{comparenotanupDAL}{{5.12}{124}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Comparison of acceptance rates $\alpha _1$, $\alpha _2$, EffUT and ESSUT between not-updating-mean and updating-mean methods. Black solid dots $\bullet $ indicate values obtained from not-updating-mean method and black solid triangular $\blacktriangle $ indicate values obtained from updating-mean method. The acceptance rates of the updating-mean method are more stable and effective samples are larger in unit computation time. \relax }}{125}}
\newlabel{comparenotanupfeatures}{{5.13}{125}}
\newlabel{algorithmlearningsurface}{{2}{126}}
\newlabel{algorithmestimaiton}{{3}{126}}
\newlabel{algorithmDA}{{4}{126}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5.2}{\ignorespaces Sliding Window MCMC.\relax }}{126}}
\newlabel{algorithmslidingwindow}{{5.2}{126}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Visualization of the parameters correlation matrix, which is found in learning phase. \relax }}{127}}
\newlabel{realdatacorMatrix}{{5.14}{127}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Trace plots of $\theta $ from learning phase after taking 1\tmspace  +\thinmuskip {.1667em}000 burn-in samples out from 5\tmspace  +\thinmuskip {.1667em}000. \relax }}{128}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Position and velocity for $X$ and $Y$ found by combined batch and sequential methods. \relax }}{129}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Zoom in on estimations. For each estimation $\mathaccentV {hat}05E{X}_i (i=1,\dots  ,t)$, there is a error circle around it. \relax }}{129}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Discussion and Future Work}{130}}
\citation{freund1995desicion}
\citation{friedman2001greedy}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Work}{131}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterFuture}{{6}{131}}
\citation{babenko2009family}
\citation{beygelzimer2015online}
\citation{chen2016xgboost}
\citation{opsomer2001nonparametric}
\citation{wang1998smoothing}
\citation{ristic2004beyond}
\citation{stroud2016bayesian}
\citation{arulampalam2002tutorial}
\citation{hartmann2016grid}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Summary}{135}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ChapterSummary}{{7}{135}}
\@writefile{toc}{\contentsline {chapter}{Appendices}{139}}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\begingroup \let \l@chapter \l@section \let \l@section \l@subsection }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Proofs of Tractor Spline Theorems}{140}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Penalty Matrix in (2.16\hbox {})}{140}}
\newlabel{PenaltyTermDetails}{{A.1}{140}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Proof of Theorem 2\hbox {}}{141}}
\newlabel{matrixD}{{A.2.1}{141}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Proof of Lemma 2\hbox {}}{144}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Proof of Theorem 3\hbox {}}{145}}
\newlabel{th3proofeq1}{{A.4.1}{145}}
\newlabel{th3proofeq2}{{A.4.3}{145}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Calculations And Figures of Adaptive Sequential MCMC}{146}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Linear Simulation Calculations}{146}}
\newlabel{linearcalculation}{{B.1}{146}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}OU-Process Calculation}{149}}
\newlabel{OUcalculation}{{B.2}{149}}
\newlabel{OUKtp1}{{B.2.2}{151}}
\newlabel{recursiveKp1}{{B.2.4}{152}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Covariance Matrix in Details}{155}}
\newlabel{covMatrixdetails}{{B.3}{155}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Real Data Implementation}{157}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Running the same amount of time and taking the same length of data, the step size $\epsilon =2.5$ returns the highest ESSUT value and generates more effective samples with a lower correlation. \relax }}{158}}
\newlabel{1koutof8kfigures}{{B.1}{158}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Impacts of data length on optimal parameter. There is a obvious trend on the estimation against length of data in estimation process. \relax }}{159}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Parameter estimation by running the whole surface learning and DA-MH processes with different length of data\relax }}{160}}
\newlabel{lengthofdatacompare}{{B.4}{161}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Comparison Between Batch and Sliding Window Methods}{161}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Comparison of $\alpha _1$, $\alpha _2$, EffUT and ESSUT between batch MCMC (orange) and sliding window MCMC (green). \relax }}{161}}
\newlabel{batchwindowkeyfeature}{{B.3}{161}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Comparison of parameters estimation between batch MCMC (orange) and sliding window MCMC (green). \relax }}{162}}
\newlabel{batchwindowparameter}{{B.4}{162}}
\bibstyle{otago}
\bibdata{thesis}
\@writefile{toc}{\contentsline {section}{\numberline {B.6}Parameter Evolution Visualization}{163}}
\@writefile{toc}{\endgroup }
\@writefile{toc}{\contentsline {chapter}{\hbox to\@tempdima {\hfil }References}{163}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Parameters Evolution Visualization. The correlation among parameters doesn't change two much. The parameters are considered static. \relax }}{166}}
\bibcite{abramovich1998wavelet}{{1}{1998}{{Abramovich {\em  et~al.}}}{{Abramovich, Sapatinas, and Silverman}}}
\bibcite{agarwal2003indexing}{{2}{2003}{{Agarwal {\em  et~al.}}}{{Agarwal, Arge, and Erickson}}}
\bibcite{anderson1979optimal}{{3}{1979}{{Anderson and Moore}}{{Anderson and Moore}}}
\bibcite{anderson2001ensemble}{{4}{2001}{{Anderson}}{{Anderson}}}
\bibcite{andrieu1999sequential}{{5}{1999}{{Andrieu {\em  et~al.}}}{{Andrieu, De~Freitas, and Doucet}}}
\bibcite{andrieu2001model}{{6}{2001}{{Andrieu {\em  et~al.}}}{{Andrieu, Djuri{\'c}, and Doucet}}}
\bibcite{andrieu2010particle}{{7}{2010}{{Andrieu {\em  et~al.}}}{{Andrieu, Doucet, and Holenstein}}}
\bibcite{andrieu2005line}{{8}{2005}{{Andrieu {\em  et~al.}}}{{Andrieu, Doucet, and Tadic}}}
\bibcite{andrieu2008tutorial}{{9}{2008}{{Andrieu and Thoms}}{{Andrieu and Thoms}}}
\bibcite{arlot2010survey}{{10}{2010}{{Arlot {\em  et~al.}}}{{Arlot, Celisse, {\em  et~al.}}}}
\bibcite{smcmip2011}{{11}{2011}{{Arnaud~Doucet}}{{Arnaud~Doucet}}}
\bibcite{aronszajn1950theory}{{12}{1950}{{Aronszajn}}{{Aronszajn}}}
\bibcite{arulampalam2002tutorial}{{13}{2002}{{Arulampalam {\em  et~al.}}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{atchade2009adaptive}{{14}{2009}{{Atchade {\em  et~al.}}}{{Atchade, Fort, Moulines, and Priouret}}}
\bibcite{Atkinson2004}{{15}{2004}{{Atkinson}}{{Atkinson}}}
\bibcite{aydin2013smoothing}{{16}{2013}{{Ayd{\i }n {\em  et~al.}}}{{Ayd{\i }n, Memmedli, and Omay}}}
\bibcite{aydin2012smoothing}{{17}{2012}{{Aydin and Tuzemen}}{{Aydin and Tuzemen}}}
\bibcite{babenko2009family}{{18}{2009}{{Babenko {\em  et~al.}}}{{Babenko, Yang, and Belongie}}}
\bibcite{bajaj2002gps}{{19}{2002}{{Bajaj {\em  et~al.}}}{{Bajaj, Ranaweera, and Agrawal}}}
\bibcite{bar1993estimation}{{20}{1993}{{Bar-Shalom and Li}}{{Bar-Shalom and Li}}}
\bibcite{bartlett1951inverse}{{21}{1951}{{Bartlett}}{{Bartlett}}}
\bibcite{bedard2007weak}{{22}{2007}{{B{\'e}dard}}{{B{\'e}dard}}}
\bibcite{ben2004geometric}{{23}{2004}{{Ben-Arieh {\em  et~al.}}}{{Ben-Arieh, Chang, Rys, and Zhang}}}
\bibcite{berlinet2011reproducing}{{24}{2011}{{Berlinet and Thomas-Agnan}}{{Berlinet and Thomas-Agnan}}}
\bibcite{berzuini1997dynamic}{{25}{1997}{{Berzuini {\em  et~al.}}}{{Berzuini, Best, Gilks, and Larizza}}}
\bibcite{beskos2009optimal}{{26}{2009}{{Beskos {\em  et~al.}}}{{Beskos, Roberts, and Stuart}}}
\bibcite{betancourt2017conceptual}{{27}{2017}{{Betancourt}}{{Betancourt}}}
\bibcite{beygelzimer2015online}{{28}{2015}{{Beygelzimer {\em  et~al.}}}{{Beygelzimer, Hazan, Kale, and Luo}}}
\bibcite{biagiotti2013online}{{29}{2013}{{Biagiotti and Melchiorri}}{{Biagiotti and Melchiorri}}}
\bibcite{bierkens2017limit}{{30}{2017}{{Bierkens and Duncan}}{{Bierkens and Duncan}}}
\bibcite{bierkens2016zig}{{31}{2016a}{{Bierkens {\em  et~al.}}}{{Bierkens, Fearnhead, and Roberts}}}
\bibcite{bierkens2016piecewise}{{32}{2016b}{{Bierkens {\em  et~al.}}}{{Bierkens, Roberts, {\em  et~al.}}}}
\bibcite{bishop2001introduction}{{33}{2001}{{Bishop and Welch}}{{Bishop and Welch}}}
\bibcite{blaauw2011flexible}{{34}{2011}{{Blaauw {\em  et~al.}}}{{Blaauw, Christen, {\em  et~al.}}}}
\bibcite{box2011bayesian}{{35}{2011}{{Box and Tiao}}{{Box and Tiao}}}
\bibcite{branson2017nonparametric}{{36}{2017}{{Branson {\em  et~al.}}}{{Branson, Rischard, Bornn, and Miratrix}}}
\bibcite{cantoni2001resistant}{{37}{2001}{{Cantoni and Ronchetti}}{{Cantoni and Ronchetti}}}
\bibcite{cappe2007overview}{{38}{2007}{{Capp{\'e} {\em  et~al.}}}{{Capp{\'e}, Godsill, and Moulines}}}
\bibcite{cappe2009inference}{{39}{2009}{{Capp{\'e} {\em  et~al.}}}{{Capp{\'e}, Moulines, and Ryd{\'e}n}}}
\bibcite{cargnoni1997bayesian}{{40}{1997}{{Cargnoni {\em  et~al.}}}{{Cargnoni, M{\"u}ller, and West}}}
\bibcite{carlin1992monte}{{41}{1992}{{Carlin {\em  et~al.}}}{{Carlin, Polson, and Stoffer}}}
\bibcite{carpenter1999improved}{{42}{1999}{{Carpenter {\em  et~al.}}}{{Carpenter, Clifford, and Fearnhead}}}
\bibcite{carvalho2010particle}{{43}{2010}{{Carvalho {\em  et~al.}}}{{Carvalho, Johannes, Lopes, Polson, {\em  et~al.}}}}
\bibcite{casella2004generalized}{{44}{2004}{{Casella {\em  et~al.}}}{{Casella, Robert, and Wells}}}
\bibcite{castro2006geometric}{{45}{2006}{{Castro {\em  et~al.}}}{{Castro, Iglesias, Rodr{\'\i }guez-Solano, and S{\'a}nchez}}}
\bibcite{chadil2008real}{{46}{2008}{{Chadil {\em  et~al.}}}{{Chadil, Russameesawang, and Keeratiwintakorn}}}
\bibcite{chandrasekar2007comparison}{{47}{2007}{{Chandrasekar {\em  et~al.}}}{{Chandrasekar, Ridley, and Bernstein}}}
\bibcite{chen2012monte}{{48}{2012}{{Chen {\em  et~al.}}}{{Chen, Shao, and Ibrahim}}}
\bibcite{chen2000mixture}{{49}{2000}{{Chen and Liu}}{{Chen and Liu}}}
\bibcite{chen2016xgboost}{{50}{2016}{{Chen and Guestrin}}{{Chen and Guestrin}}}
\bibcite{chen2009feedback}{{51}{2009}{{Chen}}{{Chen}}}
\bibcite{chen2003bayesian}{{52}{2003}{{Chen {\em  et~al.}}}{{Chen {\em  et~al.}}}}
\bibcite{chopin2002sequential}{{53}{2002}{{Chopin}}{{Chopin}}}
\bibcite{chopin2010particle}{{54}{2010}{{Chopin {\em  et~al.}}}{{Chopin, Iacobucci, Marin, Mengersen, Robert, Ryder, and Sch{\"a}fer}}}
\bibcite{christen2005markov}{{55}{2005}{{Christen and Fox}}{{Christen and Fox}}}
\bibcite{christen2010general}{{56}{2010}{{Christen {\em  et~al.}}}{{Christen, Fox, {\em  et~al.}}}}
\bibcite{cox1993analysis}{{57}{1993}{{Cox}}{{Cox}}}
\bibcite{cox1982practical}{{58}{1982}{{Cox}}{{Cox}}}
\bibcite{craven1978smoothing}{{59}{1978}{{Craven and Wahba}}{{Craven and Wahba}}}
\bibcite{de1978practical}{{60}{1978}{{De~Boor {\em  et~al.}}}{{De~Boor, De~Boor, Math{\'e}maticien, De~Boor, and De~Boor}}}
\bibcite{de1988likelihood}{{61}{1988}{{De~Jong}}{{De~Jong}}}
\bibcite{dempster1977maximum}{{62}{1977}{{Dempster {\em  et~al.}}}{{Dempster, Laird, and Rubin}}}
\bibcite{deng2011generalization}{{63}{2011}{{Deng}}{{Deng}}}
\bibcite{dierckx1995curve}{{64}{1995}{{Dierckx}}{{Dierckx}}}
\bibcite{dieudonne2013foundations}{{65}{2013}{{Dieudonn{\'e}}}{{Dieudonn{\'e}}}}
\bibcite{diggle1989spline}{{66}{1989}{{Diggle and Hutchinson}}{{Diggle and Hutchinson}}}
\bibcite{dongarra2000guest}{{67}{2000}{{Dongarra and Sullivan}}{{Dongarra and Sullivan}}}
\bibcite{donoho1995adapting}{{68}{1995}{{Donoho and Johnstone}}{{Donoho and Johnstone}}}
\bibcite{donoho1995wavelet}{{69}{1995}{{Donoho {\em  et~al.}}}{{Donoho, Johnstone, Kerkyacharian, and Picard}}}
\bibcite{donoho1994ideal}{{70}{1994}{{Donoho and Johnstone}}{{Donoho and Johnstone}}}
\bibcite{doucet2000rao}{{71}{2000}{{Doucet {\em  et~al.}}}{{Doucet, De~Freitas, Murphy, and Russell}}}
\bibcite{doucet2000sequential}{{72}{2000}{{Doucet {\em  et~al.}}}{{Doucet, Godsill, and Andrieu}}}
\bibcite{doucet2009tutorial}{{73}{2009}{{Doucet and Johansen}}{{Doucet and Johansen}}}
\bibcite{duane1987hybrid}{{74}{1987}{{Duane {\em  et~al.}}}{{Duane, Kennedy, Pendleton, and Roweth}}}
\bibcite{dubins1957curves}{{75}{1957}{{Dubins}}{{Dubins}}}
\bibcite{durbin2012time}{{76}{2012}{{Durbin and Koopman}}{{Durbin and Koopman}}}
\bibcite{eilers1996flexible}{{77}{1996}{{Eilers and Marx}}{{Eilers and Marx}}}
\bibcite{einstein1956investigations}{{78}{1956}{{Einstein}}{{Einstein}}}
\bibcite{ellis2009}{{79}{2009}{{Ellis {\em  et~al.}}}{{Ellis, Sommerlade, and Reid}}}
\bibcite{erkorkmaz2001high}{{80}{2001}{{Erkorkmaz and Altintas}}{{Erkorkmaz and Altintas}}}
\bibcite{eubank2004simple}{{81}{2004}{{Eubank}}{{Eubank}}}
\bibcite{evensen1994sequential}{{82}{1994}{{Evensen}}{{Evensen}}}
\bibcite{fearnhead2002markov}{{83}{2002}{{Fearnhead}}{{Fearnhead}}}
\bibcite{freund1995desicion}{{84}{1995}{{Freund and Schapire}}{{Freund and Schapire}}}
\bibcite{friedman2001greedy}{{85}{2001}{{Friedman}}{{Friedman}}}
\bibcite{garcia1996multivariate}{{86}{1996}{{Garc{\'\i }a-Cort{\'e}s and Sorensen}}{{Garc{\'\i }a-Cort{\'e}s and Sorensen}}}
\bibcite{gasparetto2007new}{{87}{2007}{{Gasparetto and Zanotto}}{{Gasparetto and Zanotto}}}
\bibcite{gelb1974applied}{{88}{1974}{{Gelb}}{{Gelb}}}
\bibcite{gelfand1990illustration}{{89}{1990}{{Gelfand {\em  et~al.}}}{{Gelfand, Hills, Racine-Poon, and Smith}}}
\bibcite{gelman2006prior}{{90}{2006}{{Gelman {\em  et~al.}}}{{Gelman {\em  et~al.}}}}
\bibcite{gelman2008weakly}{{91}{2008}{{Gelman {\em  et~al.}}}{{Gelman, Jakulin, Pittau, and Su}}}
\bibcite{gelman1996efficient}{{92}{1996}{{Gelman {\em  et~al.}}}{{Gelman, Roberts, Gilks, {\em  et~al.}}}}
\bibcite{geman1984stochastic}{{93}{1984}{{Geman and Geman}}{{Geman and Geman}}}
\bibcite{geweke1989bayesian}{{94}{1989}{{Geweke}}{{Geweke}}}
\bibcite{geyer1992practical}{{95}{1992}{{Geyer}}{{Geyer}}}
\bibcite{gilks1995markov}{{96}{1995}{{Gilks {\em  et~al.}}}{{Gilks, Richardson, and Spiegelhalter}}}
\bibcite{gloderer2010spline}{{97}{2010}{{Gloderer and Hertle}}{{Gloderer and Hertle}}}
\bibcite{godsill2000methodology}{{98}{2000}{{Godsill {\em  et~al.}}}{{Godsill, Doucet, and West}}}
\bibcite{godsill2001maximum}{{99}{2001}{{Godsill {\em  et~al.}}}{{Godsill, Doucet, and West}}}
\bibcite{golightly2006bayesian}{{100}{2006}{{Golightly and Wilkinson}}{{Golightly and Wilkinson}}}
\bibcite{gong2016practical}{{101}{2016}{{Gong and Flegal}}{{Gong and Flegal}}}
\bibcite{gordon1993novel}{{102}{1993}{{Gordon {\em  et~al.}}}{{Gordon, Salmond, and Smith}}}
\bibcite{graves2011automatic}{{103}{2011}{{Graves}}{{Graves}}}
\bibcite{green1995reversible}{{104}{1995}{{Green}}{{Green}}}
\bibcite{green1993nonparametric}{{105}{1993}{{Green and Silverman}}{{Green and Silverman}}}
\bibcite{gu1998model}{{106}{1998}{{Gu}}{{Gu}}}
\bibcite{gu2013smoothing}{{107}{2013}{{Gu}}{{Gu}}}
\bibcite{gu1991minimizing}{{108}{1991}{{Gu and Wahba}}{{Gu and Wahba}}}
\bibcite{gyorgy2014unscented}{{109}{2014}{{Gy{\"o}rgy {\em  et~al.}}}{{Gy{\"o}rgy, Kelemen, and D{\'a}vid}}}
\bibcite{haario1999adaptive}{{110}{1999}{{Haario {\em  et~al.}}}{{Haario, Saksman, and Tamminen}}}
\bibcite{haario2001adaptive}{{111}{2001}{{Haario {\em  et~al.}}}{{Haario, Saksman, Tamminen, {\em  et~al.}}}}
\bibcite{hammersley1964percolation}{{112}{1964}{{Hammersley and Handscomb}}{{Hammersley and Handscomb}}}
\bibcite{handschin1970monte}{{113}{1970}{{Handschin}}{{Handschin}}}
\bibcite{handschin1969monte}{{114}{1969}{{Handschin and Mayne}}{{Handschin and Mayne}}}
\bibcite{hangos2006analysis}{{115}{2006}{{Hangos {\em  et~al.}}}{{Hangos, Bokor, and Szederk{\'e}nyi}}}
\bibcite{hardle1990applied}{{116}{1990}{{H{\"a}rdle}}{{H{\"a}rdle}}}
\bibcite{hardle1988far}{{117}{1988}{{H{\"a}rdle {\em  et~al.}}}{{H{\"a}rdle, Hall, and Marron}}}
\bibcite{hartmann2016grid}{{118}{2016}{{Hartmann {\em  et~al.}}}{{Hartmann, Nowak, Pfandenhauer, Thielecke, and Heuberger}}}
\bibcite{hastie1990generalized}{{119}{1990}{{Hastie and Tibshirani}}{{Hastie and Tibshirani}}}
\bibcite{hastings1970monte}{{120}{1970}{{Hastings}}{{Hastings}}}
\bibcite{haykin2001kalman}{{121}{2001}{{Haykin {\em  et~al.}}}{{Haykin {\em  et~al.}}}}
\bibcite{heckman1991minimax}{{122}{1991}{{Heckman and Woodroofe}}{{Heckman and Woodroofe}}}
\bibcite{hightower2001location}{{123}{2001}{{Hightower and Borriello}}{{Hightower and Borriello}}}
\bibcite{higuchi2001self}{{124}{2001}{{Higuchi}}{{Higuchi}}}
\bibcite{hurvich1998smoothing}{{125}{1998}{{Hurvich {\em  et~al.}}}{{Hurvich, Simonoff, and Tsai}}}
\bibcite{jauch2017recursive}{{126}{2017}{{Jauch {\em  et~al.}}}{{Jauch, Bleimund, Rhode, and Gauterin}}}
\bibcite{jaynes1983papers}{{127}{1983}{{Jaynes}}{{Jaynes}}}
\bibcite{jeffries1961theory}{{128}{1961}{{Jeffries}}{{Jeffries}}}
\bibcite{JOHANSEN20081498}{{129}{2008}{{Johansen and Doucet}}{{Johansen and Doucet}}}
\bibcite{judd1998numerical}{{130}{1998}{{Judd}}{{Judd}}}
\bibcite{julier1997new}{{131}{1997}{{Julier and Uhlmann}}{{Julier and Uhlmann}}}
\bibcite{kailath1981lectures}{{132}{1981}{{Kailath}}{{Kailath}}}
\bibcite{kalman1960new}{{133}{1960}{{Kalman {\em  et~al.}}}{{Kalman {\em  et~al.}}}}
\bibcite{kalos2008monte}{{134}{2008}{{Kalos and Whitlock}}{{Kalos and Whitlock}}}
\bibcite{kantas2009overview}{{135}{2009}{{Kantas {\em  et~al.}}}{{Kantas, Doucet, Singh, and Maciejowski}}}
\bibcite{kaplan2005understanding}{{136}{2005}{{Kaplan and Hegarty}}{{Kaplan and Hegarty}}}
\bibcite{kass1998markov}{{137}{1998}{{Kass {\em  et~al.}}}{{Kass, Carlin, Gelman, and Neal}}}
\bibcite{katzfuss2016understanding}{{138}{2016}{{Katzfuss {\em  et~al.}}}{{Katzfuss, Stroud, and Wikle}}}
\bibcite{khan2005mcmc}{{139}{2005}{{Khan {\em  et~al.}}}{{Khan, Balch, and Dellaert}}}
\bibcite{kijima1997markov}{{140}{1997}{{Kijima}}{{Kijima}}}
\bibcite{kim2004smoothing}{{141}{2004}{{Kim and Gu}}{{Kim and Gu}}}
\bibcite{kimeldorf1971some}{{142}{1971}{{Kimeldorf and Wahba}}{{Kimeldorf and Wahba}}}
\bibcite{kimeldorf1970correspondence}{{143}{1970}{{Kimeldorf and Wahba}}{{Kimeldorf and Wahba}}}
\bibcite{kitagawa1998self}{{144}{1998}{{Kitagawa}}{{Kitagawa}}}
\bibcite{kloek1978bayesian}{{145}{1978}{{Kloek and Van~Dijk}}{{Kloek and Van~Dijk}}}
\bibcite{kohn1992nonparametric}{{146}{1992}{{Kohn {\em  et~al.}}}{{Kohn, Ansley, and Wong}}}
\bibcite{komoriya1989trajectory}{{147}{1989}{{Komoriya and Tanie}}{{Komoriya and Tanie}}}
\bibcite{kong1994sequential}{{148}{1994}{{Kong {\em  et~al.}}}{{Kong, Liu, and Wong}}}
\bibcite{krivobokova2008fast}{{149}{2008}{{Krivobokova {\em  et~al.}}}{{Krivobokova, Crainiceanu, and Kauermann}}}
\bibcite{larson1931shrinkage}{{150}{1931}{{Larson}}{{Larson}}}
\bibcite{laviola2003comparison}{{151}{2003}{{LaViola}}{{LaViola}}}
\bibcite{lindley1972bayes}{{152}{1972}{{Lindley and Smith}}{{Lindley and Smith}}}
\bibcite{liu2001combined}{{153}{2001}{{Liu and West}}{{Liu and West}}}
\bibcite{liu2008monte}{{154}{2008}{{Liu}}{{Liu}}}
\bibcite{liu2000multiple}{{155}{2000}{{Liu {\em  et~al.}}}{{Liu, Liang, and Wong}}}
\bibcite{liu2014filtering}{{156}{2014}{{Liu {\em  et~al.}}}{{Liu, Suo, Karimi, and Liu}}}
\bibcite{liu2010data}{{157}{2010}{{Liu and Guo}}{{Liu and Guo}}}
\bibcite{lopes2011particle}{{158}{2011}{{Lopes and Tsay}}{{Lopes and Tsay}}}
\bibcite{mackay2003information}{{159}{2003}{{MacKay}}{{MacKay}}}
\bibcite{magid2006spline}{{160}{2006}{{Magid {\em  et~al.}}}{{Magid, Keren, Rivlin, and Yavneh}}}
\bibcite{mahendran2012adaptive}{{161}{2012}{{Mahendran {\em  et~al.}}}{{Mahendran, Wang, Hamze, and De~Freitas}}}
\bibcite{martino2010generalized}{{162}{2010}{{Martino and M{\'\i }guez}}{{Martino and M{\'\i }guez}}}
\bibcite{mathew2012bayesian}{{163}{2012}{{Mathew {\em  et~al.}}}{{Mathew, Bauer, Koistinen, Reetz, L{\'e}on, and Sillanp{\"a}{\"a}}}}
\bibcite{mcdonald2006intelligent}{{164}{2006}{{McDonald}}{{McDonald}}}
\bibcite{medova2008bayesian}{{165}{2008}{{Medova}}{{Medova}}}
\bibcite{metropolis1953equation}{{166}{1953}{{Metropolis {\em  et~al.}}}{{Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller}}}
\bibcite{mitchell2000adaptive}{{167}{2000}{{Mitchell and Houtekamer}}{{Mitchell and Houtekamer}}}
\bibcite{muller1991generic}{{168}{1991}{{M{\"u}ller}}{{M{\"u}ller}}}
\bibcite{nason2010wavelet}{{169}{2010}{{Nason}}{{Nason}}}
\bibcite{neal2011mcmc}{{170}{2011}{{Neal {\em  et~al.}}}{{Neal {\em  et~al.}}}}
\bibcite{nelder1965simplex}{{171}{1965}{{Nelder and Mead}}{{Nelder and Mead}}}
\bibcite{opsomer2001nonparametric}{{172}{2001}{{Opsomer {\em  et~al.}}}{{Opsomer, Wang, and Yang}}}
\bibcite{oussalah2001adaptive}{{173}{2001}{{Oussalah and De~Schutter}}{{Oussalah and De~Schutter}}}
\bibcite{pang2008models}{{174}{2008}{{Pang {\em  et~al.}}}{{Pang, Li, and Godsill}}}
\bibcite{petris2009dynamic}{{175}{2009}{{Petris {\em  et~al.}}}{{Petris, Petrone, and Campagnoli}}}
\bibcite{pitt1999filtering}{{176}{1999}{{Pitt and Shephard}}{{Pitt and Shephard}}}
\bibcite{polson2008practical}{{177}{2008}{{Polson {\em  et~al.}}}{{Polson, Stroud, and M{\"u}ller}}}
\bibcite{poyiadjis2005maximum}{{178}{2005}{{Poyiadjis {\em  et~al.}}}{{Poyiadjis, Doucet, and Singh}}}
\bibcite{ran2010self}{{179}{2010}{{Ran and Deng}}{{Ran and Deng}}}
\bibcite{rasmussen2006gaussian}{{180}{2006}{{Rasmussen and Williams}}{{Rasmussen and Williams}}}
\bibcite{rhodes1971tutorial}{{181}{1971}{{Rhodes}}{{Rhodes}}}
\bibcite{ristic2004beyond}{{182}{2004}{{Ristic {\em  et~al.}}}{{Ristic, Arulampalam, and Gordon}}}
\bibcite{robert2004monte}{{183}{2004}{{Robert}}{{Robert}}}
\bibcite{elliott1995estimation}{{184}{1995}{{Robert J~Elliott}}{{Robert J~Elliott}}}
\bibcite{roberts1997weak}{{185}{1997}{{Roberts {\em  et~al.}}}{{Roberts, Gelman, Gilks, {\em  et~al.}}}}
\bibcite{roberts2009examples}{{186}{2009}{{Roberts and Rosenthal}}{{Roberts and Rosenthal}}}
\bibcite{roberts2001optimal}{{187}{2001}{{Roberts {\em  et~al.}}}{{Roberts, Rosenthal, {\em  et~al.}}}}
\bibcite{rubin2004multiple}{{188}{2004}{{Rubin}}{{Rubin}}}
\bibcite{ruppert2003semiparametric}{{189}{2003}{{Ruppert {\em  et~al.}}}{{Ruppert, Wand, and Carroll}}}
\bibcite{sarkka2013bayesian}{{190}{2013}{{S{\"a}rkk{\"a}}}{{S{\"a}rkk{\"a}}}}
\bibcite{schoenberg1964spline}{{191}{1964}{{Schoenberg}}{{Schoenberg}}}
\bibcite{schwarz2012geodesy}{{192}{2012}{{Schwarz}}{{Schwarz}}}
\bibcite{sealfon2005smoothing}{{193}{2005}{{Sealfon {\em  et~al.}}}{{Sealfon, Verde, and Jimenez}}}
\bibcite{septier2009multiple}{{194}{2009}{{Septier {\em  et~al.}}}{{Septier, Carmi, Pang, and Godsill}}}
\bibcite{septier2009mcmc}{{195}{2009}{{Septier {\em  et~al.}}}{{Septier, Pang, Carmi, and Godsill}}}
\bibcite{sherlock2013optimal}{{196}{2013}{{Sherlock}}{{Sherlock}}}
\bibcite{sherlock2010random}{{197}{2010}{{Sherlock {\em  et~al.}}}{{Sherlock, Fearnhead, and Roberts}}}
\bibcite{sherlock2016adaptive}{{198}{2016}{{Sherlock {\em  et~al.}}}{{Sherlock, Golightly, and Henderson}}}
\bibcite{sherlock2009optimal}{{199}{2009}{{Sherlock {\em  et~al.}}}{{Sherlock, Roberts, {\em  et~al.}}}}
\bibcite{sherlock2015efficiency}{{200}{2015}{{Sherlock {\em  et~al.}}}{{Sherlock, Thiery, and Golightly}}}
\bibcite{sherman1950adjustment}{{201}{1950}{{Sherman and Morrison}}{{Sherman and Morrison}}}
\bibcite{silverman1985some}{{202}{1985}{{Silverman}}{{Silverman}}}
\bibcite{smith1973general}{{203}{1973}{{Smith}}{{Smith}}}
\bibcite{smith1993bayesian}{{204}{1993}{{Smith and Roberts}}{{Smith and Roberts}}}
\bibcite{sokal1997monte}{{205}{1997}{{Sokal}}{{Sokal}}}
\bibcite{sorensen2007likelihood}{{206}{2007}{{Sorensen and Gianola}}{{Sorensen and Gianola}}}
\bibcite{sorenson1985kalman}{{207}{1985}{{Sorenson}}{{Sorenson}}}
\bibcite{speckman2003fully}{{208}{2003}{{Speckman and Sun}}{{Speckman and Sun}}}
\bibcite{st2004comparison}{{209}{2004}{{St-Pierre and Gingras}}{{St-Pierre and Gingras}}}
\bibcite{stavropoulos2001improved}{{210}{2001}{{Stavropoulos and Titterington}}{{Stavropoulos and Titterington}}}
\bibcite{storvik2002particle}{{211}{2002}{{Storvik}}{{Storvik}}}
\bibcite{stroud2007sequential}{{212}{2007}{{Stroud and Bengtsson}}{{Stroud and Bengtsson}}}
\bibcite{stroud2016bayesian}{{213}{2016}{{Stroud {\em  et~al.}}}{{Stroud, Katzfuss, and Wikle}}}
\bibcite{syed2011review}{{214}{2011}{{Syed}}{{Syed}}}
\bibcite{tandeo2011linear}{{215}{2011}{{Tandeo {\em  et~al.}}}{{Tandeo, Ailliot, and Autret}}}
\bibcite{tanner1987calculation}{{216}{1987}{{Tanner and Wong}}{{Tanner and Wong}}}
\bibcite{taylor2000integration}{{217}{2000}{{Taylor {\em  et~al.}}}{{Taylor, Woolley, and Zito}}}
\bibcite{tierney1994markov}{{218}{1994}{{Tierney}}{{Tierney}}}
\bibcite{tierney1999some}{{219}{1999}{{Tierney and Mira}}{{Tierney and Mira}}}
\bibcite{esl2009}{{220}{2009}{{Trevor~Hastie}}{{Trevor~Hastie}}}
\bibcite{turitsyn2011irreversible}{{221}{2011}{{Turitsyn {\em  et~al.}}}{{Turitsyn, Chertkov, and Vucelja}}}
\bibcite{tusell2011kalman}{{222}{2011}{{Tusell {\em  et~al.}}}{{Tusell {\em  et~al.}}}}
\bibcite{vaughan2015goodness}{{223}{2015}{{Vaughan}}{{Vaughan}}}
\bibcite{vieira2016online}{{224}{2016}{{Vieira and Wilkinson}}{{Vieira and Wilkinson}}}
\bibcite{wahba1978improper}{{225}{1978}{{Wahba}}{{Wahba}}}
\bibcite{wahba1985comparison}{{226}{1985}{{Wahba}}{{Wahba}}}
\bibcite{wahba1990spline}{{227}{1990}{{Wahba}}{{Wahba}}}
\bibcite{wahba1990optimal}{{228}{1990}{{Wahba and Wang}}{{Wahba and Wang}}}
\bibcite{wahba1975completely}{{229}{1975}{{Wahba and Wold}}{{Wahba and Wold}}}
\bibcite{wakefield2013bayesian}{{230}{2013}{{Wakefield}}{{Wakefield}}}
\bibcite{wan2000unscented}{{231}{2000}{{Wan and Van Der~Merwe}}{{Wan and Van Der~Merwe}}}
\bibcite{wand1997exact}{{232}{1997}{{Wand and Gutierrez}}{{Wand and Gutierrez}}}
\bibcite{wang1998smoothing}{{233}{1998}{{Wang}}{{Wang}}}
\bibcite{wecker1983signal}{{234}{1983}{{Wecker and Ansley}}{{Wecker and Ansley}}}
\bibcite{west1993mixture}{{235}{1993}{{West}}{{West}}}
\bibcite{whittaker1922new}{{236}{1922}{{Whittaker}}{{Whittaker}}}
\bibcite{wolberg1988cubic}{{237}{1988}{{Wolberg}}{{Wolberg}}}
\bibcite{woodbury1950inverting}{{238}{1950}{{Woodbury}}{{Woodbury}}}
\bibcite{yang2010analytical}{{239}{2010}{{Yang and Sukkarieh}}{{Yang and Sukkarieh}}}
\bibcite{yao2005functional}{{240}{2005}{{Yao {\em  et~al.}}}{{Yao, M{\"u}ller, Wang, {\em  et~al.}}}}
\bibcite{yu2004curve}{{241}{2004}{{Yu {\em  et~al.}}}{{Yu, Kim, Bailey, and Gamboa}}}
\bibcite{zhang2013cubic}{{242}{2013}{{Zhang {\em  et~al.}}}{{Zhang, Guo, and Gao}}}
