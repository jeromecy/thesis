\relax 
\citation{introkalman}
\citation{kalmaninr}
\citation{selfturningkalman}
\citation{adaptivekalman}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Statement}{1}}
\citation{ESLII}
\citation{smoothsplinein2D}
\citation{ESLII}
\citation{smoothingparameter}
\citation{kim2004smoothing}
\citation{donoho1995wavelet}
\citation{liu2010data}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Spline Reconstruction}{2}}
\citation{ESLII}
\citation{yoshimoto2003}
\citation{simon2004data}
\citation{esl2009}
\newlabel{fbasis}{{1.1}{4}}
\newlabel{mse}{{1.3}{4}}
\citation{wahba1975completely}
\newlabel{mse2}{{1.6}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Cross Validation}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}K-Fold Cross Validation}{5}}
\citation{b_gpml}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Gaussian Process Regression}{6}}
\newlabel{covdef}{{1.10}{6}}
\newlabel{covYV}{{1.15}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}A Reproducing Kernel in Space $\mathbb  {H}$}{7}}
\newlabel{sectionRK}{{1.3.1}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Filtering Methods}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Sequential Monte Carlo Markov Chain Algorithm}{7}}
\citation{ben2004geometric}
\citation{Atkinson2004}
\citation{taylor2000integration}
\citation{agarwal2003indexing}
\citation{gloderer2010spline}
\citation{magid2006spline}
\citation{yu2004curve}
\citation{komoriya1989trajectory}
\citation{gasparetto2007new}
\citation{erkorkmaz2001high}
\citation{yang2010analytical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Tractor Spline Theory}{9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{9}}
\citation{kim2004smoothing}
\citation{yao2005functional}
\citation{craven1978smoothing}
\citation{aydin2012smoothing}
\citation{green1993nonparametric}
\citation{schwarz2012geodesy}
\citation{green1993nonparametric}
\citation{sealfon2005smoothing}
\newlabel{smoothingob}{{2.1}{10}}
\citation{ben2004geometric}
\citation{zhang2013cubic}
\citation{castro2006geometric}
\citation{castro2006geometric}
\citation{silverman1985some}
\citation{donoho1995wavelet}
\citation{gu1998model}
\citation{craven1978smoothing}
\citation{wahba1985comparison}
\citation{liu2010data}
\newlabel{objective}{{2.3}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Tractor Spline}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Objective Function}{12}}
\newlabel{of2d}{{2.5}{12}}
\citation{zhang2013cubic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Basis Functions}{13}}
\newlabel{hermitebasis1}{{2.6}{13}}
\newlabel{basisindependent}{{2}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The two basis functions $N_{2k+1}$ and $N_{2k+2}$ on an arbitrary interval $[t_k, t_{k+2})$. It is apparently that these basis functions are continuous on this interval and have continuous first derivatives.\relax }}{15}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{basisfigure}{{2.1}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Solution to The Objective Function}{15}}
\newlabel{tractormse}{{2.16}{15}}
\citation{esl2009}
\citation{esl2009}
\newlabel{thetahat}{{2.20}{16}}
\newlabel{fhy}{{2.24}{16}}
\citation{green1993nonparametric}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Adjusted Penalty Term and Parameter Function}{17}}
\newlabel{adjustedpenalty}{{2.26}{17}}
\citation{green1993nonparametric}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Parameter Selection and Cross Validation}{18}}
\newlabel{originalcv}{{2.27}{18}}
\newlabel{cvlema}{{1}{18}}
\newlabel{cvscore}{{3}{19}}
\newlabel{tractorcv}{{2.35}{19}}
\newlabel{cvlemma}{{2}{19}}
\citation{donoho1994ideal}
\citation{donoho1995adapting}
\citation{abramovich1998wavelet}
\citation{nason2010wavelet}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Simulation Study}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Numerical Examples}{20}}
\newlabel{ofgamma0}{{2.41}{20}}
\newlabel{thetahat0}{{2.42}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Evaluation}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Numerical example: $\textit  {Blocks}$. Comparison of different reconstruction methods with simulated data.\relax }}{22}}
\newlabel{num1}{{2.2}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Numerical example: $\textit  {Bumps}$. Comparison of different reconstruction methods with simulated data.\relax }}{23}}
\newlabel{num2}{{2.3}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Numerical example: $\textit  {HeaviSine}$. Comparison of different reconstruction methods with simulated data.\relax }}{24}}
\newlabel{num3}{{2.4}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Numerical example: $\textit  {Doppler}$. Comparison of different reconstruction methods with simulated data.\relax }}{25}}
\newlabel{num4}{{2.5}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Distribution of penalty values in reconstructed Tractor Spline. Figures on left side indicate the values of $\lambda (t)$ varying in intervals. On right side, $\lambda (t)$ is projected into reconstructions. The bigger the blacks dots present, the larger the penalty values are.\relax }}{26}}
\newlabel{numpenalty}{{2.6}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Estimated velocity functions (original simulation functions) by Tractor Spline.\relax }}{27}}
\newlabel{numvtractor}{{2.7}{27}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces MSE. Mean square errors of different methods. The numbers in bold indicate the smallest error among these methods under the same level. The difference is not significant.\relax }}{28}}
\newlabel{mse3200}{{2.1}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces TMSE. True mean square errors of different methods. The numbers in bold indicate the smallest error among these methods under the same level. The proposed Tractor Spline returns the smallest TMSE among all the methods under the same level except for $\mathit  {Doppler}$ with SNR=7. The differences are significant. \relax }}{28}}
\newlabel{tmse3200}{{2.2}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Original data points. (a) Original positions recorded by GPS units. Circle points means the boom is not working; cross points means it is working. (b) Original trajectory with line-based method: simply connect all the points sequentially with straight lines. (c) Original positions on $x$ axis. (d) Original positions on $y$ axis.\relax }}{29}}
\newlabel{original512}{{2.8}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Application on Real Dataset}{29}}
\newlabel{splineapplication}{{2.5}{29}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Mean square error. Tractor Spline returns smallest errors among all these methods. P-spline was unable to reconstruct the $y$ trajectory as the original dataset contains value-$0$ time differences.\relax }}{30}}
\newlabel{1dxymse}{{2.3}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}1-Dimension Trajectory}{30}}
\newlabel{penaltylamb}{{2.48}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Fitted data points on $x$ axis. (a) Fitted by P-spline, which gives over-fitting on these points and misses some information. (b) Fitted by wavelet ($\mathit  {sure}$) algorithm. At some turning points, it gives over-fitting. (c) Fitted by wavelet ($\mathit  {BayesThresh}$) algorithm. It fits better than ($\mathit  {sure}$) and the result is close to the proposed method. (d) Fitted by Tractor Spline without velocity information. The reconstruction is good to get the original trajectory. (e) Fitted by Tractor Spline without adjusted penalty term. It gives less fitting at boom-not-working points because of a large time gap. (f) Fitted by proposed method. It fits all data points in a good way.\relax }}{31}}
\newlabel{1dx}{{2.9}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Fitted data points on $y$ axis. (a) Fitted P-spline is not applicable on $y$ axis as the matrix is not invertible. (b) Fitted by wavelet ($\mathit  {sure}$) algorithm. At some turning points, it gives over-fitting. (c) Fitted by wavelet ($\mathit  {BayesThresh}$) algorithm is much better than wavelet ($\mathit  {sure}$). (d) Fitted by Tractor Spline without velocity information. The reconstruction is good to get the original trajectory. (e) Fitted by Tractor Spline without adjusted penalty term. It gives less fitting at boom-not-working. (f) Fitted by proposed method. It fits all data points in a good way.\relax }}{32}}
\newlabel{1dy}{{2.10}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}2-Dimension Trajectory}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Penalty function of Tractor Spline on $x$ and $y$ axises. The big black dots in plots (c) indicate large penalty values. It can be seen that most of large penalty values occur at turnings, where the tractor likely slows down and takes breaks. \relax }}{34}}
\newlabel{penaltyxygg}{{2.11}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces 2-Dimensional reconstruction with penalty function. Larger dots indicate bigger penalty values. The mean square errors (MSE) on $x$ and $y$ are 0.240734 and 0.478422 respectively. The mean distance error $\sqrt  {(\mathaccentV {hat}05E{f}_x-x)^2+ (\mathaccentV {hat}05E{f}_y-y)^2}$ is 0.645830.\relax }}{35}}
\newlabel{2dxy}{{2.12}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Penalty function of 2-Dimensional reconstruction.\relax }}{35}}
\newlabel{2dpenalty}{{2.13}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusion and Discussion}{36}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Gaussian Process Regression}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{37}}
\citation{de1978practical}
\citation{judd1998numerical}
\citation{chen2009feedback}
\citation{ellis2009}
\citation{green1993nonparametric}
\citation{hastie1990generalized}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Spline}{38}}
\newlabel{fbasis}{{3.1}{38}}
\newlabel{mse}{{3.2}{38}}
\citation{b_gpml}
\citation{berlinet2011reproducing}
\citation{wahba1990spline}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Gaussian Process Regression}{39}}
\newlabel{covdef}{{3.6}{39}}
\citation{wang1998smoothing}
\citation{kimeldorf1971some}
\citation{kimeldorf1970correspondence}
\citation{gubook}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The Smoothing Spline as Bayes Estimates}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}A reproducing kernel on $\mathcal  {C}_{p.w.}^{2}[0,1]$}{40}}
\newlabel{maineq}{{3.17}{40}}
\newlabel{kerneleq}{{3.19}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Computation of Polynomial Smoothing Splines}{42}}
\newlabel{etaeq}{{3.23}{42}}
\newlabel{matrixeq}{{3.24}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Polynomial Smoothing Splines as Bayes Estimates}{43}}
\newlabel{rhoeq}{{3.26}{44}}
\newlabel{lem}{{3}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Numeric Simulation of Smoothing Spline and GPR}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces (a) Comparing two methods under the same parameters $\lambda =0.01$ and $\gamma =0.1$. In this graph, the blue line is reconstruction from tractor spline, the red line is the mean of Gaussian Process, which is the posterior $\mathbb  {E}(f(x) \mid \mathbf  {Y}, \mathbf  {V})$. (b) The differences between two methods under the same parameters.\relax }}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Best fitting simulation by TS and GPR.\relax }}{47}}
\citation{cappe2009inference}
\citation{smcmip2011}
\citation{elliott1995estimation}
\citation{cargnoni1997bayesian}
\citation{vieira2016online}
\citation{hangos2006analysis}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Filtering Problem}{49}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{49}}
\citation{petris2009dynamic}
\citation{petris2009dynamic}
\citation{kalman1960new}
\citation{de1988likelihood}
\newlabel{statemodel1}{{4.1}{50}}
\newlabel{statemodel2}{{4.2}{50}}
\citation{handschin1969monte}
\citation{handschin1970monte}
\citation{gordon1993novel}
\citation{cappe2009inference}
\citation{smcmip2011}
\citation{ristic2004beyond}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Filtering Problem and Estimation}{51}}
\newlabel{sectionFiltering}{{4.2}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Sequential Monte Carlo Method}{51}}
\citation{arulampalam2002tutorial}
\newlabel{rawParticleFilter}{{4.3}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Importance sampling}{53}}
\newlabel{PFexpectation}{{4.4}{53}}
\citation{smcmip2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Sequential Importance Sampling and Resampling}{54}}
\citation{andrieu2010particle}
\citation{andrieu1999sequential}
\citation{fearnhead2002markov}
\citation{storvik2002particle}
\citation{pitt1999filtering}
\citation{pitt1999filtering}
\citation{gordon1993novel}
\citation{JOHANSEN20081498}
\citation{liu2008monte}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Auxiliary Particle Filter}{56}}
\citation{chopin2002sequential}
\citation{chopin2002sequential}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Sequential Particle Filter}{57}}
\citation{tierney1994markov}
\citation{septier2009mcmc}
\citation{berzuini1997dynamic}
\citation{khan2005mcmc}
\citation{golightly2006bayesian}
\citation{pang2008models}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Sequential MCMC}{58}}
\citation{septier2009mcmc}
\citation{pang2008models}
\citation{septier2009multiple}
\citation{cappe2007overview}
\citation{kantas2009overview}
\citation{higuchi2001self}
\citation{kitagawa1998self}
\citation{liu2001combined}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}On-line State and Parameter Estimation}{60}}
\newlabel{sectionStateandPara}{{4.3}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Artificial Dynamic Noise}{60}}
\citation{polson2008practical}
\citation{polson2008practical}
\citation{kantas2009overview}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Practical Filtering}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Liu and West's Filter}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Storvik Filter}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Particle Learning}{62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Ensemble Kalman Filter}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}On-line EM}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Simulation Study}{63}}
\newlabel{sectionSimulation}{{4.4}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{63}}
\citation{vieira2016online}
\citation{kitagawa1998self}
\citation{liu2001combined}
\citation{storvik2002particle}
\citation{stroud2016bayesian}
\citation{carvalho2010particle}
\citation{stroud2016bayesian}
\citation{polson2008practical}
\citation{haario1999adaptive}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Monte Carlo Markov Chain}{65}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Bayesian Inference on Combined State and Parameters}{66}}
\newlabel{obserY}{{5.1}{66}}
\newlabel{hiddX}{{5.2}{66}}
\citation{hammersley1964percolation}
\citation{geweke1989bayesian}
\citation{casella2004generalized}
\citation{martino2010generalized}
\citation{geman1984stochastic}
\citation{metropolis1953equation}
\citation{hastings1970monte}
\newlabel{objecfun}{{5.3}{67}}
\newlabel{M1}{{5.4}{67}}
\newlabel{M2}{{5.5}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Log-likelihood Function of Parameter Posterior}{68}}
\newlabel{sectionlogParameter}{{5.2.1}{68}}
\newlabel{generaljointmatrix}{{5.6}{68}}
\newlabel{sigmayy01}{{5.7}{68}}
\newlabel{sigmayy02}{{5.8}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}The Forecast Distribution}{69}}
\newlabel{sectionforecast}{{5.2.2}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}The Estimation Distribution}{70}}
\newlabel{generalEstDistr}{{5.2.3}{70}}
\newlabel{generalmux}{{5.13}{70}}
\newlabel{generalSigx}{{5.14}{70}}
\newlabel{mixtureGaussian}{{5.15}{70}}
\newlabel{mixturemean}{{5.16}{70}}
\citation{smith1993bayesian}
\citation{tierney1994markov}
\citation{gilks1995markov}
\citation{muller1991generic}
\citation{tierney1994markov}
\citation{dongarra2000guest}
\citation{medova2008bayesian}
\citation{metropolis1953equation}
\newlabel{mixturevariance}{{5.17}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Random Walk Metropolis-Hastings algorithm}{71}}
\newlabel{alphabalance}{{5.18}{71}}
\citation{sherlock2016adaptive}
\citation{sherlock2010random}
\newlabel{stepsizeep}{{5.19}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Self-tuning Metropolis-Hastings Algorithm}{72}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Examples of 2-Dimension Random Walk Metropolis-Hastings algorithm. Figure (a) is using one-variable-at-a-time proposal Random Walk. At each time, only one variable is changed and the other one stay constant. Figure (b) and (c) are using multi-variable-at-a-time Random Walk. The difference is in figure (b), every forward step are proposed independently, but in (c) are proposed according to the covariance matrix. \relax }}{73}}
\newlabel{randomwalk}{{5.1}{73}}
\newlabel{autostepab}{{5.22}{73}}
\citation{christen2005markov}
\newlabel{algoonevarible}{{1}{74}}
\newlabel{stRWMHselect}{{3}{74}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Self-tuning Random Walk Metropolis-Hastings Algorithm.\relax }}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Adaptive Delayed Acceptance Metropolis-Hastings Algorithm}{74}}
\newlabel{dahalpha2}{{5.23}{74}}
\citation{sherlock2015efficiency}
\citation{stroud2016bayesian}
\citation{mathew2012bayesian}
\citation{sherlock2010random}
\citation{roberts2001optimal}
\newlabel{dahalpha1}{{5.24}{75}}
\citation{gelman1996efficient}
\citation{gilks1995markov}
\citation{roberts2001optimal}
\citation{roberts1997weak}
\citation{bedard2007weak}
\citation{beskos2009optimal}
\citation{sherlock2009optimal}
\citation{sherlock2013optimal}
\citation{sherlock2010random}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Efficiency of Metropolis-Hastings Algorithm}{76}}
\newlabel{effMHA}{{5.3.3}{76}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Metropolis algorithm sampling for a single parameter with (a) a large step size, (b) a small step size, (c) an appropriate step size. The upper plots show the sample chain and lower plots indicate the autocorrelation for each case.\relax }}{76}}
\newlabel{largesmallstepsize}{{5.2}{76}}
\citation{andrieu2008tutorial}
\citation{sherlock2010random}
\citation{graves2011automatic}
\citation{roberts2001optimal}
\citation{roberts2001optimal}
\citation{kass1998markov}
\citation{robert2004monte}
\citation{gong2016practical}
\citation{geyer1992practical}
\citation{sokal1997monte}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Simulation Studies}{79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Simulation on Regular Time Series Data}{80}}
\citation{lopes2011particle}
\newlabel{linearlogL}{{5.29}{81}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Linear simulation with true parameter $\theta = \{\phi =0.9,\tau ^2=0.5,\sigma ^2=1\}$. By transforming to original scale, the estimation is $\mathaccentV {hat}05E{\theta }=\{ \phi = 0.8810, \tau ^2 = 0.5247,\sigma ^2= 0.9416\}$. \relax }}{81}}
\newlabel{linearmarginplots}{{5.3}{81}}
\newlabel{sectionlinearRecursive}{{5.4.1}{81}}
\citation{sherman1950adjustment}
\citation{woodbury1950inverting}
\citation{bartlett1951inverse}
\citation{bodewig1959matrix}
\citation{deng2011generalization}
\citation{bartlett1951inverse}
\newlabel{beforeSMformula}{{5.30}{82}}
\newlabel{SMWformula}{{5.31}{82}}
\newlabel{SMformula}{{5.32}{82}}
\citation{tandeo2011linear}
\newlabel{linearmu}{{5.37}{83}}
\newlabel{linearsigma}{{5.38}{83}}
\citation{einstein1956investigations}
\citation{vaughan2015goodness}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Linear simulation of $x_{1:t}$ and sole $x_t$.In sub-figure (a), the dots is the true $x_{1:t}$ and the solid line is the estimation $\mathaccentV {hat}05E{x}_{1:t}$. In sub-figure (b), the chain in solid line is the estimation $\mathaccentV {hat}05E{x}_t$; dotted line is the true value of $x$; dot-dash line on top is the observed value of $y$; dashed lines are the estimated error. \relax }}{84}}
\newlabel{linearmarginXt}{{5.4}{84}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Simulation on Irregular Time Series Data}{84}}
\citation{kijima1997markov}
\newlabel{linearOUequation}{{5.41}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Simulated data. The circle dots are the true state $x_{1:t}$ and cross dots are observations $y_{1:t}$. Irregular time lag $\Delta _t$ are generated from \textit  {Inverse Gamma}(2,0.1) distribution.\relax }}{86}}
\newlabel{simuOUreview}{{5.5}{86}}
\newlabel{simuOUlogL}{{5.45}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Irregular time step OU process simulation. The estimation of $\mathaccentV {hat}05E{\theta }$ is $\{\gamma =0.4841, \lambda ^2=0.1032, \sigma ^2=0.9276\}$. In the plots, the horizontal dark lines are the true $\theta $. \relax }}{87}}
\newlabel{simuOUmarginplots}{{5.6}{87}}
\newlabel{linearOUK}{{5.46}{87}}
\newlabel{linearOUmu}{{5.48}{88}}
\newlabel{linearOUsigma}{{5.49}{88}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}High Dimensional OU-Process Application}{88}}
\newlabel{OUprocess}{{5.52}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Irregular time step OU process simulation of $x_{1:t}$ and sole $x_t$. In sub-figure (a), the dots is the true $x_{1:t}$ and the solid line is the estimation $\mathaccentV {hat}05E{x}_{1:t}$. In sub-figure (b), the chain in solid line is the estimation $\mathaccentV {hat}05E{x}_t$; dotted line is the true value of $x$; dot-dash line on top is the observed value of $y$; dashed lines are the estimated error. \relax }}{89}}
\newlabel{simuOUxt}{{5.7}{89}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces The trajectory of a moving tractor. The time lags (right side figure) obtained from GPS units are irregular.\relax }}{89}}
\newlabel{realdatareview}{{5.8}{89}}
\newlabel{obseq}{{5.56}{90}}
\newlabel{obmodel}{{5.57}{90}}
\newlabel{jointmatrix}{{5.58}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Approximations of The Parameters Posterior}{91}}
\newlabel{logL}{{5.59}{92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}The Forecast Distribution}{92}}
\newlabel{OUupdatingK}{{5.61}{93}}
\citation{lindley1972bayes}
\citation{smith1973general}
\citation{gelman2006prior}
\citation{jeffries1961theory}
\citation{jaynes1983papers}
\citation{box2011bayesian}
\citation{gelman2008weakly}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}The Estimation Distribution}{94}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Prior Distribution for Parameters}{94}}
\citation{stroud2007sequential}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Probability density function and cumulative distribution function of \textit  {Inverse Gamma} with two parameters $\alpha $ and $\beta $. \relax }}{96}}
\newlabel{IGPDFCDF}{{5.9}{96}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Efficiency of DA-MH}{96}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces An example of Eff, EUT, ESS and ESSUT found by running 10\tmspace  +\thinmuskip {.1667em}000 iterations with same data. \relax }}{97}}
\newlabel{effeutessessutexampletable}{{5.1}{97}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Comparing Eff, EUT, ESS and ESSUT values using different step size. The $1000^\star $ means taking 1\tmspace  +\thinmuskip {.1667em}000 samples from a longer chain, like 1\tmspace  +\thinmuskip {.1667em}000 out of 5\tmspace  +\thinmuskip {.1667em}000 sample chain. \relax }}{97}}
\newlabel{stepsizecompare}{{5.2}{97}}
\citation{polson2008practical}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces An example of Eff, EUT, ESS and ESSUT found by using the same data. \relax }}{98}}
\newlabel{effeutessessutexamplefigure}{{5.10}{98}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.6}Sliding Window State and Parameter Estimation}{98}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Comparison Eff, ESS, EffUT and ESSUT of different length of data. \relax }}{99}}
\newlabel{compareLengthData}{{5.11}{99}}
\newlabel{algorithmslidingwindow}{{2}{101}}
\newlabel{algorithmlearningsurface}{{2}{101}}
\newlabel{algorithmestimaiton}{{3}{101}}
\newlabel{algorithmDA}{{4}{101}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Sliding Window MCMC.\relax }}{101}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.7}Implementation}{101}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Comparison $\qopname  \relax o{ln}DA$ and $\qopname  \relax o{ln}L$ between not-updating and updating mean methods. \relax }}{103}}
\newlabel{comparenotanupDAL}{{5.12}{103}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Comparison between not-updating and updating mean methods. \relax }}{104}}
\newlabel{comparenotanupfeatures}{{5.13}{104}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Visualization of correlation matrix of $\theta $, which is found in learning-surface process. \relax }}{104}}
\newlabel{realdatacorMatrix}{{5.14}{104}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Trace plots of $\theta $ from learning surface process after taking 1\tmspace  +\thinmuskip {.1667em}000 samples from 5\tmspace  +\thinmuskip {.1667em}000. \relax }}{105}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Position and velocity for $X$ and $Y$ found by combined batch and sequential methods. \relax }}{106}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Zoom in on estimations. For each estimation $\mathaccentV {hat}05E{X}_i (i=1,\dots  ,t)$, there is a error circle around it. \relax }}{107}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Discussion and Future Work}{108}}
\citation{ivanov2012real}
\citation{douglas1973algorithms}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Data Simplify}{109}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{109}}
\citation{ying2011semantic}
\citation{chen2009trajectory}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Preliminary}{110}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Track Simplification Algorithm}{110}}
\citation{lawson2011compression}
\citation{lawson2011compression}
\citation{meratnia2004spatiotemporal}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Evaluation}{111}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Synchronized Euclidean Distance\relax }}{112}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces (a) error measured at fixed sampling rate as sum of perpendicular distance chords; (b) error measured at fixed sampling rates as sum of time-synchronous distance chords.\relax }}{112}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Adaptive Kalman Filter}{113}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Experimental results}{113}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Error Comparing\relax }}{113}}
\newlabel{tabledist}{{6.1}{113}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces A segment start from time 2000 to 3000, recorded by GPS units. On the left side, it's the trajectory connected by raw data with 27 points. In the middle, it's the trajectory connected by simplified data with Douglas-Peucker Algorithm with 24 points. On the right side, it's the trajectory connected by simplified data with Tractor Simplification Algorithm with 23 points.\relax }}{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Trajectory fitted by Adaptive Kalman Filter. The errors of raw data, DP and tractor algorithm caused by AKF is 26.89217, 23.97877 and 23.97097 respectively.\relax }}{114}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Future Work}{115}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Summary}{117}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Appendices}{119}}
\@writefile{toc}{\setcounter {tocdepth}{1}}
\@writefile{toc}{\begingroup \let \l@chapter \l@section \let \l@section \l@subsection }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Spline Penalty}{120}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Penalty Matrix in (2.16\hbox {})}{120}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Proof of Theorem 2\hbox {}}{121}}
\newlabel{matrixD}{{A.1}{121}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Proof of Lemma 2\hbox {}}{124}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Proof of Theorem 3\hbox {}}{125}}
\newlabel{th3proofeq1}{{A.6}{125}}
\newlabel{th3proofeq2}{{A.8}{125}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}MCMC}{126}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Appendices}{126}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.1}Linear Simulation Calculations}{126}}
\newlabel{linearcalculation}{{B.1.1}{126}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.2}OU process calculation}{129}}
\newlabel{OUcalculation}{{B.1.2}{129}}
\newlabel{OUKtp1}{{B.13}{131}}
\newlabel{recursiveKp1}{{B.15}{132}}
\newlabel{covMatrixdetails}{{B.1.2}{135}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1.3}Real Data Implementation}{137}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Running the same amount of time and taking the same length of data, the step size $\epsilon =2.5$ returns the highest ESSUT value and generates more effective samples with a lower correlation. \relax }}{138}}
\newlabel{1koutof8kfigures}{{B.1}{138}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Impacts of data length on optimal parameter. \relax }}{139}}
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Parameter estimation by running the whole surface learning and DA-MH processes with different length of data\relax }}{140}}
\newlabel{lengthofdatacompare}{{B.1.3}{141}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison Between Batch and Sliding Window Methods}{141}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Key features comparison. \relax }}{141}}
\newlabel{batchwindowkeyfeature}{{B.3}{141}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Parameter Comparison. \relax }}{142}}
\newlabel{batchwindowparameter}{{B.4}{142}}
\@writefile{toc}{\contentsline {subsubsection}{Parameter Evolution Visualization}{143}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces Parameter Evolution Visualization. \relax }}{143}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.6}{\ignorespaces Parameter Evolution Visualization. \relax }}{144}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.7}{\ignorespaces Parameter Evolution Visualization. \relax }}{145}}
\bibstyle{otago}
\bibdata{thesis}
\@writefile{lof}{\contentsline {figure}{\numberline {B.8}{\ignorespaces Parameter Evolution Visualization. \relax }}{146}}
\bibcite{abramovich1998wavelet}{{1}{1998}{{Abramovich {\em  et~al.}}}{{Abramovich, Sapatinas, and Silverman}}}
\bibcite{agarwal2003indexing}{{2}{2003}{{Agarwal {\em  et~al.}}}{{Agarwal, Arge, and Erickson}}}
\bibcite{andrieu1999sequential}{{3}{1999}{{Andrieu {\em  et~al.}}}{{Andrieu, De~Freitas, and Doucet}}}
\bibcite{andrieu2010particle}{{4}{2010}{{Andrieu {\em  et~al.}}}{{Andrieu, Doucet, and Holenstein}}}
\bibcite{andrieu2008tutorial}{{5}{2008}{{Andrieu and Thoms}}{{Andrieu and Thoms}}}
\bibcite{smcmip2011}{{6}{2011}{{Arnaud~Doucet}}{{Arnaud~Doucet}}}
\bibcite{arulampalam2002tutorial}{{7}{2002}{{Arulampalam {\em  et~al.}}}{{Arulampalam, Maskell, Gordon, and Clapp}}}
\bibcite{Atkinson2004}{{8}{2004}{{Atkinson}}{{Atkinson}}}
\bibcite{aydin2012smoothing}{{9}{2012}{{Aydin and Tuzemen}}{{Aydin and Tuzemen}}}
\bibcite{bartlett1951inverse}{{10}{1951}{{Bartlett}}{{Bartlett}}}
\@writefile{toc}{\endgroup }
\@writefile{toc}{\contentsline {chapter}{\hbox to\@tempdima {\hfil }References}{147}}
\bibcite{bedard2007weak}{{11}{2007}{{B{\'e}dard}}{{B{\'e}dard}}}
\bibcite{ben2004geometric}{{12}{2004}{{Ben-Arieh {\em  et~al.}}}{{Ben-Arieh, Chang, Rys, and Zhang}}}
\bibcite{berlinet2011reproducing}{{13}{2011}{{Berlinet and Thomas-Agnan}}{{Berlinet and Thomas-Agnan}}}
\bibcite{berzuini1997dynamic}{{14}{1997}{{Berzuini {\em  et~al.}}}{{Berzuini, Best, Gilks, and Larizza}}}
\bibcite{beskos2009optimal}{{15}{2009}{{Beskos {\em  et~al.}}}{{Beskos, Roberts, and Stuart}}}
\bibcite{box2011bayesian}{{16}{2011}{{Box and Tiao}}{{Box and Tiao}}}
\bibcite{cappe2007overview}{{17}{2007}{{Capp{\'e} {\em  et~al.}}}{{Capp{\'e}, Godsill, and Moulines}}}
\bibcite{cappe2009inference}{{18}{2009}{{Capp{\'e} {\em  et~al.}}}{{Capp{\'e}, Moulines, and Ryd{\'e}n}}}
\bibcite{cargnoni1997bayesian}{{19}{1997}{{Cargnoni {\em  et~al.}}}{{Cargnoni, M{\"u}ller, and West}}}
\bibcite{carvalho2010particle}{{20}{2010}{{Carvalho {\em  et~al.}}}{{Carvalho, Johannes, Lopes, Polson, {\em  et~al.}}}}
\bibcite{casella2004generalized}{{21}{2004}{{Casella {\em  et~al.}}}{{Casella, Robert, and Wells}}}
\bibcite{castro2006geometric}{{22}{2006}{{Castro {\em  et~al.}}}{{Castro, Iglesias, Rodr{\'\i }guez-Solano, and S{\'a}nchez}}}
\bibcite{chen2009feedback}{{23}{2009}{{Chen}}{{Chen}}}
\bibcite{chen2009trajectory}{{24}{2009}{{Chen {\em  et~al.}}}{{Chen, Jiang, Zheng, Li, and Yu}}}
\bibcite{selfturningkalman}{{25}{2010}{{Chenjian~RAN}}{{Chenjian~RAN}}}
\bibcite{chopin2002sequential}{{26}{2002}{{Chopin}}{{Chopin}}}
\bibcite{christen2005markov}{{27}{2005}{{Christen and Fox}}{{Christen and Fox}}}
\bibcite{craven1978smoothing}{{28}{1978}{{Craven and Wahba}}{{Craven and Wahba}}}
\bibcite{de1978practical}{{29}{1978}{{De~Boor {\em  et~al.}}}{{De~Boor, De~Boor, Math{\'e}maticien, De~Boor, and De~Boor}}}
\bibcite{de1988likelihood}{{30}{1988}{{De~Jong}}{{De~Jong}}}
\bibcite{deng2011generalization}{{31}{2011}{{Deng}}{{Deng}}}
\bibcite{dongarra2000guest}{{32}{2000}{{Dongarra and Sullivan}}{{Dongarra and Sullivan}}}
\bibcite{donoho1995adapting}{{33}{1995}{{Donoho and Johnstone}}{{Donoho and Johnstone}}}
\bibcite{donoho1995wavelet}{{34}{1995}{{Donoho {\em  et~al.}}}{{Donoho, Johnstone, and Kerkyacharian}}}
\bibcite{donoho1994ideal}{{35}{1994}{{Donoho and Johnstone}}{{Donoho and Johnstone}}}
\bibcite{douglas1973algorithms}{{36}{1973}{{Douglas and Peucker}}{{Douglas and Peucker}}}
\bibcite{einstein1956investigations}{{37}{1956}{{Einstein}}{{Einstein}}}
\bibcite{ellis2009}{{38}{2009}{{Ellis {\em  et~al.}}}{{Ellis, Sommerlade, and Reid}}}
\bibcite{erkorkmaz2001high}{{39}{2001}{{Erkorkmaz and Altintas}}{{Erkorkmaz and Altintas}}}
\bibcite{fearnhead2002markov}{{40}{2002}{{Fearnhead}}{{Fearnhead}}}
\bibcite{gasparetto2007new}{{41}{2007}{{Gasparetto and Zanotto}}{{Gasparetto and Zanotto}}}
\bibcite{gelman2006prior}{{42}{2006}{{Gelman {\em  et~al.}}}{{Gelman {\em  et~al.}}}}
\bibcite{gelman2008weakly}{{43}{2008}{{Gelman {\em  et~al.}}}{{Gelman, Jakulin, Pittau, and Su}}}
\bibcite{gelman1996efficient}{{44}{1996}{{Gelman {\em  et~al.}}}{{Gelman, Roberts, Gilks, {\em  et~al.}}}}
\bibcite{geman1984stochastic}{{45}{1984}{{Geman and Geman}}{{Geman and Geman}}}
\bibcite{geweke1989bayesian}{{46}{1989}{{Geweke}}{{Geweke}}}
\bibcite{geyer1992practical}{{47}{1992}{{Geyer}}{{Geyer}}}
\bibcite{gilks1995markov}{{48}{1995}{{Gilks {\em  et~al.}}}{{Gilks, Richardson, and Spiegelhalter}}}
\bibcite{gloderer2010spline}{{49}{2010}{{Gloderer and Hertle}}{{Gloderer and Hertle}}}
\bibcite{golightly2006bayesian}{{50}{2006}{{Golightly and Wilkinson}}{{Golightly and Wilkinson}}}
\bibcite{gong2016practical}{{51}{2016}{{Gong and Flegal}}{{Gong and Flegal}}}
\bibcite{gordon1993novel}{{52}{1993}{{Gordon {\em  et~al.}}}{{Gordon, Salmond, and Smith}}}
\bibcite{graves2011automatic}{{53}{2011}{{Graves}}{{Graves}}}
\bibcite{green1993nonparametric}{{54}{1993}{{Green and Silverman}}{{Green and Silverman}}}
\bibcite{introkalman}{{55}{2006}{{Greg~Welch}}{{Greg~Welch}}}
\bibcite{gu1998model}{{56}{1998}{{Gu}}{{Gu}}}
\bibcite{gubook}{{57}{2013}{{Gu}}{{Gu}}}
\bibcite{haario1999adaptive}{{58}{1999}{{Haario {\em  et~al.}}}{{Haario, Saksman, and Tamminen}}}
\bibcite{hammersley1964percolation}{{59}{1964}{{Hammersley and Handscomb}}{{Hammersley and Handscomb}}}
\bibcite{handschin1970monte}{{60}{1970}{{Handschin}}{{Handschin}}}
\bibcite{handschin1969monte}{{61}{1969}{{Handschin and Mayne}}{{Handschin and Mayne}}}
\bibcite{hangos2006analysis}{{62}{2006}{{Hangos {\em  et~al.}}}{{Hangos, Bokor, and Szederk{\'e}nyi}}}
\bibcite{ESLII}{{63}{2008}{{Hastie {\em  et~al.}}}{{Hastie, Tibshirani, Friedman, and Franklin}}}
\bibcite{hastie1990generalized}{{64}{1990}{{Hastie and Tibshirani}}{{Hastie and Tibshirani}}}
\bibcite{hastings1970monte}{{65}{1970}{{Hastings}}{{Hastings}}}
\bibcite{higuchi2001self}{{66}{2001}{{Higuchi}}{{Higuchi}}}
\bibcite{ivanov2012real}{{67}{2012}{{Ivanov}}{{Ivanov}}}
\bibcite{jaynes1983papers}{{68}{1983}{{Jaynes}}{{Jaynes}}}
\bibcite{jeffries1961theory}{{69}{1961}{{Jeffries}}{{Jeffries}}}
\bibcite{JOHANSEN20081498}{{70}{2008}{{Johansen and Doucet}}{{Johansen and Doucet}}}
\bibcite{judd1998numerical}{{71}{1998}{{Judd}}{{Judd}}}
\bibcite{kalman1960new}{{72}{1960}{{Kalman {\em  et~al.}}}{{Kalman {\em  et~al.}}}}
\bibcite{kantas2009overview}{{73}{2009}{{Kantas {\em  et~al.}}}{{Kantas, Doucet, Singh, and Maciejowski}}}
\bibcite{kass1998markov}{{74}{1998}{{Kass {\em  et~al.}}}{{Kass, Carlin, Gelman, and Neal}}}
\bibcite{khan2005mcmc}{{75}{2005}{{Khan {\em  et~al.}}}{{Khan, Balch, and Dellaert}}}
\bibcite{kijima1997markov}{{76}{1997}{{Kijima}}{{Kijima}}}
\bibcite{kim2004smoothing}{{77}{2004}{{Kim and Gu}}{{Kim and Gu}}}
\bibcite{kimeldorf1971some}{{78}{1971}{{Kimeldorf and Wahba}}{{Kimeldorf and Wahba}}}
\bibcite{kimeldorf1970correspondence}{{79}{1970}{{Kimeldorf and Wahba}}{{Kimeldorf and Wahba}}}
\bibcite{kitagawa1998self}{{80}{1998}{{Kitagawa}}{{Kitagawa}}}
\bibcite{komoriya1989trajectory}{{81}{1989}{{Komoriya and Tanie}}{{Komoriya and Tanie}}}
\bibcite{lawson2011compression}{{82}{2011}{{Lawson {\em  et~al.}}}{{Lawson, Ravi, and Hwang}}}
\bibcite{lindley1972bayes}{{83}{1972}{{Lindley and Smith}}{{Lindley and Smith}}}
\bibcite{liu2001combined}{{84}{2001}{{Liu and West}}{{Liu and West}}}
\bibcite{liu2008monte}{{85}{2008}{{Liu}}{{Liu}}}
\bibcite{liu2010data}{{86}{2010}{{Liu and Guo}}{{Liu and Guo}}}
\bibcite{lopes2011particle}{{87}{2011}{{Lopes and Tsay}}{{Lopes and Tsay}}}
\bibcite{magid2006spline}{{88}{2006}{{Magid {\em  et~al.}}}{{Magid, Keren, Rivlin, and Yavneh}}}
\bibcite{martino2010generalized}{{89}{2010}{{Martino and M{\'\i }guez}}{{Martino and M{\'\i }guez}}}
\bibcite{mathew2012bayesian}{{90}{2012}{{Mathew {\em  et~al.}}}{{Mathew, Bauer, Koistinen, Reetz, L{\'e}on, and Sillanp{\"a}{\"a}}}}
\bibcite{medova2008bayesian}{{91}{2008}{{Medova}}{{Medova}}}
\bibcite{meratnia2004spatiotemporal}{{92}{2004}{{Meratnia and Rolf}}{{Meratnia and Rolf}}}
\bibcite{metropolis1953equation}{{93}{1953}{{Metropolis {\em  et~al.}}}{{Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller}}}
\bibcite{muller1991generic}{{94}{1991}{{M{\"u}ller}}{{M{\"u}ller}}}
\bibcite{nason2010wavelet}{{95}{2010}{{Nason}}{{Nason}}}
\bibcite{adaptivekalman}{{96}{2001}{{Oussalah and De~Schutter}}{{Oussalah and De~Schutter}}}
\bibcite{pang2008models}{{97}{2008}{{Pang {\em  et~al.}}}{{Pang, Li, and Godsill}}}
\bibcite{petris2009dynamic}{{98}{2009}{{Petris {\em  et~al.}}}{{Petris, Petrone, and Campagnoli}}}
\bibcite{pitt1999filtering}{{99}{1999}{{Pitt and Shephard}}{{Pitt and Shephard}}}
\bibcite{polson2008practical}{{100}{2008}{{Polson {\em  et~al.}}}{{Polson, Stroud, and M{\"u}ller}}}
\bibcite{b_gpml}{{101}{2006}{{Rasmussen and Williams}}{{Rasmussen and Williams}}}
\bibcite{ristic2004beyond}{{102}{2004}{{Ristic {\em  et~al.}}}{{Ristic, Arulampalam, and Gordon}}}
\bibcite{robert2004monte}{{103}{2004}{{Robert}}{{Robert}}}
\bibcite{elliott1995estimation}{{104}{1995}{{Robert J~Elliott}}{{Robert J~Elliott}}}
\bibcite{roberts1997weak}{{105}{1997}{{Roberts {\em  et~al.}}}{{Roberts, Gelman, Gilks, {\em  et~al.}}}}
\bibcite{roberts2001optimal}{{106}{2001}{{Roberts {\em  et~al.}}}{{Roberts, Rosenthal, {\em  et~al.}}}}
\bibcite{schwarz2012geodesy}{{107}{2012}{{Schwarz}}{{Schwarz}}}
\bibcite{sealfon2005smoothing}{{108}{2005}{{Sealfon {\em  et~al.}}}{{Sealfon, Verde, and Jimenez}}}
\bibcite{septier2009multiple}{{109}{2009}{{Septier {\em  et~al.}}}{{Septier, Carmi, Pang, and Godsill}}}
\bibcite{septier2009mcmc}{{110}{2009}{{Septier {\em  et~al.}}}{{Septier, Pang, Carmi, and Godsill}}}
\bibcite{sherlock2013optimal}{{111}{2013}{{Sherlock}}{{Sherlock}}}
\bibcite{sherlock2010random}{{112}{2010}{{Sherlock {\em  et~al.}}}{{Sherlock, Fearnhead, and Roberts}}}
\bibcite{sherlock2016adaptive}{{113}{2016}{{Sherlock {\em  et~al.}}}{{Sherlock, Golightly, and Henderson}}}
\bibcite{sherlock2009optimal}{{114}{2009}{{Sherlock {\em  et~al.}}}{{Sherlock, Roberts, {\em  et~al.}}}}
\bibcite{sherlock2015efficiency}{{115}{2015}{{Sherlock {\em  et~al.}}}{{Sherlock, Thiery, and Golightly}}}
\bibcite{sherman1950adjustment}{{116}{1950}{{Sherman and Morrison}}{{Sherman and Morrison}}}
\bibcite{silverman1985some}{{117}{1985}{{Silverman}}{{Silverman}}}
\bibcite{simon2004data}{{118}{2004}{{Simon}}{{Simon}}}
\bibcite{smith1973general}{{119}{1973}{{Smith}}{{Smith}}}
\bibcite{smith1993bayesian}{{120}{1993}{{Smith and Roberts}}{{Smith and Roberts}}}
\bibcite{sokal1997monte}{{121}{1997}{{Sokal}}{{Sokal}}}
\bibcite{storvik2002particle}{{122}{2002}{{Storvik}}{{Storvik}}}
\bibcite{stroud2007sequential}{{123}{2007}{{Stroud and Bengtsson}}{{Stroud and Bengtsson}}}
\bibcite{stroud2016bayesian}{{124}{2016}{{Stroud {\em  et~al.}}}{{Stroud, Katzfuss, and Wikle}}}
\bibcite{tandeo2011linear}{{125}{2011}{{Tandeo {\em  et~al.}}}{{Tandeo, Ailliot, and Autret}}}
\bibcite{taylor2000integration}{{126}{2000}{{Taylor {\em  et~al.}}}{{Taylor, Woolley, and Zito}}}
\bibcite{tierney1994markov}{{127}{1994}{{Tierney}}{{Tierney}}}
\bibcite{esl2009}{{128}{2009}{{Trevor~Hastie}}{{Trevor~Hastie}}}
\bibcite{kalmaninr}{{129}{2011}{{Tusell}}{{Tusell}}}
\bibcite{vaughan2015goodness}{{130}{2015}{{Vaughan}}{{Vaughan}}}
\bibcite{vieira2016online}{{131}{2016}{{Vieira and Wilkinson}}{{Vieira and Wilkinson}}}
\bibcite{wahba1985comparison}{{132}{1985}{{Wahba}}{{Wahba}}}
\bibcite{wahba1990spline}{{133}{1990}{{Wahba}}{{Wahba}}}
\bibcite{wahba1975completely}{{134}{1975}{{Wahba and Wold}}{{Wahba and Wold}}}
\bibcite{wang1998smoothing}{{135}{1998}{{Wang}}{{Wang}}}
\bibcite{smoothingparameter}{{136}{2000}{{Wood}}{{Wood}}}
\bibcite{woodbury1950inverting}{{137}{1950}{{Woodbury}}{{Woodbury}}}
\bibcite{yang2010analytical}{{138}{2010}{{Yang and Sukkarieh}}{{Yang and Sukkarieh}}}
\bibcite{yao2005functional}{{139}{2005}{{Yao {\em  et~al.}}}{{Yao, M{\"u}ller, Wang, {\em  et~al.}}}}
\bibcite{ying2011semantic}{{140}{2011}{{Ying {\em  et~al.}}}{{Ying, Lee, Weng, and Tseng}}}
\bibcite{yoshimoto2003}{{141}{2003}{{Yoshimoto {\em  et~al.}}}{{Yoshimoto, Harada, and Yoshimoto}}}
\bibcite{yu2004curve}{{142}{2004}{{Yu {\em  et~al.}}}{{Yu, Kim, Bailey, and Gamboa}}}
\bibcite{smoothsplinein2D}{{143}{2010}{{Zamani}}{{Zamani}}}
\bibcite{zhang2013cubic}{{144}{2013}{{Zhang {\em  et~al.}}}{{Zhang, Guo, and Gao}}}
