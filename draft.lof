\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Comparing reconstructions of cubic Hermite spline and straight line. On the left side, a genuine cubic Hermite spline is cooperating with noisy velocities. Even though the vehicle is not moving, the reconstruction is following the directions of $P_1$ and $P_2$ and gives a wiggle between the two points. On the right side, it is an expected reconstruction between two not-moving points after a long time gap. \relax }}{25}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Numerical example: $\textit {Blocks}$. Comparison of different reconstruction methods with simulated data.\relax }}{30}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Numerical example: $\textit {Bumps}$. Comparison of different reconstruction methods with simulated data.\relax }}{31}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Numerical example: $\textit {HeaviSine}$. Comparison of different reconstruction methods with simulated data.\relax }}{32}
\contentsline {figure}{\numberline {2.5}{\ignorespaces Numerical example: $\textit {Doppler}$. Comparison of different reconstruction methods with simulated data\relax }}{33}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Distribution of the penalty values $\lambda (t,\eta )$ in V-spline. Figures on the left side indicate the values varying in intervals. On the right side, these values are projected into reconstructions. The bigger the blacks dots present, the larger the penalty values are.\relax }}{35}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Estimated velocity functions by V-spline. The velocity is generated from the original simulation functions by equation \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces 2.34\hbox {}\unskip \@@italiccorr )}}\relax }}{36}
\contentsline {figure}{\numberline {2.8}{\ignorespaces Histogram of $\Delta T$ for irregularly sampled data\relax }}{38}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Comparison of regularly and irregularly sampled data\relax }}{44}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Original data points. Figure 2.10a\hbox {} is th original positions recorded by GPS units. Circle points means the boom is not operating; cross points means it is operating. Figure 2.10b\hbox {} is the line-based trajectory by simply connecting all points sequentially with straight lines. Figure 2.10c\hbox {} is the original $x$ position. Figure 2.10d\hbox {} is the original $y$ positions.\relax }}{45}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Fitted data points on $x$ axis. Figure 2.11a\hbox {} Fitted by P-spline, which gives over-fitting on these points and misses some information. Figure 2.11b\hbox {} Fitted by wavelet ($\textit {sure}$) algorithm. At some turning points, it gives over-fitting. Figure 2.11c\hbox {} Fitted by wavelet ($\textit {BayesThresh}$) algorithm. It fits better than ($\textit {sure}$) and the result is close to the proposed method. Figure 2.11d\hbox {} Fitted by V-spline without velocity information. The reconstruction is good to get the original trajectory. Figure 2.11e\hbox {} Fitted by V-spline without adjusted penalty term. It gives less fitting at boom-not-operating points because of a large time gap. Figure 2.11f\hbox {} Fitted by proposed method. It fits all data points in a good way.\relax }}{46}
\contentsline {figure}{\numberline {2.12}{\ignorespaces Fitted data points on $y$ axis. Figure 2.12a\hbox {} Fitted P-spline is not applicable on $y$ axis as the matrix is not invertible. Figure 2.12b\hbox {} Fitted by wavelet ($\textit {sure}$) algorithm. At some turning points, it gives over-fitting. Figure 2.12c\hbox {} Fitted by wavelet ($\textit {BayesThresh}$) algorithm is much better than wavelet ($\textit {sure}$). Figure 2.12d\hbox {} Fitted by V-spline without velocity information. The reconstruction is good to get the original trajectory. Figure 2.12e\hbox {} Fitted by V-spline without adjusted penalty term. It gives less fitting at boom-not-operating. Figure 2.12f\hbox {} Fitted by proposed method. It fits all data points in a good way.\relax }}{47}
\contentsline {figure}{\numberline {2.13}{\ignorespaces The penalty value $\lambda (t)$ of the V-spline on $x$ and $y$ axes. Red dots are the measurements $\mathbf {y}$. The bigger red dots in figure 2.13b\hbox {} indicate larger penalty values. It can be seen that most of large penalty values occur at turnings, where the tractor likely slows down and takes breaks. \relax }}{48}
\contentsline {figure}{\numberline {2.14}{\ignorespaces Combined reconstruction on $x$ and $y$. Red dots are the measurements $\mathbf {y}$. The bigger size it is, the larger penalty value it indicates. \relax }}{49}
\contentsline {figure}{\numberline {2.15}{\ignorespaces 2-dimensional reconstruction. Larger dots indicate bigger values of penalty function $\lambda (t)$.\relax }}{50}
\contentsline {figure}{\numberline {2.16}{\ignorespaces Penalty value of $\lambda (t)$ in 2-dimensional reconstruction.\relax }}{51}
\contentsline {figure}{\numberline {2.17}{\ignorespaces 2-dimensional reconstruction. Larger dots indicate bigger values of penalty function $\lambda (t)$.\relax }}{51}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Comparison of (trivial) V-spline and its Bayes estimate. Two methods are corresponding to each other. In this figure, black dots are the observations and the solid line is the reconstruction. The V-spline and its posterior $\mathrm {E}(f(t) \mid \mathbf {y}, \mathbf {v})$ of Bayes estimate give the same results. \relax }}{66}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Trace plots. Repeatedly running 50 times with cutting off the first \textbf {300} data. It is apparent that all these algorithms converge to the true parameter (black horizontal line) along with time. St, PL and MCMC-vary have a narrower range. MCMC-100 has a higher variability and MCMC-vary has the least. The more data incorporated in the estimation phase the better approximation to be obtained. \relax }}{90}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Box-plots comparison of all the algorithms. The proposed MCMC algorithm is more stable than other filters.\relax }}{91}
\contentsline {figure}{\numberline {4.3}{\ignorespaces The filtering for $x_{300:897}$ is competitive. These algorithms return close estimations. \relax }}{92}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Examples of 2-Dimensional random walk Metropolis-Hastings algorithm. Figure 5.1a\hbox {} is the trace of one-variable-at-a-time random walk. At each time, only one variable is changed and the other one stay constant. Figure 5.1b\hbox {} and 5.1c\hbox {} present the traces by multi-variable-at-a-time random walk. In figure 5.1b\hbox {}, the proposal for each step is independent, but in figure 5.1c\hbox {} the proposal are proposed correlated.\relax }}{101}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Metropolis-Hastings sampler for a single parameter with: 5.2a\hbox {} a large step size, 5.2b\hbox {} a small step size, 5.2c\hbox {} an appropriate step size. The upper plots show the sample chains and lower plots indicate the autocorrelation values for each case.\relax }}{105}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Linear simulation with true parameter $\theta = \{ \phi =0.9,\tau ^2=0.5,\sigma ^2=1\}$. By transforming back to the original scale, the estimation of $\mathaccentV {hat}05E{\theta }$ is $\{\phi = 0.8810, \tau ^2 = 0.5247, \sigma ^2= 0.9416\}$. \relax }}{110}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Linear simulation of $x_{1:t}$ and single $x_t$.In figure 5.4a\hbox {}, the dots is the true $x_{1:t}$ and the solid line is the estimation $\mathaccentV {hat}05E{x}_{1:t}$. In figure 5.4b\hbox {}, the estimation $\mathaccentV {hat}05E{x}_t$ is very close to the true $x$. In fact, the true $x$ falls in the interval $[\mathaccentV {hat}05E{x}-\varepsilon ,\mathaccentV {hat}05E{x}+\varepsilon ]$.\relax }}{112}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Simulated data. The solid dots indicate the true state $x$ and cross dots indicate observation $y$. Irregular time lag $\Delta _t$ are generated from \textit {Inverse Gamma}(2,0.1) distribution.\relax }}{114}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Irregular time step OU process simulation. The estimation of $\mathaccentV {hat}05E{\theta }$ is $\{\gamma =0.4841, \lambda ^2=0.1032, \sigma ^2=0.9276\}$. In the plots, the horizontal dark lines are the true $\theta $. \relax }}{115}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Irregular time step OU process simulation of $x_{1:t}$ and sole $x_t$. In figure 5.7a\hbox {}, the dots is the true $x_{1:t}$ and the solid line is the estimation $\mathaccentV {hat}05E{x}_{1:t}$. In figure 5.7b\hbox {}, the chain in solid line is the estimation $\mathaccentV {hat}05E{x}_t$; dotted line is the true value of $x$; dot-dash line on top is the observed value of $y$; dashed lines are the estimated error. \relax }}{117}
\contentsline {figure}{\numberline {5.8}{\ignorespaces The trajectory of a moving tractor. The time lags (right side figure) obtained from GPS units are irregular.\relax }}{117}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Probability density function and cumulative distribution function of \textit {Inverse Gamma} with two parameters $\alpha $ and $\beta $. \relax }}{124}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Influences of different step sizes on sampling efficiency (Eff), efficiency in unit time (EffUT), effective sample size (ESS) and effective sample size in unit time (ESSUT) found with the same data\relax }}{126}
\contentsline {figure}{\numberline {5.11}{\ignorespaces Comparison of efficiency (Eff), efficiency in unit time (EffUT), effective sample size (ESS) and effective sample size in unit time (ESSUT) against the different length of data. Increasing data length does not significantly improve the efficiency and ESSUT.\relax }}{128}
\contentsline {figure}{\numberline {5.12}{\ignorespaces Comparison of $\qopname \relax o{ln}DA$ and $\qopname \relax o{ln}L$ surfaces between not-updating-mean and updating-mean methods. It is obviously that the updating-mean method has dense log-surfaces indicating more effective samples.\relax }}{129}
\contentsline {figure}{\numberline {5.13}{\ignorespaces Comparison of acceptance rates $\alpha _1$, $\alpha _2$, EffUT and ESSUT between not-updating-mean and updating-mean methods. Black solid dots $\bullet $ indicate values obtained from not-updating-mean method and black solid triangular $\blacktriangle $ indicate values obtained from updating-mean method. The acceptance rates of the updating-mean method are more stable and effective samples are larger in unit computation time. \relax }}{130}
\contentsline {figure}{\numberline {5.14}{\ignorespaces Visualization of the parameters correlation matrix, which is found in learning phase. \relax }}{131}
\contentsline {figure}{\numberline {5.15}{\ignorespaces Trace plots of $\theta $ from learning phase after taking 1\tmspace +\thinmuskip {.1667em}000 burn-in samples out from 5\tmspace +\thinmuskip {.1667em}000. \relax }}{133}
\contentsline {figure}{\numberline {5.16}{\ignorespaces Estimations of $X$ and $Y$ found by combined batch and sequential methods. The black line is the estimation by batch method and the blue line is the sequential MCMC filtering estimation. Red dots are the measurements.\relax }}{134}
\contentsline {figure}{\numberline {5.17}{\ignorespaces Zoom in on estimations. For each estimation $\mathaccentV {hat}05E{X}_i (i=1,\dots ,t)$, there is an error circle around it. \relax }}{135}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces Reconstructions of generated \textit {Blocks}, \textit {Bumps}, \textit {HeaviSine} and \textit {Doppler} functions by V-spline at SNR=3. The penalty values $\lambda (t)$ in V-spline are projected into reconstructions. The blacks dots are the measurements. The bigger blacks dots indicate the larger penalty values.\relax }}{154}
\contentsline {figure}{\numberline {A.2}{\ignorespaces ACF of residuals at SNR level of 7.\relax }}{155}
\contentsline {figure}{\numberline {A.3}{\ignorespaces ACF of residuals at SNR level of 3.\relax }}{156}
\contentsline {figure}{\numberline {A.4}{\ignorespaces Residuals of 2-dimensional real data reconstruction \relax }}{157}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {B.1}{\ignorespaces Running the same amount of time and taking the same length of data, the step size $\epsilon =2.5$ returns the highest ESSUT value and generates more effective samples in a lower correlated relationship. \relax }}{170}
\contentsline {figure}{\numberline {B.2}{\ignorespaces Impacts of data length on optimal parameter. There is an obvious trend on the estimation against length of data in the estimation process. \relax }}{171}
\contentsline {figure}{\numberline {B.3}{\ignorespaces Comparison of $\alpha _1$, $\alpha _2$, EffUT and ESSUT between batch MCMC (orange) and sliding window MCMC (green). \relax }}{173}
\contentsline {figure}{\numberline {B.4}{\ignorespaces Comparison of parameters estimation between batch MCMC (orange) and sliding window MCMC (green). \relax }}{174}
\contentsline {figure}{\numberline {B.4}{\ignorespaces Parameter Evolution Visualization. The correlation among parameters does not change two much. The parameters are considered static. \relax }}{178}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {C.1}{\ignorespaces Synchronized Euclidean Distance\relax }}{182}
\contentsline {figure}{\numberline {C.2}{\ignorespaces C.2a\hbox {} indicates that the errors are measured at fixed sampling rate as sum of perpendicular distance chords. C.2b\hbox {} indicates that the errors are measured at fixed sampling rates as sum of time-synchronous distance chords.\relax }}{182}
\contentsline {figure}{\numberline {C.3}{\ignorespaces A segment start from time $t=2\tmspace +\thinmuskip {.1667em}000$ to $3\tmspace +\thinmuskip {.1667em}000$, recorded by GPS units. $\blacktriangle $ indicates that the boom is not operating. $\bullet $ indicates that the boom is operating. Figure C.3a\hbox {}, the trajectory connected by raw data with 27 points. Figure C.3b\hbox {}, the trajectory connected by simplified data with Douglas-Peucker algorithm with 24 points. Figure C.3c\hbox {}, the trajectory connected by simplified data with proposed simplification algorithm with 23 points.\relax }}{184}
\contentsline {figure}{\numberline {C.4}{\ignorespaces Trajectory fitted by Kalman filter. The mean squared errors of raw data, DP and proposed algorithm are 26.8922, 23.9788 and 23.9710 respectively.\relax }}{185}
